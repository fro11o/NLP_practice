{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown Corpus from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique word: 56057\n",
      "# of words: 1161192 # of sents: 57340\n",
      "max len(sents[i]): 180\n",
      "# of sents with length < 30: 45692\n",
      "sample sent: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "===============after preprocessing===============\n",
      "unique word: 48052\n",
      "# of words: 653927 # of sents: 48129\n",
      "max len(sents[i]): 28\n",
      "sample sent: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import collections\n",
    "import re\n",
    "\n",
    "words = brown.words()  # only use 'news' for quick development purpose\n",
    "sents = brown.sents()  # only use 'news' for quick development purpose\n",
    "counter = collections.Counter()\n",
    "for word in words:\n",
    "    counter[word] += 1\n",
    "print(\"unique word:\", len(counter))\n",
    "print(\"# of words:\", len(words), \"# of sents:\", len(sents))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s in sents]))\n",
    "print(\"# of sents with length < 30:\", len([len(s) for s in sents if len(s) < 30]))\n",
    "print(\"sample sent:\", sents[0])\n",
    "\n",
    "\n",
    "preprocess_sents = []\n",
    "corpus_counter = collections.Counter()\n",
    "for sent in sents:\n",
    "    tmp_sent = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            tmp_sent.append(word.lower())\n",
    "            corpus_counter[word.lower()] += 1\n",
    "    if len(tmp_sent) <= 28:\n",
    "        preprocess_sents.append(tmp_sent)\n",
    "\n",
    "print(\"===============after preprocessing===============\")\n",
    "\n",
    "print(\"unique word:\", len(corpus_counter))\n",
    "print(\"# of words:\", sum([len(s) for s in preprocess_sents]), \"# of sents:\", len(preprocess_sents))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s in preprocess_sents]))\n",
    "print(\"sample sent:\", preprocess_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Brown Corpus to train an seq2seq autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, vocabulary={}, state_size=64, n_max_length=30):     \n",
    "        self.state_size = state_size\n",
    "        self.n_max_length = n_max_length\n",
    "        self.vocabulary = vocabulary\n",
    "        self.reverse_vocabulary = {k: v for k, v in vocabulary.items()}\n",
    "        \n",
    "        ######################\n",
    "        # Graph Construction #\n",
    "        ######################\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #self.sen_en = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_en\")\n",
    "            #self.sen_de = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_de\")\n",
    "            self.sen_en = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_en\")\n",
    "            self.sen_de = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_de\")\n",
    "            self.sen_en_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_length\")\n",
    "            self.sen_de_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_de_length\")\n",
    "            \n",
    "            batch_size_en = tf.shape(self.sen_en)[0]\n",
    "            batch_size_de = tf.shape(self.sen_de)[0]\n",
    "            batch_max_length_de = tf.shape(self.sen_de)[1]\n",
    "            \n",
    "            # TODO sen_en_embedding could also be self-trained embedding: embedding_lookup\n",
    "            self.embedding = tf.Variable(tf.random_uniform([len(self.vocabulary), self.state_size], -1.0, 1.0), dtype=tf.float32)\n",
    "            #self.sen_en_embedding = tf.one_hot(self.sen_en, len(self.vocabulary))\n",
    "            #self.sen_de_embedding = tf.one_hot(self.sen_de, len(self.vocabulary))\n",
    "            self.sen_en_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_en)\n",
    "            self.sen_de_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_de)\n",
    "            \n",
    "            # build encoder decoder structure\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en_init = self.cell_en.zero_state(batch_size_en, tf.float32)\n",
    "                self.h_state_en, self.final_state_en = tf.nn.dynamic_rnn(\n",
    "                    self.cell_en,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_en_init,\n",
    "                )\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de_init = self.final_state_en\n",
    "                self.h_state_de, self.final_state_de = tf.nn.dynamic_rnn(\n",
    "                    self.cell_de,\n",
    "                    self.sen_de_embedding,\n",
    "                    sequence_length=self.sen_de_length,\n",
    "                    initial_state=self.cell_de_init,\n",
    "                )\n",
    "            \n",
    "\n",
    "            with tf.variable_scope(\"softmax\") as scope:\n",
    "                W = tf.get_variable(\"W\", [self.state_size, len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))\n",
    "                b = tf.get_variable(\"b\", [len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))               \n",
    "            self.logits = tf.reshape(\n",
    "                tf.add(tf.matmul(tf.reshape(self.h_state_de, (-1, self.state_size)), W), b),\n",
    "                shape=(-1, batch_max_length_de, len(self.vocabulary))\n",
    "            )\n",
    "            self.prediction = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # construct loss and train op\n",
    "            self.cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.sen_en,\n",
    "                logits=self.logits\n",
    "            )        \n",
    "            #self.mask = tf.sign(tf.reduce_max(tf.abs(self.sen_de_embedding), 2))\n",
    "            self.mask = tf.sequence_mask(self.sen_de_length, maxlen=batch_max_length_de)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                #tf.reduce_sum(tf.multiply(self.cross_ent, self.mask), 1) / tf.reduce_sum(self.mask, 1)\n",
    "                tf.divide(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.where(\n",
    "                            self.mask,\n",
    "                            self.cross_ent,\n",
    "                            tf.zeros_like(self.cross_ent)\n",
    "                        ), 1\n",
    "                    ),\n",
    "                    tf.to_float(self.sen_de_length)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \"\"\"\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train = optimizer.minimize(self.loss)\n",
    "            \"\"\"\n",
    "            # Calculate and clip gradients\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "            # Optimization\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train = optimizer.apply_gradients(zip(self.clipped_gradients, params))\n",
    "            \n",
    "            # initializer\n",
    "            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph,\n",
    "                config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )           \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "    def train(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        assert batch_sen_en.shape[1] == self.n_max_length  # training always input same length as self.n_max_length\n",
    "        _, loss, prediction, sen_en_embedding, mask, cross_ent, clipped_gradients = self.sess.run(\n",
    "            [self.op_train, self.loss, self.prediction, self.sen_en_embedding, self.mask, self.cross_ent, self.clipped_gradients],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, prediction, sen_en_embedding, mask, cross_ent, clipped_gradients\n",
    "        \n",
    "    def predict(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        loss, prediction = self.sess.run(\n",
    "            [self.loss, self.prediction],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, prediction\n",
    "    \n",
    "    def encode(self, batch_sen_en, batch_sen_en_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        Returns\n",
    "        -------\n",
    "        #batch_state_en: numpy, shape=(n, self.state_size), dtype=float\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        \"\"\"\n",
    "        print(\"QQ\", batch_sen_en.shape)\n",
    "        print(\"QQ\", batch_sen_en.dtype)\n",
    "        batch_state_en = self.sess.run(\n",
    "            self.final_state_en,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return batch_state_en\n",
    "    \n",
    "    def decode(self, batch_state_en):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        Returns\n",
    "        -------\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        \"\"\"\n",
    "        batch_size = batch_state_en.c.shape[0]\n",
    "        batch_sen_de = np.empty([batch_size, self.n_max_length], dtype=np.int32)\n",
    "        \n",
    "        tmp_sen_de = np.empty([batch_size, 1], dtype=np.int32)\n",
    "        tmp_sen_de_length = np.ones([batch_size], dtype=np.int32)\n",
    "        tmp_sen_de[:] = self.vocabulary[\"<s>\"]\n",
    "        tmp_last_state = batch_state_en\n",
    "        for i in range(self.n_max_length):\n",
    "            tmp_predict, tmp_last_state = self.sess.run(\n",
    "                [self.prediction, self.final_state_de],\n",
    "                feed_dict={\n",
    "                    self.cell_de_init: tmp_last_state,\n",
    "                    self.sen_de: tmp_sen_de,\n",
    "                    self.sen_de_length: tmp_sen_de_length,\n",
    "                }\n",
    "            )\n",
    "            tmp_sen_de = np.argmax(tmp_predict, axis=2)\n",
    "            batch_sen_de[:,i] = tmp_sen_de[:,0]\n",
    "           \n",
    "        return batch_sen_de\n",
    "\n",
    "    \n",
    "def evaluate(batch_sen_en, batch_sen_en_length, batch_prediction, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "    batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "    batch_prediction: numpy, shape=(n, max_length, len(vocabulary))\n",
    "    \"\"\"\n",
    "    assert batch_sen_en.shape[0] == batch_prediction.shape[0]\n",
    "    acc_word = 0\n",
    "    acc_sen_end = 0\n",
    "    for i in range(batch_sen_en.shape[0]):\n",
    "        is_first_end = False\n",
    "        for j in range(batch_sen_en_length[i]):\n",
    "            cur_pred_word = np.argmax(batch_prediction[i, j])\n",
    "            if cur_pred_word == batch_sen_en[i, j]:\n",
    "                acc_word += 1\n",
    "                if not is_first_end and cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                    acc_sen_end += 1\n",
    "            if cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                is_first_end = True\n",
    "    return 1. * acc_word / np.sum(batch_sen_en_length), 1. * acc_sen_end / batch_sen_en.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 30802 n_valid 7700 n_test 9627\n",
      "[  0 201 201 201 201  52 201  28 201   1 201 201 201 201 201  49 201   6\n",
      "  73 201 201 159 203 200 200 200 200 200 200 200]\n",
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n",
      "['the', '<unk>', '<unk>', '<unk>', '<unk>', 'said', '<unk>', 'an', '<unk>', 'of', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'no', '<unk>', 'that', 'any', '<unk>', '<unk>', 'place', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[201 129 201 203 200 200 200 200 200 200 200 200 200 200 200 200 200 200\n",
      " 200 200 200 200 200 200 200 200 200 200 200 200]\n",
      "['ekstrohm', 'never', 'slept']\n",
      "['<unk>', 'never', '<unk>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "epoch 0 valid (0.49963278971811209, 0.6585714285714286)\n",
      "last loss 2.50658\n",
      "epoch 1 valid (0.56367858300032403, 0.7858441558441558)\n",
      "last loss 1.80427\n",
      "epoch 2 valid (0.6042877200561616, 0.8797402597402597)\n",
      "last loss 1.33824\n",
      "epoch 3 valid (0.64341721568203913, 0.8722077922077922)\n",
      "last loss 0.996659\n",
      "epoch 4 valid (0.66492061777729772, 0.8677922077922078)\n",
      "last loss 0.792531\n",
      "train (0.68330086346794427, 0.8226738523472502)\n",
      "test (0.6582716091329166, 0.8677677365742184)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(corpus_sents, max_length, extend_vocabulary):\n",
    "    sen_en = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_de = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_en_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_de_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "\n",
    "    def get_random_sequence(sent, max_length):\n",
    "        x = np.full((max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "        for i, word in enumerate(sent):\n",
    "            if word in extend_vocabulary:\n",
    "                x[i] = extend_vocabulary[word]\n",
    "            else:\n",
    "                x[i] = extend_vocabulary[\"<unk>\"]\n",
    "        return x\n",
    "\n",
    "    for i in range(len(corpus_sents)):\n",
    "        l = len(corpus_sents[i])\n",
    "        sen_en[i, :] = get_random_sequence(corpus_sents[i], max_length)\n",
    "        sen_en[i, l] = extend_vocabulary[\"</s>\"]\n",
    "        sen_de[i, 1:l+1] = sen_en[i, :l]\n",
    "        sen_de[i, 0] = extend_vocabulary[\"<s>\"]\n",
    "        sen_en_length[i] = l + 1\n",
    "        sen_de_length[i] = l + 1\n",
    "    \n",
    "    return sen_en, sen_de, sen_en_length, sen_de_length\n",
    "\n",
    "def get_total_accuracy(data_sen_en, data_sen_de, data_sen_en_length, data_sen_de_length, extend_vocabulary, pretrained_lstm):\n",
    "    n_hit_word, n_hit_length = 0, 0\n",
    "    n_total_word, n_total_length = 0, 0\n",
    "    cur_idx = 0\n",
    "    while cur_idx < data_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = data_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = data_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        _, predictions = pretrained_lstm.predict(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "        cur_acc_word, cur_acc_length = evaluate(batch_sen_en, batch_sen_en_length, predictions, extend_vocabulary)\n",
    "        n_hit_word += cur_acc_word * np.sum(batch_sen_en_length)\n",
    "        n_total_word += np.sum(batch_sen_en_length)\n",
    "        n_hit_length += cur_acc_length * batch_sen_en.shape[0]\n",
    "        n_total_length += batch_sen_en.shape[0]\n",
    "    return 1. * n_hit_word / n_total_word, 1. * n_hit_length / n_total_length\n",
    "    \n",
    "# hyperparameter\n",
    "vocabulary_size = 200\n",
    "origin_vocabulary = {}\n",
    "for word, n in corpus_counter.most_common(vocabulary_size):\n",
    "    origin_vocabulary[\"{}\".format(word)] = len(origin_vocabulary)\n",
    "extend_vocabulary = dict(origin_vocabulary)\n",
    "for w in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]:\n",
    "    extend_vocabulary[w] = len(extend_vocabulary)\n",
    "#vocabulary = {\"<pad>\": 0, \"<unk>\": 1, \"<s>\": 2, \"</s>\": 3, \"a\": 4, \"b\": 5}\n",
    "state_size=64\n",
    "n_max_length=30\n",
    "n_batch_size=100\n",
    "\n",
    "# generate training/testing data\n",
    "n_train = int(len(preprocess_sents) * 0.8 * 0.8)\n",
    "n_valid = int(len(preprocess_sents) * 0.8 * 0.2)\n",
    "n_test = len(preprocess_sents) - n_train - n_valid\n",
    "print(\"n_train\", n_train, \"n_valid\", n_valid, \"n_test\", n_test)\n",
    "train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length = generate_data(preprocess_sents[:n_train],\n",
    "                                                                                     n_max_length, extend_vocabulary)\n",
    "valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length = generate_data(preprocess_sents[n_train:n_train+n_valid],\n",
    "                                                                                     n_max_length, extend_vocabulary)\n",
    "test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length = generate_data(preprocess_sents[n_train+n_valid:],\n",
    "                                                                                 n_max_length, extend_vocabulary)\n",
    "reverse_extend_vocabulary = {v: k for k, v in extend_vocabulary.items()}\n",
    "print(train_sen_en[0])\n",
    "print(preprocess_sents[0])\n",
    "print([reverse_extend_vocabulary[i] for i in train_sen_en[0]])\n",
    "print(test_sen_en[0])\n",
    "print(preprocess_sents[n_train+n_valid])\n",
    "print([reverse_extend_vocabulary[i] for i in test_sen_en[0]])\n",
    "\n",
    "pretrained_lstm = EncoderDecoder(vocabulary=extend_vocabulary, state_size=state_size, n_max_length=n_max_length)\n",
    "\n",
    "for epoch in range(5):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = train_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = train_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent, clipped_gradients = pretrained_lstm.train(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"epoch\", epoch, \"valid\", get_total_accuracy(\n",
    "        valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "    )) \n",
    "    print(\"last loss\", loss)\n",
    "print(\"train\", get_total_accuracy(\n",
    "    train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "))\n",
    "print(\"test\", get_total_accuracy(\n",
    "    test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_sen_en.shape (2, 30)\n",
      "batch_sen_de.shape (2, 30)\n",
      "batch_sen_en_length.shape (2,)\n",
      "batch_sen_de_length.shape (2,)\n",
      "[201  11   8 104 203 200 200 200 200 200 200 200 200 200 200 200 200 200\n",
      " 200 200 200 200 200 200 200 200 200 200 200 200]\n",
      "5\n",
      "['<unk>', 'it', 'was', 'just', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "['<s>', '<unk>', 'it', 'was', 'just', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "int32\n",
      "(1, 30)\n",
      "QQ (1, 30)\n",
      "QQ int32\n",
      "batch_state_en.c.shape (1, 64)\n",
      "batch_state_en.h.shape (1, 64)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n",
      "b (1, 1, 204)\n",
      "a (1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[201,  11,   8, 201, 203, 203, 203, 203, 203, 203, 203, 203, 203,\n",
       "        203, 203, 203, 203, 203, 203, 203,  25, 203, 203, 203, 203, 203,\n",
       "         25, 203, 203, 203]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"batch_sen_en.shape\", batch_sen_en.shape)\n",
    "print(\"batch_sen_de.shape\", batch_sen_de.shape)\n",
    "print(\"batch_sen_en_length.shape\", batch_sen_en_length.shape)\n",
    "print(\"batch_sen_de_length.shape\", batch_sen_de_length.shape)\n",
    "print(batch_sen_en[0])\n",
    "print(batch_sen_en_length[0])\n",
    "print([reverse_extend_vocabulary[i] for i in batch_sen_en[0]])\n",
    "print([reverse_extend_vocabulary[i] for i in batch_sen_de[0]])\n",
    "print(batch_sen_en[0:1].dtype)\n",
    "print(batch_sen_en[0:1].shape)\n",
    "batch_state_en = pretrained_lstm.encode(batch_sen_en[0:1], batch_sen_en_length[0:1])\n",
    "print(\"batch_state_en.c.shape\", batch_state_en.c.shape)\n",
    "print(\"batch_state_en.h.shape\", batch_state_en.h.shape)\n",
    "pretrained_lstm.decode(batch_state_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([201,  11,   8, 201, 203, 180, 180, 180, 180, 180, 180, 180, 180,\n",
       "       180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180, 180,\n",
       "       180, 180, 180, 180])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202, 201,  11,   8, 104, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "       200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "       200, 200, 200, 200], dtype=int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sen_de[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 30)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sen_de.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([202,   9,  21, 201,   3, 201, 201,   3, 201,  14,  93,   3,   0,\n",
       "       201,   2,   9,   8, 201, 200, 200, 200, 200, 200, 200, 200, 200,\n",
       "       200, 200, 200, 200], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sen_de[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
