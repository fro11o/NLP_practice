{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_pos) 12500\n",
      "len(train_neg) 12500\n",
      "len(test_pos) 12500\n",
      "len(test_neg) 12500\n",
      "len(unlabeled_data) 50000\n",
      "train_pos[0] ['``', 'The', 'Last', 'Hard', 'Men', \"''\", 'is', 'a', 'typical', 'western', 'for', 'the', '70', \"'s\", '.', 'Most', 'of', 'them', 'seem', 'to', 'be', 'inspired', 'by', 'Sam', 'Peckinpah', '.', 'Also', 'this', 'one', ',', 'but', 'Director', 'Andrew', 'McLaglan', 'is', 'a', 'John', 'Ford', 'Pupil', 'and', 'this', 'can', 'be', 'obviously', 'shown', 'in', 'many', 'scenes', '.', 'IMO', 'the', 'beginning', 'is', 'very', 'good', '.', 'In', 'a', 'certain', 'way', 'McLaglan', 'wanted', 'to', 'show', 'the', 'audience', 'a', 'travel', 'from', 'the', 'civilization', 'to', 'the', 'wilderness', '.', 'In', 'the', 'third', 'part', 'there', 'are', 'some', 'illogical', 'flaws', 'and', 'I', 'complain', 'a', 'bit', 'about', 'Charlton', 'Heston', '.', 'He', 'has', 'to', 'play', 'an', 'old', 'ex-lawman', 'named', 'Sam', 'Burgade', 'but', 'he', 'is', 'in', 'a', 'fantastic', 'physical', 'shape', '.', 'I', 'never', 'got', 'the', 'feeling', 'that', 'he', 'really', 'has', 'problems', 'to', 'climb', 'on', 'a', 'horse', 'or', 'on', 'a', 'rock', '.', 'For', 'me', 'he', 'did', \"n't\", 'looks', 'very', 'motivated', 'as', 'he', 'usual', 'do', 'in', 'most', 'of', 'his', 'epic', 'movies', '.', 'Same', 'goes', 'to', 'the', 'beautiful', 'Barbara', 'Hershey', 'who', 'is', 'playing', 'the', 'sheriff', \"'s\", 'daughter', '.', 'Maybe', 'both', 'had', 'troubles', 'with', 'the', 'director', 'or', 'were', 'unhappy', 'with', 'their', 'roles', '.', 'Hershey', 'and', 'Coburn', 'are', 'not', 'showing', 'their', 'best', 'but', 'they', 'are', 'still', 'good', '.', 'If', 'the', 'scriptwriter', 'had', 'John', 'Wayne', 'in', 'their', 'mind', 'as', 'Sam', 'Burgade', '?', 'Also', 'Michael', 'Parks', 'as', 'modern', 'sheriff', 'is', 'a', 'bit', 'underused', 'in', 'his', 'role', '.', 'On', 'the', 'other', 'Hand', 'there', 'is', 'James', 'Coburn', 'as', 'outlaw', 'Zach', 'Provo', '.', 'Coburn', 'is', 'a', 'really', 'great', 'villain', 'in', 'this', 'one', '.', 'He', 'is', 'portraying', 'the', 'bad', 'guy', 'between', 'maniac', 'hate', 'and', 'cleverness', '.', 'His', 'role', 'and', 'his', 'acting', 'is', 'the', 'best', 'of', 'the', 'movie.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Landscapes', 'and', 'Shootouts', 'are', 'terrific', '.', 'The', 'shootings', 'scenes', 'are', 'bloody', 'and', 'the', 'violence', 'looks', 'realistic', '.', 'Zach', 'Provo', 'and', 'his', 'gang', 'had', 'some', 'gory', 'and', 'violent', 'scenes', '.', 'What', 'I', 'miss', 'is', 'the', 'typical', 'western', 'action', 'in', 'the', 'middle', 'of', 'the', 'movie', '.', 'I', 'would', 'have', 'appreciated', 'a', 'bank', 'robbery', 'or', 'something', 'similar', '.', 'Overall', 'it', \"'s\", 'an', 'entertaining', 'western', 'flick', '.', 'Not', 'a', 'great', 'movie', 'but', 'above', 'the', 'average', 'because', 'of', 'a', 'great', 'Coburn', ',', 'a', 'very', 'good', 'beginning', 'and', 'some', 'gory', 'and', 'violent', 'scenes', '.']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_sent_in_directory(path_directory):\n",
    "    sents = []\n",
    "    for filename in os.listdir(path_directory):\n",
    "        if os.path.isfile(os.path.join(path_directory, filename)):\n",
    "            with open(os.path.join(path_directory, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    sents.append(nltk.tokenize.word_tokenize(line))\n",
    "    return sents\n",
    "                    \n",
    "                \n",
    "train_pos = get_sent_in_directory(\"./aclImdb/train/pos\")\n",
    "train_neg = get_sent_in_directory(\"./aclImdb/train/neg\")\n",
    "test_pos = get_sent_in_directory(\"./aclImdb/test/pos\")\n",
    "test_neg = get_sent_in_directory(\"./aclImdb/test/neg\")\n",
    "unlabeled_data = get_sent_in_directory(\"./aclImdb/train/unsup\")\n",
    "\n",
    "print(\"len(train_pos)\", len(train_pos))\n",
    "print(\"len(train_neg)\", len(train_neg))\n",
    "print(\"len(test_pos)\", len(test_pos))\n",
    "print(\"len(test_neg)\", len(test_neg))\n",
    "print(\"len(unlabeled_data)\", len(unlabeled_data))\n",
    "print(\"train_pos[0]\", train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique word: 251952\n",
      "# of sents: 25000\n",
      "max len(sents[i]): 2502\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGSUAoCISEYpAIiIZNT/BxT2MyCQ1jP1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd890889fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_counter = collections.Counter()\n",
    "train_preprocess = []\n",
    "test_preprocess = []\n",
    "unlabeled_preprocess = []\n",
    "for sent in train_pos:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    train_preprocess.append((res, 1))\n",
    "for sent in train_neg:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    train_preprocess.append((res, 0))\n",
    "for sent in test_pos:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    test_preprocess.append((res, 1))\n",
    "for sent in test_neg:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    test_preprocess.append((res, 0))\n",
    "for sent in unlabeled_data:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    unlabeled_preprocess.append(res)\n",
    "\n",
    "print(\"unique word:\", len(corpus_counter))\n",
    "print(\"# of sents:\", len(train_preprocess))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s, _ in train_preprocess]))\n",
    "\n",
    "train_length = []\n",
    "for sent, label in train_preprocess:\n",
    "    train_length.append(len(sent))\n",
    "train_length = sorted(train_length)\n",
    "stat_x, stat_y = [], []\n",
    "for i, l in enumerate(train_length):\n",
    "    stat_x.append(i / len(train_length))\n",
    "    stat_y.append(l)\n",
    "plt.plot(stat_x, stat_y, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Brown Corpus to train an seq2seq autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, vocabulary={}, label={}, state_size=64, n_max_length=30):     \n",
    "        self.state_size = state_size\n",
    "        self.n_max_length = n_max_length\n",
    "        self.vocabulary = vocabulary\n",
    "        self.label = label\n",
    "        self.reverse_vocabulary = {k: v for k, v in vocabulary.items()}\n",
    "        \n",
    "        ######################\n",
    "        # Graph Construction #\n",
    "        ######################\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #self.sen_en = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_en\")\n",
    "            #self.sen_de = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_de\")\n",
    "            self.sen_en = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_en\")\n",
    "            self.sen_de = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_de\")\n",
    "            self.sen_en_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_length\")\n",
    "            self.sen_de_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_de_length\")\n",
    "            self.sen_en_label = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_label\")\n",
    "                        \n",
    "            batch_size_en = tf.shape(self.sen_en)[0]\n",
    "            batch_size_de = tf.shape(self.sen_de)[0]\n",
    "            batch_max_length_de = tf.shape(self.sen_de)[1]\n",
    "            \n",
    "            # TODO sen_en_embedding could also be self-trained embedding: embedding_lookup\n",
    "            self.embedding = tf.Variable(tf.random_uniform([len(self.vocabulary), self.state_size], -1.0, 1.0), dtype=tf.float32)\n",
    "            #self.sen_en_embedding = tf.one_hot(self.sen_en, len(self.vocabulary))\n",
    "            #self.sen_de_embedding = tf.one_hot(self.sen_de, len(self.vocabulary))\n",
    "            self.sen_en_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_en)\n",
    "            self.sen_de_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_de)\n",
    "            \n",
    "            # build encoder decoder structure\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en_init = self.cell_en.zero_state(batch_size_en, tf.float32)\n",
    "                self.h_state_en, self.final_state_en = tf.nn.dynamic_rnn(\n",
    "                    self.cell_en,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_en_init,\n",
    "                )\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de_init = self.final_state_en\n",
    "                self.h_state_de, self.final_state_de = tf.nn.dynamic_rnn(\n",
    "                    self.cell_de,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_de_init,\n",
    "                )\n",
    "            \n",
    "            # autoencoder softmax\n",
    "            with tf.variable_scope(\"softmax\") as scope:\n",
    "                W = tf.get_variable(\"W\", [self.state_size, len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))\n",
    "                b = tf.get_variable(\"b\", [len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))               \n",
    "            self.logits = tf.reshape(\n",
    "                tf.add(tf.matmul(tf.reshape(self.h_state_de, (-1, self.state_size)), W), b),\n",
    "                shape=(-1, batch_max_length_de, len(self.vocabulary))\n",
    "            )\n",
    "            self.pred_sa = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # construct loss and train op\n",
    "            self.cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.sen_de,\n",
    "                logits=self.logits\n",
    "            )        \n",
    "            self.mask = tf.sequence_mask(self.sen_de_length, maxlen=batch_max_length_de)\n",
    "            self.loss_sa = tf.reduce_mean(\n",
    "                tf.divide(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.where(\n",
    "                            self.mask,\n",
    "                            self.cross_ent,\n",
    "                            tf.zeros_like(self.cross_ent)\n",
    "                        ), 1\n",
    "                    ),\n",
    "                    tf.to_float(self.sen_de_length)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # classifier layer\n",
    "            with tf.variable_scope(\"classifier\") as scope:\n",
    "                clf_input = tf.concat([self.final_state_en.h, self.final_state_en.c], axis=1)\n",
    "                clf_dense = tf.layers.dense(clf_input, 30, activation=tf.nn.relu)\n",
    "                self.clf_output = tf.layers.dense(clf_dense, len(self.label))\n",
    "            \n",
    "            self.pred_clf = tf.nn.softmax(self.clf_output)\n",
    "                   \n",
    "            # construct classifier loss\n",
    "            self.loss_clf = tf.reduce_sum(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.sen_en_label,\n",
    "                    logits=self.clf_output,\n",
    "                )\n",
    "            )        \n",
    "            \n",
    "            # Optimization\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train_sa = optimizer.minimize(self.loss_sa)\n",
    "            self.op_train_clf = optimizer.minimize(self.loss_clf)\n",
    "               \n",
    "            # initializer\n",
    "            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph,\n",
    "                config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )           \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "    def train_sa(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        assert batch_sen_en.shape[1] == self.n_max_length  # training always input same length as self.n_max_length\n",
    "        _, loss, pred, sen_en_embedding, mask, cross_ent = self.sess.run(\n",
    "            [self.op_train_sa, self.loss_sa, self.pred_sa, self.sen_en_embedding, self.mask, self.cross_ent],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, pred, sen_en_embedding, mask, cross_ent\n",
    "        \n",
    "    def predict_sa(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        loss, pred = self.sess.run(\n",
    "            [self.loss_sa, self.pred_sa],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, pred\n",
    "    \n",
    "    def encode_sa(self, batch_sen_en, batch_sen_en_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        Returns\n",
    "        -------\n",
    "        #batch_state_en: numpy, shape=(n, self.state_size), dtype=float\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        \"\"\"\n",
    "        batch_state_en = self.sess.run(\n",
    "            self.final_state_en,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return batch_state_en\n",
    "    \n",
    "    def decode_sa(self, batch_state_en):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        Returns\n",
    "        -------\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        \"\"\"\n",
    "        batch_size = batch_state_en.c.shape[0]\n",
    "        batch_sen_de = np.empty([batch_size, self.n_max_length], dtype=np.int32)\n",
    "        \n",
    "        tmp_sen_de = np.empty([batch_size, 1], dtype=np.int32)\n",
    "        tmp_sen_de_length = np.ones([batch_size], dtype=np.int32)\n",
    "        tmp_sen_de[:] = self.vocabulary[\"<s>\"]\n",
    "        tmp_last_state = batch_state_en\n",
    "        for i in range(self.n_max_length):\n",
    "            tmp_predict, tmp_last_state = self.sess.run(\n",
    "                [self.pred_sa, self.final_state_de],\n",
    "                feed_dict={\n",
    "                    self.cell_de_init: tmp_last_state,\n",
    "                    self.sen_de: tmp_sen_de,\n",
    "                    self.sen_de_length: tmp_sen_de_length,\n",
    "                }\n",
    "            )\n",
    "            tmp_sen_de = np.argmax(tmp_predict, axis=2)\n",
    "            batch_sen_de[:,i] = tmp_sen_de[:,0]\n",
    "           \n",
    "        return batch_sen_de\n",
    "    \n",
    "    def train_clf(self, batch_sen_en, batch_sen_en_length, batch_label):\n",
    "        _, loss, pred = self.sess.run(\n",
    "            [self.op_train_clf, self.loss_clf, self.pred_clf],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_en_label: batch_label\n",
    "            }\n",
    "        )\n",
    "        return loss, pred\n",
    "    \n",
    "    def predict_clf(self, batch_sen_en, batch_sen_en_length):\n",
    "        pred = self.sess.run(\n",
    "            self.pred_clf,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    \n",
    "def evaluate_sa(batch_sen_en, batch_sen_en_length, batch_prediction, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "    batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "    batch_prediction: numpy, shape=(n, max_length, len(vocabulary))\n",
    "    \"\"\"\n",
    "    assert batch_sen_en.shape[0] == batch_prediction.shape[0]\n",
    "    acc_word = 0\n",
    "    acc_sen_end = 0\n",
    "    for i in range(batch_sen_en.shape[0]):\n",
    "        is_first_end = False\n",
    "        for j in range(batch_sen_en_length[i]):\n",
    "            cur_pred_word = np.argmax(batch_prediction[i, j])\n",
    "            if cur_pred_word == batch_sen_en[i, j]:\n",
    "                acc_word += 1\n",
    "                if not is_first_end and cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                    acc_sen_end += 1\n",
    "            if cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                is_first_end = True\n",
    "    return 1. * acc_word / np.sum(batch_sen_en_length), 1. * acc_sen_end / batch_sen_en.shape[0]\n",
    "\n",
    "\n",
    "def evaluate_clf(batch_pred, batch_label):\n",
    "    assert batch_pred.shape[0] == batch_label.shape[0]\n",
    "    hit, tot = 0, 0\n",
    "    for i in range(batch_label.shape[0]):\n",
    "        if np.argmax(batch_pred[i]) == batch_label[i]:\n",
    "            hit += 1\n",
    "        tot += 1\n",
    "    return 1. * hit / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 20000 n_valid 5000 n_test 25000\n",
      "last loss 5.48966\n",
      "valid_acc (0.17257993986950426, 0.244)\n",
      "epoch 0 train_acc (0.17395700799323882, 0.24785) valid_acc (0.17257993986950426, 0.244) test_acc (0.17355747975761041, 0.24284)\n",
      "best_epoch (valid) 0\n",
      "last loss 5.1919\n",
      "valid_acc (0.19414705432395263, 0.2284)\n",
      "epoch 1 train_acc (0.19669924363361013, 0.2267) valid_acc (0.19414705432395263, 0.2284) test_acc (0.19532524103419721, 0.22228)\n",
      "best_epoch (valid) 1\n",
      "last loss 4.99944\n",
      "valid_acc (0.20813548937305223, 0.2982)\n",
      "epoch 2 train_acc (0.21123157778928459, 0.304) valid_acc (0.20813548937305223, 0.2982) test_acc (0.20854693626050239, 0.30044)\n",
      "best_epoch (valid) 2\n",
      "last loss 4.8717\n",
      "valid_acc (0.21736129185736519, 0.6044)\n",
      "epoch 3 train_acc (0.22157774132361299, 0.60805) valid_acc (0.21736129185736519, 0.6044) test_acc (0.21797130208681445, 0.5928)\n",
      "best_epoch (valid) 3\n",
      "last loss 4.77074\n",
      "valid_acc (0.22339393142770431, 0.6884)\n",
      "epoch 4 train_acc (0.2285969338452562, 0.7039) valid_acc (0.22339393142770431, 0.6884) test_acc (0.22378076073235556, 0.67956)\n",
      "best_epoch (valid) 4\n",
      "last loss 4.67312\n",
      "valid_acc (0.22958363668705142, 0.7012)\n",
      "epoch 5 train_acc (0.2353449583655067, 0.71135) valid_acc (0.22958363668705142, 0.7012) test_acc (0.22981297417409569, 0.70652)\n",
      "best_epoch (valid) 5\n",
      "last loss 4.6047\n",
      "valid_acc (0.23487757367745771, 0.833)\n",
      "epoch 6 train_acc (0.24157284285965647, 0.8451) valid_acc (0.23487757367745771, 0.833) test_acc (0.23505757890545978, 0.8358)\n",
      "best_epoch (valid) 6\n",
      "last loss 4.54929\n",
      "valid_acc (0.23787195997897156, 0.9626)\n",
      "epoch 7 train_acc (0.24546588163828548, 0.96905) valid_acc (0.23787195997897156, 0.9626) test_acc (0.2380118180100266, 0.96244)\n",
      "best_epoch (valid) 7\n",
      "last loss 4.4971\n",
      "valid_acc (0.23980541924447074, 0.992)\n",
      "epoch 8 train_acc (0.24855083417172727, 0.99535) valid_acc (0.23980541924447074, 0.992) test_acc (0.24014253373175332, 0.99256)\n",
      "best_epoch (valid) 8\n",
      "last loss 4.45099\n",
      "valid_acc (0.2426554100434212, 0.9934)\n",
      "epoch 9 train_acc (0.25225824172062133, 0.99675) valid_acc (0.2426554100434212, 0.9934) test_acc (0.24264435016331345, 0.99468)\n",
      "best_epoch (valid) 9\n",
      "last loss 4.41093\n",
      "valid_acc (0.24482947268217381, 0.9946)\n",
      "epoch 10 train_acc (0.2552867189511468, 0.997) valid_acc (0.24482947268217381, 0.9946) test_acc (0.24492141487491481, 0.99568)\n",
      "best_epoch (valid) 10\n",
      "last loss 4.37361\n",
      "valid_acc (0.24844733183740836, 0.998)\n",
      "epoch 11 train_acc (0.25939927004651714, 0.99915) valid_acc (0.24844733183740836, 0.998) test_acc (0.24826173664713636, 0.99788)\n",
      "best_epoch (valid) 11\n",
      "last loss 4.31885\n",
      "valid_acc (0.25202971887927372, 0.997)\n",
      "epoch 12 train_acc (0.26373859182940901, 0.99885) valid_acc (0.25202971887927372, 0.997) test_acc (0.25173373876650712, 0.9976)\n",
      "best_epoch (valid) 12\n",
      "last loss 4.29211\n",
      "valid_acc (0.25492149605573705, 0.9978)\n",
      "epoch 13 train_acc (0.26759237322504881, 0.9996) valid_acc (0.25492149605573705, 0.9978) test_acc (0.25454955520700034, 0.99816)\n",
      "best_epoch (valid) 13\n",
      "last loss 4.25343\n",
      "valid_acc (0.25749525153005604, 0.998)\n",
      "epoch 14 train_acc (0.27098830304966615, 0.9995) valid_acc (0.25749525153005604, 0.998) test_acc (0.25721110560426008, 0.99832)\n",
      "best_epoch (valid) 14\n",
      "last loss 4.21353\n",
      "valid_acc (0.25953758123874165, 0.9976)\n",
      "epoch 15 train_acc (0.27351340463755303, 0.9991) valid_acc (0.25953758123874165, 0.9976) test_acc (0.25911751660660642, 0.998)\n",
      "best_epoch (valid) 15\n",
      "last loss 4.16963\n",
      "valid_acc (0.26008585411091767, 0.9966)\n",
      "epoch 16 train_acc (0.2745029107307016, 0.9984) valid_acc (0.26008585411091767, 0.9966) test_acc (0.25942490435569199, 0.99772)\n",
      "best_epoch (valid) 16\n",
      "last loss 4.13179\n",
      "valid_acc (0.26341290942244339, 0.9976)\n",
      "epoch 17 train_acc (0.27891123805687007, 0.9985) valid_acc (0.26341290942244339, 0.9976) test_acc (0.26298027064154944, 0.99788)\n",
      "best_epoch (valid) 17\n",
      "last loss 4.09752\n",
      "valid_acc (0.26651821446543816, 0.9984)\n",
      "epoch 18 train_acc (0.28289312013869677, 0.9993) valid_acc (0.26651821446543816, 0.9984) test_acc (0.26608786460116435, 0.99888)\n",
      "best_epoch (valid) 18\n",
      "last loss 4.07156\n",
      "valid_acc (0.26707266803984925, 0.9978)\n",
      "epoch 19 train_acc (0.28440160949136473, 0.9992) valid_acc (0.26707266803984925, 0.9978) test_acc (0.26678552896129387, 0.9982)\n",
      "best_epoch (valid) 19\n",
      "last loss 4.04887\n",
      "valid_acc (0.26757911604118056, 0.9992)\n",
      "epoch 20 train_acc (0.28539316399884657, 0.9999) valid_acc (0.26757911604118056, 0.9992) test_acc (0.26732476642235048, 0.99952)\n",
      "best_epoch (valid) 20\n",
      "last loss 4.02347\n",
      "valid_acc (0.2687152520232266, 0.9994)\n",
      "epoch 21 train_acc (0.28687028463266806, 0.9999) valid_acc (0.2687152520232266, 0.9994) test_acc (0.26806599169068746, 0.99948)\n",
      "best_epoch (valid) 21\n",
      "last loss 3.98431\n",
      "valid_acc (0.27141662726649213, 0.9994)\n",
      "epoch 22 train_acc (0.29050717514972779, 1.0) valid_acc (0.27141662726649213, 0.9994) test_acc (0.27064451125506594, 0.99984)\n",
      "best_epoch (valid) 22\n",
      "last loss 3.94984\n",
      "valid_acc (0.2745323757963512, 0.9992)\n",
      "epoch 23 train_acc (0.29431705926749008, 0.99995) valid_acc (0.2745323757963512, 0.9992) test_acc (0.27356265108173011, 0.99956)\n",
      "best_epoch (valid) 23\n",
      "last loss 3.92785\n",
      "valid_acc (0.27415781883198437, 0.9996)\n",
      "best_epoch (valid) 23\n",
      "last loss 3.90677\n",
      "valid_acc (0.27473955586843868, 0.9992)\n",
      "epoch 25 train_acc (0.29601006151234055, 0.9999) valid_acc (0.27473955586843868, 0.9992) test_acc (0.27392787575011907, 0.99976)\n",
      "best_epoch (valid) 25\n",
      "last loss 3.87577\n",
      "valid_acc (0.27812516704215867, 0.9996)\n",
      "epoch 26 train_acc (0.29996992983437482, 0.9998) valid_acc (0.27812516704215867, 0.9996) test_acc (0.277153743334145, 0.99972)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.85367\n",
      "valid_acc (0.27864407900711674, 0.9976)\n",
      "epoch 27 train_acc (0.30140265111452141, 0.9985) valid_acc (0.27864407900711674, 0.9976) test_acc (0.27784683248019876, 0.99788)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.87101\n",
      "valid_acc (0.27855416869088701, 0.9998)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.81347\n",
      "valid_acc (0.28051897956531396, 0.9996)\n",
      "epoch 29 train_acc (0.30474578780204975, 0.99995) valid_acc (0.28051897956531396, 0.9996) test_acc (0.27975198486203928, 0.99952)\n",
      "best_epoch (valid) 29\n",
      "last loss 3.79315\n",
      "valid_acc (0.28237088248603848, 0.9988)\n",
      "epoch 30 train_acc (0.30750613186098724, 0.99865) valid_acc (0.28237088248603848, 0.9988) test_acc (0.28160388719476204, 0.99852)\n",
      "best_epoch (valid) 30\n",
      "last loss 3.77494\n",
      "valid_acc (0.28263033620757361, 0.9986)\n",
      "epoch 31 train_acc (0.30863511687932643, 0.99795) valid_acc (0.28263033620757361, 0.9986) test_acc (0.28182277264542061, 0.99752)\n",
      "best_epoch (valid) 31\n",
      "last loss 3.77836\n",
      "valid_acc (0.28227468591675331, 1.0)\n",
      "best_epoch (valid) 31\n",
      "last loss 3.7366\n",
      "valid_acc (0.28261576129338878, 0.9998)\n",
      "best_epoch (valid) 31\n",
      "last loss 3.72419\n",
      "valid_acc (0.28371641667549435, 0.9994)\n",
      "epoch 34 train_acc (0.31141900798625372, 0.99965) valid_acc (0.28371641667549435, 0.9994) test_acc (0.28292668738770188, 0.99936)\n",
      "best_epoch (valid) 34\n",
      "last loss 3.70426\n",
      "valid_acc (0.28445927790730846, 0.998)\n",
      "epoch 35 train_acc (0.31330645419485875, 0.99865) valid_acc (0.28445927790730846, 0.998) test_acc (0.2837694512063626, 0.99812)\n",
      "best_epoch (valid) 35\n",
      "last loss 3.69273\n",
      "valid_acc (0.2843880960741047, 0.9982)\n",
      "best_epoch (valid) 35\n",
      "last loss 3.67997\n",
      "valid_acc (0.28482539676226659, 0.9998)\n",
      "epoch 37 train_acc (0.31443907889299655, 0.99985) valid_acc (0.28482539676226659, 0.9998) test_acc (0.28417865333873377, 0.99944)\n",
      "best_epoch (valid) 37\n",
      "last loss 3.65732\n",
      "valid_acc (0.28623357980683684, 0.9974)\n",
      "epoch 38 train_acc (0.31728408366039285, 0.99695) valid_acc (0.28623357980683684, 0.9974) test_acc (0.28586120431616557, 0.99728)\n",
      "best_epoch (valid) 38\n",
      "last loss 3.64776\n",
      "valid_acc (0.28617508656286417, 1.0)\n",
      "best_epoch (valid) 38\n",
      "last loss 3.64008\n",
      "valid_acc (0.2859992967287654, 0.9998)\n",
      "best_epoch (valid) 38\n",
      "last loss 3.64108\n",
      "valid_acc (0.28362647056550139, 0.9984)\n",
      "best_epoch (valid) 38\n",
      "last loss 3.60119\n",
      "valid_acc (0.28685506895763341, 1.0)\n",
      "epoch 42 train_acc (0.32018391147138148, 0.99995) valid_acc (0.28685506895763341, 1.0) test_acc (0.28637648696319723, 0.99952)\n",
      "best_epoch (valid) 42\n",
      "last loss 3.58865\n",
      "valid_acc (0.28803738194013623, 0.9912)\n",
      "epoch 43 train_acc (0.32185758867646269, 0.99245) valid_acc (0.28803738194013623, 0.9912) test_acc (0.28738652929551961, 0.99216)\n",
      "best_epoch (valid) 43\n",
      "last loss 3.57056\n",
      "valid_acc (0.28840986470033514, 0.9992)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 44 train_acc (0.32299186074814268, 0.9996) valid_acc (0.28840986470033514, 0.9992) test_acc (0.28792609663589919, 0.99916)\n",
      "best_epoch (valid) 44\n",
      "last loss 3.55716\n",
      "valid_acc (0.28722127861766522, 0.9934)\n",
      "best_epoch (valid) 44\n",
      "last loss 3.55036\n",
      "valid_acc (0.28818806621388149, 0.9958)\n",
      "best_epoch (valid) 44\n",
      "last loss 3.56075\n",
      "valid_acc (0.28527533356817908, 0.9986)\n",
      "best_epoch (valid) 44\n",
      "last loss 3.52526\n",
      "valid_acc (0.28747028836417587, 0.9928)\n",
      "best_epoch (valid) 44\n",
      "last loss 3.52158\n",
      "valid_acc (0.28857728112603215, 0.9894)\n",
      "epoch 49 train_acc (0.32580706899870948, 0.9896) valid_acc (0.28857728112603215, 0.9894) test_acc (0.28803053383740596, 0.98816)\n",
      "best_epoch (valid) 49\n",
      "last loss 3.51565\n",
      "valid_acc (0.2892468244840139, 0.9994)\n",
      "epoch 50 train_acc (0.32673796825825518, 0.9998) valid_acc (0.2892468244840139, 0.9994) test_acc (0.28834120769784949, 0.99916)\n",
      "best_epoch (valid) 50\n",
      "last loss 3.50417\n",
      "valid_acc (0.28892884999535295, 0.9852)\n",
      "best_epoch (valid) 50\n",
      "last loss 3.48717\n",
      "valid_acc (0.28953993261201377, 0.9918)\n",
      "epoch 52 train_acc (0.32807556177889713, 0.993) valid_acc (0.28953993261201377, 0.9918) test_acc (0.28871225935531047, 0.99164)\n",
      "best_epoch (valid) 52\n",
      "last loss 3.50103\n",
      "valid_acc (0.28839736534828186, 0.9982)\n",
      "best_epoch (valid) 52\n",
      "last loss 3.48299\n",
      "valid_acc (0.29056310887644821, 0.9994)\n",
      "epoch 54 train_acc (0.32899445332192734, 0.9999) valid_acc (0.29056310887644821, 0.9994) test_acc (0.28945229513936127, 0.99956)\n",
      "best_epoch (valid) 54\n",
      "last loss 3.45746\n",
      "valid_acc (0.29028694688403817, 0.9966)\n",
      "best_epoch (valid) 54\n",
      "last loss 3.44698\n",
      "valid_acc (0.29024721661792879, 0.9992)\n",
      "best_epoch (valid) 54\n",
      "last loss 3.44479\n",
      "valid_acc (0.29025766515128604, 0.9992)\n",
      "best_epoch (valid) 54\n",
      "last loss 3.43143\n",
      "valid_acc (0.29067609901223185, 0.9898)\n",
      "epoch 58 train_acc (0.3318598375475677, 0.99155) valid_acc (0.29067609901223185, 0.9898) test_acc (0.28957637569759326, 0.98992)\n",
      "best_epoch (valid) 58\n",
      "last loss 3.43911\n",
      "valid_acc (0.28903570019114166, 0.9986)\n",
      "best_epoch (valid) 58\n",
      "last loss 3.41227\n",
      "valid_acc (0.29114900346935818, 0.999)\n",
      "epoch 60 train_acc (0.33310960920936583, 0.99965) valid_acc (0.29114900346935818, 0.999) test_acc (0.29026068357694468, 0.9984)\n",
      "best_epoch (valid) 60\n",
      "last loss 3.40531\n",
      "valid_acc (0.29173496140612554, 0.998)\n",
      "epoch 61 train_acc (0.33457003100056998, 0.99885) valid_acc (0.29173496140612554, 0.998) test_acc (0.2907855968147256, 0.99684)\n",
      "best_epoch (valid) 61\n",
      "last loss 3.41477\n",
      "valid_acc (0.29120548983113309, 0.9886)\n",
      "best_epoch (valid) 61\n",
      "last loss 3.39642\n",
      "valid_acc (0.29074522423457427, 0.9988)\n",
      "best_epoch (valid) 61\n",
      "last loss 3.40844\n",
      "valid_acc (0.29010480345649137, 0.9986)\n",
      "best_epoch (valid) 61\n",
      "last loss 3.37668\n",
      "valid_acc (0.29249441803152076, 0.998)\n",
      "epoch 65 train_acc (0.33694667911313858, 0.99965) valid_acc (0.29249441803152076, 0.998) test_acc (0.29128916250660453, 0.99824)\n",
      "best_epoch (valid) 65\n",
      "last loss 3.36663\n",
      "valid_acc (0.29262624405552451, 0.9988)\n",
      "epoch 66 train_acc (0.33772186101908741, 0.9998) valid_acc (0.29262624405552451, 0.9988) test_acc (0.29140068719841722, 0.999)\n",
      "best_epoch (valid) 66\n",
      "last loss 3.38597\n",
      "valid_acc (0.29278528749212357, 0.997)\n",
      "epoch 67 train_acc (0.33806740593970575, 0.99855) valid_acc (0.29278528749212357, 0.997) test_acc (0.29146522030811084, 0.997)\n",
      "best_epoch (valid) 67\n",
      "last loss 3.35955\n",
      "valid_acc (0.29310125465261555, 0.9938)\n",
      "epoch 68 train_acc (0.33947235998806152, 0.99545) valid_acc (0.29310125465261555, 0.9938) test_acc (0.29221659080886619, 0.99408)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.35997\n",
      "valid_acc (0.29279361246425473, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.35117\n",
      "valid_acc (0.29178514590582666, 0.9974)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.35261\n",
      "valid_acc (0.29148795084227902, 0.9978)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.34419\n",
      "valid_acc (0.29133306293082517, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.33515\n",
      "valid_acc (0.29031194669994365, 0.9964)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.31915\n",
      "valid_acc (0.29184991372560598, 0.9986)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.32204\n",
      "valid_acc (0.29127245873239077, 0.9974)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.31986\n",
      "valid_acc (0.29164069729729708, 0.9988)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.30709\n",
      "valid_acc (0.29252583062540566, 0.9976)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.325\n",
      "valid_acc (0.29065309542257817, 0.9974)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.30642\n",
      "valid_acc (0.29067608558156932, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.29587\n",
      "valid_acc (0.29144194243022797, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.2828\n",
      "valid_acc (0.29200479543580588, 0.9972)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.28512\n",
      "valid_acc (0.29176209230727151, 0.9974)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.27969\n",
      "valid_acc (0.29085187852458577, 0.9992)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.29008\n",
      "valid_acc (0.29016964599904066, 0.9974)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.27013\n",
      "valid_acc (0.29140433316133196, 0.9986)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.26172\n",
      "valid_acc (0.29186876219713226, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.25372\n",
      "valid_acc (0.29098366578437546, 0.9976)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.25315\n",
      "valid_acc (0.29047731865212523, 0.9978)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.24444\n",
      "valid_acc (0.29166791628186234, 0.9986)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.24382\n",
      "valid_acc (0.29119503853735207, 0.9924)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.23708\n",
      "valid_acc (0.29020531170165692, 0.997)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.26396\n",
      "valid_acc (0.28943521241067527, 0.9956)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.26101\n",
      "valid_acc (0.28983700394905254, 0.9986)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.22661\n",
      "valid_acc (0.29017388858246929, 0.9994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.22166\n",
      "valid_acc (0.29035597352367326, 0.9968)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.23111\n",
      "valid_acc (0.29051707122988302, 0.9988)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.21721\n",
      "valid_acc (0.29024920145463423, 0.9942)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.21639\n",
      "valid_acc (0.29100881616242535, 0.996)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.21781\n",
      "valid_acc (0.29028477996024626, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.22324\n",
      "valid_acc (0.28798519300175424, 0.9922)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.21185\n",
      "valid_acc (0.29014463064180945, 0.996)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.20497\n",
      "valid_acc (0.28911084474674265, 0.9966)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.22508\n",
      "valid_acc (0.28610400688727028, 0.9964)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.18473\n",
      "valid_acc (0.29036016770970996, 0.9964)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.19411\n",
      "valid_acc (0.28937460453992536, 0.9934)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.18916\n",
      "valid_acc (0.28763996853850116, 0.9944)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.19778\n",
      "valid_acc (0.28897914053167517, 0.9946)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.21761\n",
      "valid_acc (0.28545738478570509, 0.9976)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.17055\n",
      "valid_acc (0.28936202434012015, 0.997)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.16741\n",
      "valid_acc (0.28932013872731382, 0.9952)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.16465\n",
      "valid_acc (0.28941424415165229, 0.9944)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.16407\n",
      "valid_acc (0.28915483563017946, 0.987)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.17235\n",
      "valid_acc (0.28854592266321882, 0.9992)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.16915\n",
      "valid_acc (0.2899813650044758, 0.9958)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.15883\n",
      "valid_acc (0.2881044130135253, 0.9938)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.1562\n",
      "valid_acc (0.28833458037518406, 0.9954)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.17957\n",
      "valid_acc (0.28587802781820415, 0.9996)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.16419\n",
      "valid_acc (0.28731974309278008, 0.9954)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.14547\n",
      "valid_acc (0.28876140496319463, 0.996)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.14724\n",
      "valid_acc (0.28864007037390882, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.13961\n",
      "valid_acc (0.28838268081066243, 0.9962)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.13462\n",
      "valid_acc (0.28816296258233448, 0.9876)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.13614\n",
      "valid_acc (0.28763354489036252, 0.9966)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.13388\n",
      "valid_acc (0.28786167846451538, 0.9964)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.13973\n",
      "valid_acc (0.28636973192184867, 0.9946)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.13876\n",
      "valid_acc (0.28649953194108857, 0.9928)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.15366\n",
      "valid_acc (0.28473129118408808, 0.9958)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.14455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_acc (0.28456390004445731, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.15286\n",
      "valid_acc (0.28497410223384101, 0.9942)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.12035\n",
      "valid_acc (0.28641581072623634, 0.9844)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.12471\n",
      "valid_acc (0.28531511629545625, 0.9908)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.11934\n",
      "valid_acc (0.28640535686017415, 0.997)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.11322\n",
      "valid_acc (0.28666478442223403, 0.9906)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.10883\n",
      "valid_acc (0.28665638263078747, 0.9972)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.11352\n",
      "valid_acc (0.28671702681769784, 0.9948)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.1353\n",
      "valid_acc (0.28508285086108354, 0.9988)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.10889\n",
      "valid_acc (0.28620442935370588, 0.9942)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.09303\n",
      "valid_acc (0.28637393951760381, 0.9882)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.10301\n",
      "valid_acc (0.28505780025255095, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.10739\n",
      "valid_acc (0.28485262671958955, 0.9958)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.09432\n",
      "valid_acc (0.28536742454898523, 0.9982)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.09681\n",
      "valid_acc (0.28486099473804477, 0.9906)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.10725\n",
      "valid_acc (0.28584454703223183, 0.9964)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.11192\n",
      "valid_acc (0.28354269972604668, 0.9984)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.11803\n",
      "valid_acc (0.28317031773664281, 0.9982)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.09861\n",
      "valid_acc (0.2836390039081238, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.09043\n",
      "valid_acc (0.28505774364154263, 0.9966)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0843\n",
      "valid_acc (0.28563107816020022, 0.9976)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07878\n",
      "valid_acc (0.28506195024000019, 0.9968)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0878\n",
      "valid_acc (0.28293587792377828, 0.9984)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.09097\n",
      "valid_acc (0.28478565138257395, 0.9966)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.08298\n",
      "valid_acc (0.28411613760527232, 0.9976)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07764\n",
      "valid_acc (0.28502431056195038, 0.9916)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.08593\n",
      "valid_acc (0.28069697150133455, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.08671\n",
      "valid_acc (0.28432739367230558, 0.98)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07337\n",
      "valid_acc (0.28478354494624059, 0.9964)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07609\n",
      "valid_acc (0.28484847467127472, 0.9966)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07845\n",
      "valid_acc (0.28267225043677924, 0.9916)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.06952\n",
      "valid_acc (0.28220987907652056, 0.9938)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07222\n",
      "valid_acc (0.2839675902954123, 0.9994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.06004\n",
      "valid_acc (0.28462039204447254, 0.9966)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05748\n",
      "valid_acc (0.28450529847849387, 0.9932)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0793\n",
      "valid_acc (0.28335229064890494, 0.994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.07906\n",
      "valid_acc (0.28297148026298652, 0.9974)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.06676\n",
      "valid_acc (0.28381047411392823, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05871\n",
      "valid_acc (0.2840009337740354, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05621\n",
      "valid_acc (0.28474380505963764, 0.9956)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.06215\n",
      "valid_acc (0.28377077349920116, 0.9962)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04515\n",
      "valid_acc (0.28446347754299173, 0.9968)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05454\n",
      "valid_acc (0.28327064090070464, 0.9962)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.06292\n",
      "valid_acc (0.28319115755262686, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0568\n",
      "valid_acc (0.28332513865316733, 0.9994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05319\n",
      "valid_acc (0.2847564030491388, 0.9988)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04398\n",
      "valid_acc (0.28363265922838471, 0.9976)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05138\n",
      "valid_acc (0.28272875271573028, 0.9924)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04808\n",
      "valid_acc (0.28312214335563479, 0.9988)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04024\n",
      "valid_acc (0.28349034445282179, 0.9972)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03988\n",
      "valid_acc (0.28374573361057109, 0.9924)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05656\n",
      "valid_acc (0.28264707456159066, 0.9996)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04885\n",
      "valid_acc (0.28258219931249701, 0.9884)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05931\n",
      "valid_acc (0.28316811250930052, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03389\n",
      "valid_acc (0.28299653493858079, 0.9932)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03257\n",
      "valid_acc (0.28337115749517311, 0.9942)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04052\n",
      "valid_acc (0.28312629508449966, 0.995)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03613\n",
      "valid_acc (0.27972602322208873, 0.9992)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.05875\n",
      "valid_acc (0.28098783148490253, 0.9934)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03055\n",
      "valid_acc (0.28279780414534678, 0.9994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03358\n",
      "valid_acc (0.2827852538454439, 0.9994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.03889\n",
      "valid_acc (0.28345904742069261, 0.9986)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04025\n",
      "valid_acc (0.28251950369331758, 0.9978)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0395\n",
      "valid_acc (0.28157161568375733, 0.9982)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0515\n",
      "valid_acc (0.27981602747477091, 0.998)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0253\n",
      "valid_acc (0.28222031270456588, 0.9986)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04004\n",
      "valid_acc (0.28028893884023204, 0.9954)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.02827\n",
      "valid_acc (0.28037890591781156, 0.992)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04676\n",
      "valid_acc (0.28088318636086501, 0.9958)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.04087\n",
      "valid_acc (0.28176621124048423, 0.9902)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.0442\n",
      "valid_acc (0.28177457985862825, 0.9994)\n",
      "best_epoch (valid) 68\n",
      "last loss 3.02414\n",
      "valid_acc (0.28236676439329667, 0.999)\n",
      "best_epoch (valid) 68\n",
      "last loss 6.17785\n",
      "valid_acc 0.8264\n",
      "epoch 0 train_acc 0.8770459081836327 valid_acc 0.8264 test_acc 0.8179640718562874\n",
      "best_epoch (valid) 0\n",
      "last loss 5.62691\n",
      "valid_acc 0.8226\n",
      "best_epoch (valid) 0\n",
      "last loss 3.50737\n",
      "valid_acc 0.8074\n",
      "best_epoch (valid) 0\n",
      "last loss 0.252542\n",
      "valid_acc 0.8076\n",
      "best_epoch (valid) 0\n",
      "last loss 0.210167\n",
      "valid_acc 0.8094\n",
      "best_epoch (valid) 0\n",
      "last loss 0.0164241\n",
      "valid_acc 0.8098\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00810114\n",
      "valid_acc 0.8062\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00305667\n",
      "valid_acc 0.8074\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000644124\n",
      "valid_acc 0.8064\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000548877\n",
      "valid_acc 0.8072\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00041766\n",
      "valid_acc 0.808\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000312656\n",
      "valid_acc 0.8076\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000212298\n",
      "valid_acc 0.8072\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000156037\n",
      "valid_acc 0.8072\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000112172\n",
      "valid_acc 0.8062\n",
      "best_epoch (valid) 0\n",
      "last loss 7.97488e-05\n",
      "valid_acc 0.8064\n",
      "best_epoch (valid) 0\n",
      "last loss 5.61465e-05\n",
      "valid_acc 0.806\n",
      "best_epoch (valid) 0\n",
      "last loss 3.92193e-05\n",
      "valid_acc 0.806\n",
      "best_epoch (valid) 0\n",
      "last loss 2.70602e-05\n",
      "valid_acc 0.806\n",
      "best_epoch (valid) 0\n",
      "last loss 1.89541e-05\n",
      "valid_acc 0.8058\n",
      "best_epoch (valid) 0\n",
      "last loss 1.32322e-05\n",
      "valid_acc 0.8066\n",
      "best_epoch (valid) 0\n"
     ]
    }
   ],
   "source": [
    "def generate_data(corpus_sents, corpus_labels, max_length, extend_vocabulary):\n",
    "    sen_en = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_de = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_en_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_de_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_label = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "\n",
    "    def get_random_sequence(sent, max_length):\n",
    "        x = np.full((max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "        for i, word in enumerate(sent):\n",
    "            if word in extend_vocabulary:\n",
    "                x[i] = extend_vocabulary[word]\n",
    "            else:\n",
    "                x[i] = extend_vocabulary[\"<unk>\"]\n",
    "        return x\n",
    "\n",
    "    for i in range(len(corpus_sents)):\n",
    "        l = min(len(corpus_sents[i]), max_length-2) + 2\n",
    "        sen_en[i, :] = get_random_sequence([\"<s>\"] + corpus_sents[i][:max_length-2] + [\"</s>\"], max_length)\n",
    "        sen_de[i, :l-1] = sen_en[i, 1:l]\n",
    "        sen_en_length[i] = l\n",
    "        sen_de_length[i] = l - 1\n",
    "    \n",
    "    for i in range(len(corpus_sents)):\n",
    "        sen_label[i] = corpus_labels[i]\n",
    "    \n",
    "    return sen_en, sen_de, sen_en_length, sen_de_length, sen_label\n",
    "\n",
    "def get_total_accuracy_sa(data_sen_en, data_sen_de, data_sen_en_length, data_sen_de_length, extend_vocabulary, pretrained_lstm):\n",
    "    n_hit_word, n_hit_length = 0, 0\n",
    "    n_total_word, n_total_length = 0, 0\n",
    "    cur_idx = 0\n",
    "    while cur_idx < data_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = data_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = data_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        _, predictions = pretrained_lstm.predict_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "        cur_acc_word, cur_acc_length = evaluate_sa(batch_sen_de, batch_sen_de_length, predictions, extend_vocabulary)\n",
    "        n_hit_word += cur_acc_word * np.sum(batch_sen_en_length)\n",
    "        n_total_word += np.sum(batch_sen_en_length)\n",
    "        n_hit_length += cur_acc_length * batch_sen_en.shape[0]\n",
    "        n_total_length += batch_sen_en.shape[0]\n",
    "    return 1. * n_hit_word / n_total_word, 1. * n_hit_length / n_total_length\n",
    "\n",
    "def get_total_accuracy_clf(data_sen_en, data_sen_en_length, data_label, pretrained_lstm):\n",
    "    cur_idx = 0\n",
    "    hit, tot = 0, 0\n",
    "    while cur_idx < valid_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]   \n",
    "        batch_label = data_label[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_pred = pretrained_lstm.predict_clf(batch_sen_en, batch_sen_en_length)\n",
    "        assert batch_pred.shape[0] == batch_sen_en.shape[0]\n",
    "        cur_acc = evaluate_clf(batch_pred, batch_label)\n",
    "        hit += cur_acc * batch_pred.shape[0]\n",
    "        tot += batch_pred.shape[0]\n",
    "        cur_idx += n_batch_size\n",
    "    return 1. * hit / tot\n",
    "    \n",
    "# hyperparameter\n",
    "vocabulary_size = 20000\n",
    "origin_vocabulary = {}\n",
    "for word, n in corpus_counter.most_common(vocabulary_size):\n",
    "    origin_vocabulary[\"{}\".format(word)] = len(origin_vocabulary)\n",
    "extend_vocabulary = dict(origin_vocabulary)\n",
    "for w in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]:\n",
    "    extend_vocabulary[w] = len(extend_vocabulary)\n",
    "reverse_extend_vocabulary = {v: k for k, v in extend_vocabulary.items()}\n",
    "imdb_label = {\"0\": 0, \"1\": 1}\n",
    "\n",
    "state_size=128\n",
    "n_max_length=100\n",
    "n_batch_size=30\n",
    "\n",
    "# generate training/testing data\n",
    "preprocess_sents = []\n",
    "preprocess_labels = []\n",
    "random.shuffle(train_preprocess)\n",
    "for sent, label in train_preprocess:\n",
    "    preprocess_sents.append(sent)\n",
    "    preprocess_labels.append(label)\n",
    "n_train = int(len(preprocess_sents) * 0.8)\n",
    "n_valid = len(preprocess_sents) - n_train\n",
    "\n",
    "print(\"n_train\", n_train, \"n_valid\", n_valid, \"n_test\", len(test_preprocess))\n",
    "\n",
    "train_sents = preprocess_sents[:n_train]\n",
    "train_labels = preprocess_labels[:n_train]\n",
    "valid_sents = preprocess_sents[n_train:]\n",
    "valid_labels = preprocess_labels[n_train:]\n",
    "test_sents, test_labels = [], []\n",
    "for sent, label in test_preprocess:\n",
    "    test_sents.append(sent)\n",
    "    test_labels.append(label)\n",
    "unlabeled_sents, unlabeled_labels = [], []\n",
    "for sent in unlabeled_preprocess:\n",
    "    unlabeled_sents.append(sent)\n",
    "    unlabeled_labels.append(0)  # just assign anything\n",
    "    \n",
    "train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, train_sen_en_label = generate_data(\n",
    "    train_sents, train_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, valid_sen_en_label = generate_data(\n",
    "    valid_sents, valid_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, test_sen_en_label = generate_data(\n",
    "    test_sents, test_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "unlabeled_sen_en, unlabeled_sen_de, unlabeled_sen_en_length, unlabeled_sen_de_length, unlabeled_sen_en_label = generate_data(\n",
    "    unlabeled_sents, unlabeled_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "\"\"\"\n",
    "print(train_sen_en[0])\n",
    "print(preprocess_sents[0])\n",
    "print([reverse_extend_vocabulary[i] for i in train_sen_en[0]])\n",
    "print(valid_sen_en[0])\n",
    "print(preprocess_sents[n_train])\n",
    "print([reverse_extend_vocabulary[i] for i in valid_sen_en[0]])\n",
    "\"\"\"\n",
    "pretrained_lstm = EncoderDecoder(vocabulary=extend_vocabulary, label=imdb_label, state_size=state_size, n_max_length=n_max_length)\n",
    "\n",
    "# train_sa\n",
    "best_epoch = None\n",
    "best_valid_acc = None\n",
    "early_stopping = 20\n",
    "\n",
    "curve_valid_acc = []\n",
    "for epoch in range(200):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = train_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = train_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent = pretrained_lstm.train_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    cur_idx = 0\n",
    "    while cur_idx < unlabeled_sen_en.shape[0]:\n",
    "        batch_sen_en = unlabeled_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = unlabeled_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = unlabeled_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = unlabeled_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent = pretrained_lstm.train_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"last loss\", loss)\n",
    "    valid_acc = get_total_accuracy_sa(\n",
    "        valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "    )\n",
    "    print(\"valid_acc\", valid_acc)\n",
    "    curve_valid_acc.append(valid_acc)\n",
    "    if best_valid_acc is None or best_valid_acc < valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        train_acc = get_total_accuracy_sa(\n",
    "            train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "        )\n",
    "        test_acc = get_total_accuracy_sa(\n",
    "            test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "        )\n",
    "        print(\"epoch\", epoch, \"train_acc\", train_acc, \"valid_acc\", valid_acc, \"test_acc\", test_acc)\n",
    "    print(\"best_epoch (valid)\", best_epoch)\n",
    "    \"\"\"\n",
    "    if epoch - best_epoch == early_stopping:\n",
    "        break\n",
    "    \"\"\"\n",
    "\n",
    "# train_clf\n",
    "best_epoch = None\n",
    "best_valid_acc = None\n",
    "early_stopping = 20\n",
    "\n",
    "for epoch in range(200):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_label = train_sen_en_label[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, pred = pretrained_lstm.train_clf(\n",
    "            batch_sen_en, batch_sen_en_length, batch_sen_en_label\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"last loss\", loss)\n",
    "    valid_acc = get_total_accuracy_clf(\n",
    "        valid_sen_en, valid_sen_en_length, valid_sen_en_label, pretrained_lstm\n",
    "    )\n",
    "    print(\"valid_acc\", valid_acc)\n",
    "    if best_valid_acc is None or best_valid_acc < valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        train_acc = get_total_accuracy_clf(\n",
    "            train_sen_en, train_sen_en_length, train_sen_en_label, pretrained_lstm\n",
    "        )\n",
    "        test_acc = get_total_accuracy_clf(\n",
    "            test_sen_en, test_sen_en_length, test_sen_en_label, pretrained_lstm\n",
    "        )\n",
    "        print(\"epoch\", epoch, \"train_acc\", train_acc, \"valid_acc\", valid_acc, \"test_acc\", test_acc)\n",
    "    print(\"best_epoch (valid)\", best_epoch)\n",
    "    if epoch - best_epoch == early_stopping:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEE+v+4uxIlFOjfVgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd84aea7668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(curve_valid_acc)), curve_valid_acc, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
