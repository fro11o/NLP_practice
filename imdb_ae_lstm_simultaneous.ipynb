{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_pos) 12500\n",
      "len(train_neg) 12500\n",
      "len(test_pos) 12500\n",
      "len(test_neg) 12500\n",
      "train_pos[0] ['``', 'The', 'Last', 'Hard', 'Men', \"''\", 'is', 'a', 'typical', 'western', 'for', 'the', '70', \"'s\", '.', 'Most', 'of', 'them', 'seem', 'to', 'be', 'inspired', 'by', 'Sam', 'Peckinpah', '.', 'Also', 'this', 'one', ',', 'but', 'Director', 'Andrew', 'McLaglan', 'is', 'a', 'John', 'Ford', 'Pupil', 'and', 'this', 'can', 'be', 'obviously', 'shown', 'in', 'many', 'scenes', '.', 'IMO', 'the', 'beginning', 'is', 'very', 'good', '.', 'In', 'a', 'certain', 'way', 'McLaglan', 'wanted', 'to', 'show', 'the', 'audience', 'a', 'travel', 'from', 'the', 'civilization', 'to', 'the', 'wilderness', '.', 'In', 'the', 'third', 'part', 'there', 'are', 'some', 'illogical', 'flaws', 'and', 'I', 'complain', 'a', 'bit', 'about', 'Charlton', 'Heston', '.', 'He', 'has', 'to', 'play', 'an', 'old', 'ex-lawman', 'named', 'Sam', 'Burgade', 'but', 'he', 'is', 'in', 'a', 'fantastic', 'physical', 'shape', '.', 'I', 'never', 'got', 'the', 'feeling', 'that', 'he', 'really', 'has', 'problems', 'to', 'climb', 'on', 'a', 'horse', 'or', 'on', 'a', 'rock', '.', 'For', 'me', 'he', 'did', \"n't\", 'looks', 'very', 'motivated', 'as', 'he', 'usual', 'do', 'in', 'most', 'of', 'his', 'epic', 'movies', '.', 'Same', 'goes', 'to', 'the', 'beautiful', 'Barbara', 'Hershey', 'who', 'is', 'playing', 'the', 'sheriff', \"'s\", 'daughter', '.', 'Maybe', 'both', 'had', 'troubles', 'with', 'the', 'director', 'or', 'were', 'unhappy', 'with', 'their', 'roles', '.', 'Hershey', 'and', 'Coburn', 'are', 'not', 'showing', 'their', 'best', 'but', 'they', 'are', 'still', 'good', '.', 'If', 'the', 'scriptwriter', 'had', 'John', 'Wayne', 'in', 'their', 'mind', 'as', 'Sam', 'Burgade', '?', 'Also', 'Michael', 'Parks', 'as', 'modern', 'sheriff', 'is', 'a', 'bit', 'underused', 'in', 'his', 'role', '.', 'On', 'the', 'other', 'Hand', 'there', 'is', 'James', 'Coburn', 'as', 'outlaw', 'Zach', 'Provo', '.', 'Coburn', 'is', 'a', 'really', 'great', 'villain', 'in', 'this', 'one', '.', 'He', 'is', 'portraying', 'the', 'bad', 'guy', 'between', 'maniac', 'hate', 'and', 'cleverness', '.', 'His', 'role', 'and', 'his', 'acting', 'is', 'the', 'best', 'of', 'the', 'movie.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Landscapes', 'and', 'Shootouts', 'are', 'terrific', '.', 'The', 'shootings', 'scenes', 'are', 'bloody', 'and', 'the', 'violence', 'looks', 'realistic', '.', 'Zach', 'Provo', 'and', 'his', 'gang', 'had', 'some', 'gory', 'and', 'violent', 'scenes', '.', 'What', 'I', 'miss', 'is', 'the', 'typical', 'western', 'action', 'in', 'the', 'middle', 'of', 'the', 'movie', '.', 'I', 'would', 'have', 'appreciated', 'a', 'bank', 'robbery', 'or', 'something', 'similar', '.', 'Overall', 'it', \"'s\", 'an', 'entertaining', 'western', 'flick', '.', 'Not', 'a', 'great', 'movie', 'but', 'above', 'the', 'average', 'because', 'of', 'a', 'great', 'Coburn', ',', 'a', 'very', 'good', 'beginning', 'and', 'some', 'gory', 'and', 'violent', 'scenes', '.']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_sent_in_directory(path_directory):\n",
    "    sents = []\n",
    "    for filename in os.listdir(path_directory):\n",
    "        if os.path.isfile(os.path.join(path_directory, filename)):\n",
    "            with open(os.path.join(path_directory, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    sents.append(nltk.tokenize.word_tokenize(line))\n",
    "    return sents\n",
    "                    \n",
    "                \n",
    "train_pos = get_sent_in_directory(\"./aclImdb/train/pos\")\n",
    "train_neg = get_sent_in_directory(\"./aclImdb/train/neg\")\n",
    "test_pos = get_sent_in_directory(\"./aclImdb/test/pos\")\n",
    "test_neg = get_sent_in_directory(\"./aclImdb/test/neg\")\n",
    "\n",
    "print(\"len(train_pos)\", len(train_pos))\n",
    "print(\"len(train_neg)\", len(train_neg))\n",
    "print(\"len(test_pos)\", len(test_pos))\n",
    "print(\"len(test_neg)\", len(test_neg))\n",
    "print(\"train_pos[0]\", train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique word: 165718\n",
      "# of sents: 25000\n",
      "max len(sents[i]): 2502\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGSUAoCISEYpAIiIZNT/BxT2MyCQ1jP1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f95c65cb550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_counter = collections.Counter()\n",
    "train_preprocess = []\n",
    "test_preprocess = []\n",
    "for sent in train_pos:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    train_preprocess.append((res, 1))\n",
    "for sent in train_neg:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    train_preprocess.append((res, 0))\n",
    "for sent in test_pos:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    test_preprocess.append((res, 1))\n",
    "for sent in test_neg:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    test_preprocess.append((res, 0))\n",
    "\n",
    "print(\"unique word:\", len(corpus_counter))\n",
    "print(\"# of sents:\", len(train_preprocess))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s, _ in train_preprocess]))\n",
    "\n",
    "train_length = []\n",
    "for sent, label in train_preprocess:\n",
    "    train_length.append(len(sent))\n",
    "train_length = sorted(train_length)\n",
    "stat_x, stat_y = [], []\n",
    "for i, l in enumerate(train_length):\n",
    "    stat_x.append(i / len(train_length))\n",
    "    stat_y.append(l)\n",
    "plt.plot(stat_x, stat_y, 'r-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Brown Corpus to train an seq2seq autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, vocabulary={}, label={}, state_size=64, n_max_length=30):     \n",
    "        self.state_size = state_size\n",
    "        self.n_max_length = n_max_length\n",
    "        self.vocabulary = vocabulary\n",
    "        self.label = label\n",
    "        self.reverse_vocabulary = {k: v for k, v in vocabulary.items()}\n",
    "        \n",
    "        ######################\n",
    "        # Graph Construction #\n",
    "        ######################\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #self.sen_en = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_en\")\n",
    "            #self.sen_de = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_de\")\n",
    "            self.sen_en = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_en\")\n",
    "            self.sen_de = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_de\")\n",
    "            self.sen_en_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_length\")\n",
    "            self.sen_de_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_de_length\")\n",
    "            self.sen_en_label = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_label\")\n",
    "                        \n",
    "            batch_size_en = tf.shape(self.sen_en)[0]\n",
    "            batch_size_de = tf.shape(self.sen_de)[0]\n",
    "            batch_max_length_de = tf.shape(self.sen_de)[1]\n",
    "            \n",
    "            # TODO sen_en_embedding could also be self-trained embedding: embedding_lookup\n",
    "            self.embedding = tf.Variable(tf.random_uniform([len(self.vocabulary), self.state_size], -1.0, 1.0), dtype=tf.float32)\n",
    "            #self.sen_en_embedding = tf.one_hot(self.sen_en, len(self.vocabulary))\n",
    "            #self.sen_de_embedding = tf.one_hot(self.sen_de, len(self.vocabulary))\n",
    "            self.sen_en_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_en)\n",
    "            self.sen_de_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_de)\n",
    "            \n",
    "            # build encoder decoder structure\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en_init = self.cell_en.zero_state(batch_size_en, tf.float32)\n",
    "                self.h_state_en, self.final_state_en = tf.nn.dynamic_rnn(\n",
    "                    self.cell_en,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_en_init,\n",
    "                )\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de_init = self.final_state_en\n",
    "                self.h_state_de, self.final_state_de = tf.nn.dynamic_rnn(\n",
    "                    self.cell_de,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_de_init,\n",
    "                )\n",
    "            \n",
    "            # autoencoder softmax\n",
    "            with tf.variable_scope(\"softmax\") as scope:\n",
    "                W = tf.get_variable(\"W\", [self.state_size, len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))\n",
    "                b = tf.get_variable(\"b\", [len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))               \n",
    "            self.logits = tf.reshape(\n",
    "                tf.add(tf.matmul(tf.reshape(self.h_state_de, (-1, self.state_size)), W), b),\n",
    "                shape=(-1, batch_max_length_de, len(self.vocabulary))\n",
    "            )\n",
    "            self.pred_sa = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # construct loss and train op\n",
    "            self.cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.sen_de,\n",
    "                logits=self.logits\n",
    "            )        \n",
    "            self.mask = tf.sequence_mask(self.sen_de_length, maxlen=batch_max_length_de)\n",
    "            self.loss_sa = tf.reduce_mean(\n",
    "                tf.divide(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.where(\n",
    "                            self.mask,\n",
    "                            self.cross_ent,\n",
    "                            tf.zeros_like(self.cross_ent)\n",
    "                        ), 1\n",
    "                    ),\n",
    "                    tf.to_float(self.sen_de_length)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # classifier layer\n",
    "            with tf.variable_scope(\"classifier\") as scope:\n",
    "                clf_input = tf.concat([self.final_state_en.h, self.final_state_en.c], axis=1)\n",
    "                clf_dense = tf.layers.dense(clf_input, 30, activation=tf.nn.relu)\n",
    "                self.clf_output = tf.layers.dense(clf_dense, len(self.label))\n",
    "            \n",
    "            self.pred_clf = tf.nn.softmax(self.clf_output)\n",
    "                   \n",
    "            # construct classifier loss\n",
    "            self.loss_clf = tf.reduce_sum(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.sen_en_label,\n",
    "                    logits=self.clf_output,\n",
    "                )\n",
    "            )        \n",
    "            \n",
    "            # Optimization\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train_sa = optimizer.minimize(self.loss_sa)\n",
    "            self.op_train_clf = optimizer.minimize(self.loss_clf)\n",
    "               \n",
    "            # initializer\n",
    "            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph,\n",
    "                config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )           \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "    def train_sa(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        assert batch_sen_en.shape[1] == self.n_max_length  # training always input same length as self.n_max_length\n",
    "        _, loss, pred, sen_en_embedding, mask, cross_ent = self.sess.run(\n",
    "            [self.op_train_sa, self.loss_sa, self.pred_sa, self.sen_en_embedding, self.mask, self.cross_ent],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, pred, sen_en_embedding, mask, cross_ent\n",
    "        \n",
    "    def predict_sa(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        loss, pred = self.sess.run(\n",
    "            [self.loss_sa, self.pred_sa],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, pred\n",
    "    \n",
    "    def encode_sa(self, batch_sen_en, batch_sen_en_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        Returns\n",
    "        -------\n",
    "        #batch_state_en: numpy, shape=(n, self.state_size), dtype=float\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        \"\"\"\n",
    "        batch_state_en = self.sess.run(\n",
    "            self.final_state_en,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return batch_state_en\n",
    "    \n",
    "    def decode_sa(self, batch_state_en):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        Returns\n",
    "        -------\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        \"\"\"\n",
    "        batch_size = batch_state_en.c.shape[0]\n",
    "        batch_sen_de = np.empty([batch_size, self.n_max_length], dtype=np.int32)\n",
    "        \n",
    "        tmp_sen_de = np.empty([batch_size, 1], dtype=np.int32)\n",
    "        tmp_sen_de_length = np.ones([batch_size], dtype=np.int32)\n",
    "        tmp_sen_de[:] = self.vocabulary[\"<s>\"]\n",
    "        tmp_last_state = batch_state_en\n",
    "        for i in range(self.n_max_length):\n",
    "            tmp_predict, tmp_last_state = self.sess.run(\n",
    "                [self.pred_sa, self.final_state_de],\n",
    "                feed_dict={\n",
    "                    self.cell_de_init: tmp_last_state,\n",
    "                    self.sen_de: tmp_sen_de,\n",
    "                    self.sen_de_length: tmp_sen_de_length,\n",
    "                }\n",
    "            )\n",
    "            tmp_sen_de = np.argmax(tmp_predict, axis=2)\n",
    "            batch_sen_de[:,i] = tmp_sen_de[:,0]\n",
    "           \n",
    "        return batch_sen_de\n",
    "    \n",
    "    def train_clf(self, batch_sen_en, batch_sen_en_length, batch_label):\n",
    "        _, loss, pred = self.sess.run(\n",
    "            [self.op_train_clf, self.loss_clf, self.pred_clf],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_en_label: batch_label\n",
    "            }\n",
    "        )\n",
    "        return loss, pred\n",
    "    \n",
    "    def predict_clf(self, batch_sen_en, batch_sen_en_length):\n",
    "        pred = self.sess.run(\n",
    "            self.pred_clf,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    \n",
    "def evaluate_sa(batch_sen_en, batch_sen_en_length, batch_prediction, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "    batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "    batch_prediction: numpy, shape=(n, max_length, len(vocabulary))\n",
    "    \"\"\"\n",
    "    assert batch_sen_en.shape[0] == batch_prediction.shape[0]\n",
    "    acc_word = 0\n",
    "    acc_sen_end = 0\n",
    "    for i in range(batch_sen_en.shape[0]):\n",
    "        is_first_end = False\n",
    "        for j in range(batch_sen_en_length[i]):\n",
    "            cur_pred_word = np.argmax(batch_prediction[i, j])\n",
    "            if cur_pred_word == batch_sen_en[i, j]:\n",
    "                acc_word += 1\n",
    "                if not is_first_end and cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                    acc_sen_end += 1\n",
    "            if cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                is_first_end = True\n",
    "    return 1. * acc_word / np.sum(batch_sen_en_length), 1. * acc_sen_end / batch_sen_en.shape[0]\n",
    "\n",
    "\n",
    "def evaluate_clf(batch_pred, batch_label):\n",
    "    assert batch_pred.shape[0] == batch_label.shape[0]\n",
    "    hit, tot = 0, 0\n",
    "    for i in range(batch_label.shape[0]):\n",
    "        if np.argmax(batch_pred[i]) == batch_label[i]:\n",
    "            hit += 1\n",
    "        tot += 1\n",
    "    return 1. * hit / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 20000 n_valid 5000 n_test 25000\n",
      "last loss 6.02811\n",
      "valid_acc (0.14017147779571684, 0.0)\n",
      "epoch 0 train_acc (0.14249437168118609, 0.0) valid_acc (0.14017147779571684, 0.0) test_acc (0.14088370662104821, 4e-05)\n",
      "best_epoch (valid) 0\n",
      "last loss 5.79073\n",
      "valid_acc (0.15586097361535003, 0.0)\n",
      "epoch 1 train_acc (0.15957060245619889, 0.00025) valid_acc (0.15586097361535003, 0.0) test_acc (0.15605294935763481, 0.00036)\n",
      "best_epoch (valid) 1\n",
      "last loss 5.63481\n",
      "valid_acc (0.16485657113139546, 0.0024)\n",
      "epoch 2 train_acc (0.17011068519768988, 0.00385) valid_acc (0.16485657113139546, 0.0024) test_acc (0.16477593158763756, 0.00328)\n",
      "best_epoch (valid) 2\n",
      "last loss 5.497\n",
      "valid_acc (0.17438998371706471, 0.0518)\n",
      "epoch 3 train_acc (0.18170031087417984, 0.0604) valid_acc (0.17438998371706471, 0.0518) test_acc (0.17435753534162721, 0.05176)\n",
      "best_epoch (valid) 3\n",
      "last loss 5.38356\n",
      "valid_acc (0.18519142233166166, 0.1236)\n",
      "epoch 4 train_acc (0.19385440917192326, 0.13225) valid_acc (0.18519142233166166, 0.1236) test_acc (0.1849118954031137, 0.124)\n",
      "best_epoch (valid) 4\n",
      "last loss 5.27097\n",
      "valid_acc (0.19093106303333013, 0.0566)\n",
      "epoch 5 train_acc (0.20088727914118951, 0.05215) valid_acc (0.19093106303333013, 0.0566) test_acc (0.19026475638517465, 0.0544)\n",
      "best_epoch (valid) 5\n",
      "last loss 5.1845\n",
      "valid_acc (0.19518723563392062, 0.078)\n",
      "epoch 6 train_acc (0.20687262768944248, 0.0755) valid_acc (0.19518723563392062, 0.078) test_acc (0.19414180136811504, 0.07644)\n",
      "best_epoch (valid) 6\n",
      "last loss 5.1159\n",
      "valid_acc (0.19784053269321639, 0.421)\n",
      "epoch 7 train_acc (0.21148793751019496, 0.45645) valid_acc (0.19784053269321639, 0.421) test_acc (0.1966775373382241, 0.41552)\n",
      "best_epoch (valid) 7\n",
      "last loss 5.029\n",
      "valid_acc (0.20110694699341006, 0.4048)\n",
      "epoch 8 train_acc (0.21679592506058865, 0.4453) valid_acc (0.20110694699341006, 0.4048) test_acc (0.20007155236770696, 0.41124)\n",
      "best_epoch (valid) 8\n",
      "last loss 4.96226\n",
      "valid_acc (0.20195223798904766, 0.4698)\n",
      "epoch 9 train_acc (0.2190293321245092, 0.52645) valid_acc (0.20195223798904766, 0.4698) test_acc (0.20046986648757367, 0.46984)\n",
      "best_epoch (valid) 9\n",
      "last loss 4.89631\n",
      "valid_acc (0.20495712819199369, 0.4592)\n",
      "epoch 10 train_acc (0.22388931343518423, 0.5075) valid_acc (0.20495712819199369, 0.4592) test_acc (0.20339220586540724, 0.45664)\n",
      "best_epoch (valid) 10\n",
      "last loss 4.84872\n",
      "valid_acc (0.20640302618913722, 0.2832)\n",
      "epoch 11 train_acc (0.22665545141293622, 0.28565) valid_acc (0.20640302618913722, 0.2832) test_acc (0.20472881477136984, 0.28244)\n",
      "best_epoch (valid) 11\n",
      "last loss 4.78931\n",
      "valid_acc (0.20767110554053045, 0.0856)\n",
      "epoch 12 train_acc (0.22948528769653606, 0.0752) valid_acc (0.20767110554053045, 0.0856) test_acc (0.20569104713642394, 0.08724)\n",
      "best_epoch (valid) 12\n",
      "last loss 4.7232\n",
      "valid_acc (0.20860645878870465, 0.0708)\n",
      "epoch 13 train_acc (0.23223201084659276, 0.0559) valid_acc (0.20860645878870465, 0.0708) test_acc (0.2064771545327283, 0.06404)\n",
      "best_epoch (valid) 13\n",
      "last loss 4.67498\n",
      "valid_acc (0.20958780177122585, 0.1028)\n",
      "epoch 14 train_acc (0.23482092904951013, 0.08845) valid_acc (0.20958780177122585, 0.1028) test_acc (0.20738362631525856, 0.09912)\n",
      "best_epoch (valid) 14\n",
      "last loss 4.62677\n",
      "valid_acc (0.21165308668194369, 0.345)\n",
      "epoch 15 train_acc (0.23908138364568007, 0.34805) valid_acc (0.21165308668194369, 0.345) test_acc (0.20931938328522515, 0.33632)\n",
      "best_epoch (valid) 15\n",
      "last loss 4.5776\n",
      "valid_acc (0.21270771090174889, 0.4144)\n",
      "epoch 16 train_acc (0.24241246400162159, 0.42495) valid_acc (0.21270771090174889, 0.4144) test_acc (0.21031554969850957, 0.40916)\n",
      "best_epoch (valid) 16\n",
      "last loss 4.53136\n",
      "valid_acc (0.21293785248639821, 0.4484)\n",
      "epoch 17 train_acc (0.24447969773596456, 0.46035) valid_acc (0.21293785248639821, 0.4484) test_acc (0.21047193230167885, 0.43852)\n",
      "best_epoch (valid) 17\n",
      "last loss 4.49311\n",
      "valid_acc (0.21251309684050215, 0.158)\n",
      "best_epoch (valid) 17\n",
      "last loss 4.45956\n",
      "valid_acc (0.2133166205079666, 0.6928)\n",
      "epoch 19 train_acc (0.24894561328102405, 0.7789) valid_acc (0.2133166205079666, 0.6928) test_acc (0.21091135468966596, 0.69168)\n",
      "best_epoch (valid) 19\n",
      "last loss 4.40477\n",
      "valid_acc (0.21351328661797053, 0.7262)\n",
      "epoch 20 train_acc (0.25113728047190903, 0.8255) valid_acc (0.21351328661797053, 0.7262) test_acc (0.21107192798294164, 0.7224)\n",
      "best_epoch (valid) 20\n",
      "last loss 4.36212\n",
      "valid_acc (0.21384387708322553, 0.7264)\n",
      "epoch 21 train_acc (0.25377526961202534, 0.82285) valid_acc (0.21384387708322553, 0.7264) test_acc (0.21145722850240387, 0.72412)\n",
      "best_epoch (valid) 21\n",
      "last loss 4.32583\n",
      "valid_acc (0.21340240819766143, 0.6202)\n",
      "best_epoch (valid) 21\n",
      "last loss 4.29281\n",
      "valid_acc (0.21396319247917273, 0.5004)\n",
      "epoch 23 train_acc (0.25772161182963194, 0.5383) valid_acc (0.21396319247917273, 0.5004) test_acc (0.21157168572810381, 0.49624)\n",
      "best_epoch (valid) 23\n",
      "last loss 4.25608\n",
      "valid_acc (0.21425608690769193, 0.4628)\n",
      "epoch 24 train_acc (0.26037945622061132, 0.4858) valid_acc (0.21425608690769193, 0.4628) test_acc (0.21183118296144718, 0.4564)\n",
      "best_epoch (valid) 24\n",
      "last loss 4.21442\n",
      "valid_acc (0.21421844546784569, 0.564)\n",
      "best_epoch (valid) 24\n",
      "last loss 4.17667\n",
      "valid_acc (0.21428540153959239, 0.5772)\n",
      "epoch 26 train_acc (0.26500376355286759, 0.62355) valid_acc (0.21428540153959239, 0.5772) test_acc (0.21181233850110839, 0.57304)\n",
      "best_epoch (valid) 26\n",
      "last loss 4.14097\n",
      "valid_acc (0.21450300562643221, 0.597)\n",
      "epoch 27 train_acc (0.26729941567761312, 0.6418) valid_acc (0.21450300562643221, 0.597) test_acc (0.2116995603629635, 0.5894)\n",
      "best_epoch (valid) 27\n",
      "last loss 4.10918\n",
      "valid_acc (0.2139234045499338, 0.7022)\n",
      "best_epoch (valid) 27\n",
      "last loss 4.07424\n",
      "valid_acc (0.2135844277894722, 0.7714)\n",
      "best_epoch (valid) 27\n",
      "last loss 4.03992\n",
      "valid_acc (0.21291902905811733, 0.7934)\n",
      "best_epoch (valid) 27\n",
      "last loss 4.00928\n",
      "valid_acc (0.21277466889565635, 0.5086)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.97596\n",
      "valid_acc (0.21257165970734987, 0.5192)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.95642\n",
      "valid_acc (0.21233727538869734, 0.831)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.92504\n",
      "valid_acc (0.21126176242501432, 0.854)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.89475\n",
      "valid_acc (0.21090814850798728, 0.8456)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.87023\n",
      "valid_acc (0.21117388411114382, 0.8478)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.83371\n",
      "valid_acc (0.21078675784876827, 0.8266)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.80269\n",
      "valid_acc (0.21018621331344381, 0.848)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.7754\n",
      "valid_acc (0.20978240137010362, 0.8162)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.75144\n",
      "valid_acc (0.20914840620463312, 0.8112)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.72698\n",
      "valid_acc (0.20786357091026023, 0.8136)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.69953\n",
      "valid_acc (0.20716889644308972, 0.703)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.67261\n",
      "valid_acc (0.20650139415961771, 0.67)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.64899\n",
      "valid_acc (0.20521246113677127, 0.4408)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.62566\n",
      "valid_acc (0.20534633270680272, 0.5728)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.60028\n",
      "valid_acc (0.20542587095384363, 0.5224)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.57637\n",
      "valid_acc (0.20446542745897298, 0.3962)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.55279\n",
      "valid_acc (0.20371630107648542, 0.8382)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.5306\n",
      "valid_acc (0.20284581691109199, 0.8584)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.51845\n",
      "valid_acc (0.20376651220194156, 0.8368)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.49485\n",
      "valid_acc (0.20335640137586353, 0.8026)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.47328\n",
      "valid_acc (0.20256125501284875, 0.8352)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.45327\n",
      "valid_acc (0.20168447182646607, 0.8342)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.43566\n",
      "valid_acc (0.20119064996728517, 0.814)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.41597\n",
      "valid_acc (0.20051690141283401, 0.7926)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.40543\n",
      "valid_acc (0.19984517391754963, 0.822)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.37737\n",
      "valid_acc (0.19938895905056514, 0.8382)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.35643\n",
      "valid_acc (0.19871098388679045, 0.8696)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.33649\n",
      "valid_acc (0.19804558005249634, 0.8736)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.32565\n",
      "valid_acc (0.19748271083666283, 0.8744)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.31753\n",
      "valid_acc (0.19777986935508057, 0.8822)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.2975\n",
      "valid_acc (0.19766897026119748, 0.8334)\n",
      "best_epoch (valid) 27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss 3.27561\n",
      "valid_acc (0.1969073049715496, 0.845)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.26461\n",
      "valid_acc (0.19648672338171042, 0.8524)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.25019\n",
      "valid_acc (0.19447792305052114, 0.8724)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.23105\n",
      "valid_acc (0.19384596201552096, 0.8776)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.21275\n",
      "valid_acc (0.19359699909955294, 0.8738)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.18606\n",
      "valid_acc (0.19383133542238484, 0.865)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.17382\n",
      "valid_acc (0.19332500456733515, 0.8516)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.16307\n",
      "valid_acc (0.19300902897321623, 0.865)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.15533\n",
      "valid_acc (0.19318060006536647, 0.8518)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.14378\n",
      "valid_acc (0.19282698621487226, 0.764)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.12121\n",
      "valid_acc (0.19219296277503986, 0.808)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.10054\n",
      "valid_acc (0.19110482947973573, 0.8228)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.09024\n",
      "valid_acc (0.18923208221207649, 0.8728)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.09131\n",
      "valid_acc (0.18867547257070777, 0.865)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.06805\n",
      "valid_acc (0.19016952570729678, 0.8896)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.04292\n",
      "valid_acc (0.18993305543498273, 0.8656)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.03808\n",
      "valid_acc (0.18970082783791936, 0.877)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.03112\n",
      "valid_acc (0.1882528572771669, 0.8834)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.02874\n",
      "valid_acc (0.18740328429849026, 0.8824)\n",
      "best_epoch (valid) 27\n",
      "last loss 3.00536\n",
      "valid_acc (0.18792013984199099, 0.8708)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.99173\n",
      "valid_acc (0.18708943863975738, 0.864)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.9772\n",
      "valid_acc (0.18632357968247604, 0.884)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.97232\n",
      "valid_acc (0.18581095589845811, 0.8898)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.95133\n",
      "valid_acc (0.1852103634894296, 0.8818)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.93803\n",
      "valid_acc (0.1849696909155901, 0.879)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.93093\n",
      "valid_acc (0.18589033802540059, 0.8492)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.92666\n",
      "valid_acc (0.1858108522454695, 0.8044)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.90463\n",
      "valid_acc (0.18578156566131224, 0.809)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.89091\n",
      "valid_acc (0.18450929503938066, 0.8398)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.89176\n",
      "valid_acc (0.18372873480211779, 0.8766)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.8794\n",
      "valid_acc (0.18323707670369946, 0.887)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.86818\n",
      "valid_acc (0.18481062553660618, 0.8276)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.85088\n",
      "valid_acc (0.18367230618072999, 0.781)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.85604\n",
      "valid_acc (0.18178491449777989, 0.8806)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.84874\n",
      "valid_acc (0.18087466863858875, 0.8742)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.82379\n",
      "valid_acc (0.18010048165231896, 0.889)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.81169\n",
      "valid_acc (0.18041848394954513, 0.8492)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.81302\n",
      "valid_acc (0.18060469507733487, 0.8446)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.79134\n",
      "valid_acc (0.17928851370473858, 0.8164)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.78303\n",
      "valid_acc (0.1789914469119554, 0.829)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.78229\n",
      "valid_acc (0.17946641235127328, 0.8166)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.77525\n",
      "valid_acc (0.17933249679953081, 0.8722)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.77374\n",
      "valid_acc (0.17942032783458212, 0.8578)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.75694\n",
      "valid_acc (0.17932624174924591, 0.8536)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.74911\n",
      "valid_acc (0.1790248291749845, 0.8546)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.74488\n",
      "valid_acc (0.17813760160272718, 0.8788)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.74233\n",
      "valid_acc (0.17795561033080073, 0.8668)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.74015\n",
      "valid_acc (0.17690939315554541, 0.884)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.72067\n",
      "valid_acc (0.177009842353714, 0.8698)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.72736\n",
      "valid_acc (0.17718972418677029, 0.8772)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.70719\n",
      "valid_acc (0.17694076606432815, 0.8562)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.70409\n",
      "valid_acc (0.17640094752581797, 0.8562)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.69126\n",
      "valid_acc (0.17545713831602824, 0.8892)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.69792\n",
      "valid_acc (0.17473732164645697, 0.8838)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.69276\n",
      "valid_acc (0.17534414176323834, 0.8916)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.67606\n",
      "valid_acc (0.17645109142150517, 0.861)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.66821\n",
      "valid_acc (0.17664569956315501, 0.8168)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.6565\n",
      "valid_acc (0.17676290974273848, 0.861)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.66283\n",
      "valid_acc (0.1762104050307462, 0.8718)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.64975\n",
      "valid_acc (0.17585472554802931, 0.8758)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.64359\n",
      "valid_acc (0.17525420377188172, 0.8518)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.64117\n",
      "valid_acc (0.17403009511044057, 0.8672)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.62851\n",
      "valid_acc (0.17317217076665198, 0.8936)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.61378\n",
      "valid_acc (0.17410749829633687, 0.8904)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.62469\n",
      "valid_acc (0.17373711010548176, 0.8942)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.63676\n",
      "valid_acc (0.17306127413487352, 0.85)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.61593\n",
      "valid_acc (0.17240629147218195, 0.9012)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.61213\n",
      "valid_acc (0.17340024501447129, 0.9014)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.62446\n",
      "valid_acc (0.173755958838966, 0.9012)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.59769\n",
      "valid_acc (0.17331030930772209, 0.8904)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.59103\n",
      "valid_acc (0.17212375608989258, 0.9022)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.58571\n",
      "valid_acc (0.17237280264062288, 0.8842)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.5908\n",
      "valid_acc (0.17329775521491694, 0.891)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.5778\n",
      "valid_acc (0.17330187174501077, 0.861)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.56428\n",
      "valid_acc (0.17289379488728115, 0.8716)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.54869\n",
      "valid_acc (0.17205684443891114, 0.893)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.56037\n",
      "valid_acc (0.17171157671344028, 0.9038)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.5714\n",
      "valid_acc (0.1723225670329277, 0.8192)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.559\n",
      "valid_acc (0.17216982025658867, 0.834)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.55288\n",
      "valid_acc (0.17153159224022721, 0.8934)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.53485\n",
      "valid_acc (0.17101485531582655, 0.9066)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.53647\n",
      "valid_acc (0.17174501922417371, 0.9066)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.52251\n",
      "valid_acc (0.17232678248259894, 0.8852)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.52891\n",
      "valid_acc (0.17178479978998118, 0.863)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.52583\n",
      "valid_acc (0.17059006934482676, 0.8544)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.52862\n",
      "valid_acc (0.17077414747270114, 0.8884)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.52121\n",
      "valid_acc (0.17069245474563022, 0.8638)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.52452\n",
      "valid_acc (0.17145209668513575, 0.8864)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.51478\n",
      "valid_acc (0.1714186335013386, 0.902)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.50578\n",
      "valid_acc (0.17117382531579092, 0.9118)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.51796\n",
      "valid_acc (0.1711340896812841, 0.9018)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.51224\n",
      "valid_acc (0.17024682813618441, 0.9038)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.50252\n",
      "valid_acc (0.17002087119904377, 0.8988)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.48961\n",
      "valid_acc (0.17020500883853232, 0.892)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.48045\n",
      "valid_acc (0.17032212280538411, 0.8662)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.5151\n",
      "valid_acc (0.16889929763777189, 0.8476)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.49113\n",
      "valid_acc (0.16830501730162045, 0.9092)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.4919\n",
      "valid_acc (0.16923415902626732, 0.915)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.47071\n",
      "valid_acc (0.17044986239867427, 0.9054)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.47372\n",
      "valid_acc (0.1695500773020891, 0.9096)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.47002\n",
      "valid_acc (0.1688531886697453, 0.9132)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.46508\n",
      "valid_acc (0.16894736397818463, 0.91)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.46091\n",
      "valid_acc (0.16881137009543573, 0.909)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.47028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_acc (0.16918378136556689, 0.9024)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.47381\n",
      "valid_acc (0.16918384303523215, 0.895)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.46731\n",
      "valid_acc (0.16896626968981235, 0.9084)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.45419\n",
      "valid_acc (0.16873607230263515, 0.8978)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.44629\n",
      "valid_acc (0.16748679715582904, 0.9168)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.4453\n",
      "valid_acc (0.16807690854013829, 0.9228)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.45142\n",
      "valid_acc (0.1680811026641589, 0.9176)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.43476\n",
      "valid_acc (0.16839292511784074, 0.9088)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.43912\n",
      "valid_acc (0.16818367182110425, 0.9204)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.442\n",
      "valid_acc (0.16749939330791366, 0.9186)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.44281\n",
      "valid_acc (0.1678801887444295, 0.9086)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.43336\n",
      "valid_acc (0.16886574382246294, 0.9018)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.42673\n",
      "valid_acc (0.16788443672693956, 0.8796)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.40961\n",
      "valid_acc (0.16722108300764216, 0.8928)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.42669\n",
      "valid_acc (0.16647204310006639, 0.9164)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.42061\n",
      "valid_acc (0.16754750824697756, 0.925)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.44969\n",
      "valid_acc (0.168150126948099, 0.908)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.42414\n",
      "valid_acc (0.16801413587864969, 0.9088)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.40736\n",
      "valid_acc (0.16821922244483609, 0.8844)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.40858\n",
      "valid_acc (0.16738425562744833, 0.9232)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.39701\n",
      "valid_acc (0.16743239911790816, 0.9182)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.40265\n",
      "valid_acc (0.16638403626067405, 0.9112)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.41221\n",
      "valid_acc (0.1663611007400983, 0.9292)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.39711\n",
      "valid_acc (0.16670214696679653, 0.9288)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.3986\n",
      "valid_acc (0.16720854017417117, 0.9028)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.41856\n",
      "valid_acc (0.16704946926068551, 0.911)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.38908\n",
      "valid_acc (0.16677124398871318, 0.9318)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.38796\n",
      "valid_acc (0.16638405865075842, 0.925)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.39602\n",
      "valid_acc (0.16706207912396728, 0.922)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.40026\n",
      "valid_acc (0.16711022667785053, 0.925)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.37808\n",
      "valid_acc (0.16722531181944256, 0.9128)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.37918\n",
      "valid_acc (0.16667913631866207, 0.9264)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.38509\n",
      "valid_acc (0.16673557805260061, 0.8948)\n",
      "best_epoch (valid) 27\n",
      "last loss 2.38525\n",
      "valid_acc (0.16624392722869599, 0.9032)\n",
      "best_epoch (valid) 27\n",
      "last loss 7.93496\n",
      "valid_acc 0.786\n",
      "epoch 0 train_acc 0.8397205588822355 valid_acc 0.786 test_acc 0.725748502994012\n",
      "best_epoch (valid) 0\n",
      "last loss 4.36973\n",
      "valid_acc 0.7812\n",
      "best_epoch (valid) 0\n",
      "last loss 0.190494\n",
      "valid_acc 0.778\n",
      "best_epoch (valid) 0\n",
      "last loss 0.0298098\n",
      "valid_acc 0.782\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00843308\n",
      "valid_acc 0.7844\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00414254\n",
      "valid_acc 0.7852\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00254892\n",
      "valid_acc 0.7854\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00166991\n",
      "valid_acc 0.787\n",
      "epoch 7 train_acc 1.0 valid_acc 0.787 test_acc 0.7419161676646706\n",
      "best_epoch (valid) 7\n",
      "last loss 0.00109665\n",
      "valid_acc 0.787\n",
      "best_epoch (valid) 7\n",
      "last loss 0.000676324\n",
      "valid_acc 0.7864\n",
      "best_epoch (valid) 7\n",
      "last loss 0.000426623\n",
      "valid_acc 0.7866\n",
      "best_epoch (valid) 7\n",
      "last loss 0.000271905\n",
      "valid_acc 0.7862\n",
      "best_epoch (valid) 7\n",
      "last loss 4.33919e-05\n",
      "valid_acc 0.784\n",
      "best_epoch (valid) 7\n",
      "last loss 9.53673e-06\n",
      "valid_acc 0.7844\n",
      "best_epoch (valid) 7\n",
      "last loss 4.05311e-06\n",
      "valid_acc 0.7846\n",
      "best_epoch (valid) 7\n",
      "last loss 2.02656e-06\n",
      "valid_acc 0.7844\n",
      "best_epoch (valid) 7\n",
      "last loss 1.3113e-06\n",
      "valid_acc 0.7856\n",
      "best_epoch (valid) 7\n",
      "last loss 8.34465e-07\n",
      "valid_acc 0.7858\n",
      "best_epoch (valid) 7\n",
      "last loss 4.76837e-07\n",
      "valid_acc 0.7862\n",
      "best_epoch (valid) 7\n",
      "last loss 1.19209e-07\n",
      "valid_acc 0.787\n",
      "best_epoch (valid) 7\n",
      "last loss 1.19209e-07\n",
      "valid_acc 0.786\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7864\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7854\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7852\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7854\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7854\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7862\n",
      "best_epoch (valid) 7\n",
      "last loss 0.0\n",
      "valid_acc 0.7854\n",
      "best_epoch (valid) 7\n"
     ]
    }
   ],
   "source": [
    "def generate_data(corpus_sents, corpus_labels, max_length, extend_vocabulary):\n",
    "    sen_en = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_de = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_en_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_de_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_label = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "\n",
    "    def get_random_sequence(sent, max_length):\n",
    "        x = np.full((max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "        for i, word in enumerate(sent):\n",
    "            if word in extend_vocabulary:\n",
    "                x[i] = extend_vocabulary[word]\n",
    "            else:\n",
    "                x[i] = extend_vocabulary[\"<unk>\"]\n",
    "        return x\n",
    "\n",
    "    for i in range(len(corpus_sents)):\n",
    "        l = min(len(corpus_sents[i]), max_length-2) + 2\n",
    "        sen_en[i, :] = get_random_sequence([\"<s>\"] + corpus_sents[i][:max_length-2] + [\"</s>\"], max_length)\n",
    "        sen_de[i, :l-1] = sen_en[i, 1:l]\n",
    "        sen_en_length[i] = l\n",
    "        sen_de_length[i] = l - 1\n",
    "    \n",
    "    for i in range(len(corpus_sents)):\n",
    "        sen_label[i] = corpus_labels[i]\n",
    "    \n",
    "    return sen_en, sen_de, sen_en_length, sen_de_length, sen_label\n",
    "\n",
    "def get_total_accuracy_sa(data_sen_en, data_sen_de, data_sen_en_length, data_sen_de_length, extend_vocabulary, pretrained_lstm):\n",
    "    n_hit_word, n_hit_length = 0, 0\n",
    "    n_total_word, n_total_length = 0, 0\n",
    "    cur_idx = 0\n",
    "    while cur_idx < data_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = data_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = data_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        _, predictions = pretrained_lstm.predict_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "        cur_acc_word, cur_acc_length = evaluate_sa(batch_sen_de, batch_sen_de_length, predictions, extend_vocabulary)\n",
    "        n_hit_word += cur_acc_word * np.sum(batch_sen_en_length)\n",
    "        n_total_word += np.sum(batch_sen_en_length)\n",
    "        n_hit_length += cur_acc_length * batch_sen_en.shape[0]\n",
    "        n_total_length += batch_sen_en.shape[0]\n",
    "    return 1. * n_hit_word / n_total_word, 1. * n_hit_length / n_total_length\n",
    "\n",
    "def get_total_accuracy_clf(data_sen_en, data_sen_en_length, data_label, pretrained_lstm):\n",
    "    cur_idx = 0\n",
    "    hit, tot = 0, 0\n",
    "    while cur_idx < valid_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]   \n",
    "        batch_label = data_label[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_pred = pretrained_lstm.predict_clf(batch_sen_en, batch_sen_en_length)\n",
    "        assert batch_pred.shape[0] == batch_sen_en.shape[0]\n",
    "        cur_acc = evaluate_clf(batch_pred, batch_label)\n",
    "        hit += cur_acc * batch_pred.shape[0]\n",
    "        tot += batch_pred.shape[0]\n",
    "        cur_idx += n_batch_size\n",
    "    return 1. * hit / tot\n",
    "    \n",
    "# hyperparameter\n",
    "vocabulary_size = 20000\n",
    "origin_vocabulary = {}\n",
    "for word, n in corpus_counter.most_common(vocabulary_size):\n",
    "    origin_vocabulary[\"{}\".format(word)] = len(origin_vocabulary)\n",
    "extend_vocabulary = dict(origin_vocabulary)\n",
    "for w in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]:\n",
    "    extend_vocabulary[w] = len(extend_vocabulary)\n",
    "reverse_extend_vocabulary = {v: k for k, v in extend_vocabulary.items()}\n",
    "imdb_label = {\"0\": 0, \"1\": 1}\n",
    "\n",
    "state_size=128\n",
    "n_max_length=100\n",
    "n_batch_size=30\n",
    "\n",
    "# generate training/testing data\n",
    "preprocess_sents = []\n",
    "preprocess_labels = []\n",
    "random.shuffle(train_preprocess)\n",
    "for sent, label in train_preprocess:\n",
    "    preprocess_sents.append(sent)\n",
    "    preprocess_labels.append(label)\n",
    "n_train = int(len(preprocess_sents) * 0.8)\n",
    "n_valid = len(preprocess_sents) - n_train\n",
    "\n",
    "print(\"n_train\", n_train, \"n_valid\", n_valid, \"n_test\", len(test_preprocess))\n",
    "\n",
    "train_sents = preprocess_sents[:n_train]\n",
    "train_labels = preprocess_labels[:n_train]\n",
    "valid_sents = preprocess_sents[n_train:]\n",
    "valid_labels = preprocess_labels[n_train:]\n",
    "test_sents, test_labels = [], []\n",
    "for sent, label in test_preprocess:\n",
    "    test_sents.append(sent)\n",
    "    test_labels.append(label)\n",
    "\n",
    "train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, train_sen_en_label = generate_data(\n",
    "    train_sents, train_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, valid_sen_en_label = generate_data(\n",
    "    valid_sents, valid_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, test_sen_en_label = generate_data(\n",
    "    test_sents, test_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "\"\"\"\n",
    "print(train_sen_en[0])\n",
    "print(preprocess_sents[0])\n",
    "print([reverse_extend_vocabulary[i] for i in train_sen_en[0]])\n",
    "print(valid_sen_en[0])\n",
    "print(preprocess_sents[n_train])\n",
    "print([reverse_extend_vocabulary[i] for i in valid_sen_en[0]])\n",
    "\"\"\"\n",
    "pretrained_lstm = EncoderDecoder(vocabulary=extend_vocabulary, label=imdb_label, state_size=state_size, n_max_length=n_max_length)\n",
    "\n",
    "# train_sa\n",
    "best_epoch = None\n",
    "best_valid_acc = None\n",
    "early_stopping = 20\n",
    "\n",
    "for epoch in range(200):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = train_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = train_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent = pretrained_lstm.train_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"last loss\", loss)\n",
    "    valid_acc = get_total_accuracy_sa(\n",
    "        valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "    )\n",
    "    print(\"valid_acc\", valid_acc)\n",
    "    if best_valid_acc is None or best_valid_acc < valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        train_acc = get_total_accuracy_sa(\n",
    "            train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "        )\n",
    "        test_acc = get_total_accuracy_sa(\n",
    "            test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "        )\n",
    "        print(\"epoch\", epoch, \"train_acc\", train_acc, \"valid_acc\", valid_acc, \"test_acc\", test_acc)\n",
    "    print(\"best_epoch (valid)\", best_epoch)\n",
    "    \"\"\"\n",
    "    if epoch - best_epoch == early_stopping:\n",
    "        break\n",
    "    \"\"\"\n",
    "\n",
    "# train_clf\n",
    "best_epoch = None\n",
    "best_valid_acc = None\n",
    "early_stopping = 20\n",
    "\n",
    "for epoch in range(200):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_label = train_sen_en_label[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, pred = pretrained_lstm.train_clf(\n",
    "            batch_sen_en, batch_sen_en_length, batch_sen_en_label\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"last loss\", loss)\n",
    "    valid_acc = get_total_accuracy_clf(\n",
    "        valid_sen_en, valid_sen_en_length, valid_sen_en_label, pretrained_lstm\n",
    "    )\n",
    "    print(\"valid_acc\", valid_acc)\n",
    "    if best_valid_acc is None or best_valid_acc < valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        train_acc = get_total_accuracy_clf(\n",
    "            train_sen_en, train_sen_en_length, train_sen_en_label, pretrained_lstm\n",
    "        )\n",
    "        test_acc = get_total_accuracy_clf(\n",
    "            test_sen_en, test_sen_en_length, test_sen_en_label, pretrained_lstm\n",
    "        )\n",
    "        print(\"epoch\", epoch, \"train_acc\", train_acc, \"valid_acc\", valid_acc, \"test_acc\", test_acc)\n",
    "    print(\"best_epoch (valid)\", best_epoch)\n",
    "    if epoch - best_epoch == early_stopping:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
