{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_pos) 12500\n",
      "len(train_neg) 12500\n",
      "len(test_pos) 12500\n",
      "len(test_neg) 12500\n",
      "len(unlabeled_data) 50000\n",
      "train_pos[0] ['``', 'The', 'Last', 'Hard', 'Men', \"''\", 'is', 'a', 'typical', 'western', 'for', 'the', '70', \"'s\", '.', 'Most', 'of', 'them', 'seem', 'to', 'be', 'inspired', 'by', 'Sam', 'Peckinpah', '.', 'Also', 'this', 'one', ',', 'but', 'Director', 'Andrew', 'McLaglan', 'is', 'a', 'John', 'Ford', 'Pupil', 'and', 'this', 'can', 'be', 'obviously', 'shown', 'in', 'many', 'scenes', '.', 'IMO', 'the', 'beginning', 'is', 'very', 'good', '.', 'In', 'a', 'certain', 'way', 'McLaglan', 'wanted', 'to', 'show', 'the', 'audience', 'a', 'travel', 'from', 'the', 'civilization', 'to', 'the', 'wilderness', '.', 'In', 'the', 'third', 'part', 'there', 'are', 'some', 'illogical', 'flaws', 'and', 'I', 'complain', 'a', 'bit', 'about', 'Charlton', 'Heston', '.', 'He', 'has', 'to', 'play', 'an', 'old', 'ex-lawman', 'named', 'Sam', 'Burgade', 'but', 'he', 'is', 'in', 'a', 'fantastic', 'physical', 'shape', '.', 'I', 'never', 'got', 'the', 'feeling', 'that', 'he', 'really', 'has', 'problems', 'to', 'climb', 'on', 'a', 'horse', 'or', 'on', 'a', 'rock', '.', 'For', 'me', 'he', 'did', \"n't\", 'looks', 'very', 'motivated', 'as', 'he', 'usual', 'do', 'in', 'most', 'of', 'his', 'epic', 'movies', '.', 'Same', 'goes', 'to', 'the', 'beautiful', 'Barbara', 'Hershey', 'who', 'is', 'playing', 'the', 'sheriff', \"'s\", 'daughter', '.', 'Maybe', 'both', 'had', 'troubles', 'with', 'the', 'director', 'or', 'were', 'unhappy', 'with', 'their', 'roles', '.', 'Hershey', 'and', 'Coburn', 'are', 'not', 'showing', 'their', 'best', 'but', 'they', 'are', 'still', 'good', '.', 'If', 'the', 'scriptwriter', 'had', 'John', 'Wayne', 'in', 'their', 'mind', 'as', 'Sam', 'Burgade', '?', 'Also', 'Michael', 'Parks', 'as', 'modern', 'sheriff', 'is', 'a', 'bit', 'underused', 'in', 'his', 'role', '.', 'On', 'the', 'other', 'Hand', 'there', 'is', 'James', 'Coburn', 'as', 'outlaw', 'Zach', 'Provo', '.', 'Coburn', 'is', 'a', 'really', 'great', 'villain', 'in', 'this', 'one', '.', 'He', 'is', 'portraying', 'the', 'bad', 'guy', 'between', 'maniac', 'hate', 'and', 'cleverness', '.', 'His', 'role', 'and', 'his', 'acting', 'is', 'the', 'best', 'of', 'the', 'movie.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'Landscapes', 'and', 'Shootouts', 'are', 'terrific', '.', 'The', 'shootings', 'scenes', 'are', 'bloody', 'and', 'the', 'violence', 'looks', 'realistic', '.', 'Zach', 'Provo', 'and', 'his', 'gang', 'had', 'some', 'gory', 'and', 'violent', 'scenes', '.', 'What', 'I', 'miss', 'is', 'the', 'typical', 'western', 'action', 'in', 'the', 'middle', 'of', 'the', 'movie', '.', 'I', 'would', 'have', 'appreciated', 'a', 'bank', 'robbery', 'or', 'something', 'similar', '.', 'Overall', 'it', \"'s\", 'an', 'entertaining', 'western', 'flick', '.', 'Not', 'a', 'great', 'movie', 'but', 'above', 'the', 'average', 'because', 'of', 'a', 'great', 'Coburn', ',', 'a', 'very', 'good', 'beginning', 'and', 'some', 'gory', 'and', 'violent', 'scenes', '.']\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections\n",
    "import re\n",
    "import os\n",
    "import nltk\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_sent_in_directory(path_directory):\n",
    "    sents = []\n",
    "    for filename in os.listdir(path_directory):\n",
    "        if os.path.isfile(os.path.join(path_directory, filename)):\n",
    "            with open(os.path.join(path_directory, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    sents.append(nltk.tokenize.word_tokenize(line))\n",
    "    return sents\n",
    "                    \n",
    "                \n",
    "train_pos = get_sent_in_directory(\"./aclImdb/train/pos\")\n",
    "train_neg = get_sent_in_directory(\"./aclImdb/train/neg\")\n",
    "test_pos = get_sent_in_directory(\"./aclImdb/test/pos\")\n",
    "test_neg = get_sent_in_directory(\"./aclImdb/test/neg\")\n",
    "unlabeled_data = get_sent_in_directory(\"./aclImdb/train/unsup\")\n",
    "\n",
    "print(\"len(train_pos)\", len(train_pos))\n",
    "print(\"len(train_neg)\", len(train_neg))\n",
    "print(\"len(test_pos)\", len(test_pos))\n",
    "print(\"len(test_neg)\", len(test_neg))\n",
    "print(\"len(unlabeled_data)\", len(unlabeled_data))\n",
    "print(\"train_pos[0]\", train_pos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique word: 251952\n",
      "# of sents: 25000\n",
      "max len(sents[i]): 2502\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGSUAoCISEYpAIiIZNT/BxT2MyCQ1jP1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff307878a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\nt = None\\nfor x in corpus_counter.most_common(60000):\\n    t = x\\nprint(\"x\", x)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_counter = collections.Counter()\n",
    "train_preprocess = []\n",
    "test_preprocess = []\n",
    "unlabeled_preprocess = []\n",
    "for sent in train_pos:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    train_preprocess.append((res, 1))\n",
    "for sent in train_neg:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    train_preprocess.append((res, 0))\n",
    "for sent in test_pos:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    test_preprocess.append((res, 1))\n",
    "for sent in test_neg:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    test_preprocess.append((res, 0))\n",
    "for sent in unlabeled_data:\n",
    "    res = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            corpus_counter[word.lower()] += 1\n",
    "            res.append(word.lower())\n",
    "    unlabeled_preprocess.append(res)\n",
    "\n",
    "print(\"unique word:\", len(corpus_counter))\n",
    "print(\"# of sents:\", len(train_preprocess))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s, _ in train_preprocess]))\n",
    "\n",
    "train_length = []\n",
    "for sent, label in train_preprocess:\n",
    "    train_length.append(len(sent))\n",
    "train_length = sorted(train_length)\n",
    "stat_x, stat_y = [], []\n",
    "for i, l in enumerate(train_length):\n",
    "    stat_x.append(i / len(train_length))\n",
    "    stat_y.append(l)\n",
    "plt.plot(stat_x, stat_y, 'r-')\n",
    "plt.show()\n",
    "\"\"\"\n",
    "t = None\n",
    "for x in corpus_counter.most_common(60000):\n",
    "    t = x\n",
    "print(\"x\", x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Brown Corpus to train an seq2seq autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, vocabulary={}, label={}, state_size=64, n_max_length=30):     \n",
    "        self.state_size = state_size\n",
    "        self.n_max_length = n_max_length\n",
    "        self.vocabulary = vocabulary\n",
    "        self.label = label\n",
    "        self.reverse_vocabulary = {k: v for k, v in vocabulary.items()}\n",
    "        \n",
    "        ######################\n",
    "        # Graph Construction #\n",
    "        ######################\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #self.sen_en = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_en\")\n",
    "            #self.sen_de = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_de\")\n",
    "            self.sen_en = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_en\")\n",
    "            self.sen_de = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_de\")\n",
    "            self.sen_en_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_length\")\n",
    "            self.sen_de_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_de_length\")\n",
    "            self.sen_en_label = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_label\")\n",
    "                        \n",
    "            batch_size_en = tf.shape(self.sen_en)[0]\n",
    "            batch_size_de = tf.shape(self.sen_de)[0]\n",
    "            batch_max_length_de = tf.shape(self.sen_de)[1]\n",
    "            \n",
    "            # TODO sen_en_embedding could also be self-trained embedding: embedding_lookup\n",
    "            self.embedding = tf.Variable(tf.random_uniform([len(self.vocabulary), self.state_size], -1.0, 1.0), dtype=tf.float32)\n",
    "            #self.sen_en_embedding = tf.one_hot(self.sen_en, len(self.vocabulary))\n",
    "            #self.sen_de_embedding = tf.one_hot(self.sen_de, len(self.vocabulary))\n",
    "            self.sen_en_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_en)\n",
    "            self.sen_de_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_de)\n",
    "            \n",
    "            # build encoder decoder structure\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en_init = self.cell_en.zero_state(batch_size_en, tf.float32)\n",
    "                self.h_state_en, self.final_state_en = tf.nn.dynamic_rnn(\n",
    "                    self.cell_en,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_en_init,\n",
    "                )\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de_init = self.final_state_en\n",
    "                self.h_state_de, self.final_state_de = tf.nn.dynamic_rnn(\n",
    "                    self.cell_de,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state=self.cell_de_init,\n",
    "                )\n",
    "            \n",
    "            # autoencoder softmax\n",
    "            with tf.variable_scope(\"softmax\") as scope:\n",
    "                W = tf.get_variable(\"W\", [self.state_size, len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))\n",
    "                b = tf.get_variable(\"b\", [len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))               \n",
    "            self.logits = tf.reshape(\n",
    "                tf.add(tf.matmul(tf.reshape(self.h_state_de, (-1, self.state_size)), W), b),\n",
    "                shape=(-1, batch_max_length_de, len(self.vocabulary))\n",
    "            )\n",
    "            self.pred_sa = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # construct loss and train op\n",
    "            self.cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.sen_de,\n",
    "                logits=self.logits\n",
    "            )        \n",
    "            self.mask = tf.sequence_mask(self.sen_de_length, maxlen=batch_max_length_de)\n",
    "            self.loss_sa = tf.reduce_mean(\n",
    "                tf.divide(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.where(\n",
    "                            self.mask,\n",
    "                            self.cross_ent,\n",
    "                            tf.zeros_like(self.cross_ent)\n",
    "                        ), 1\n",
    "                    ),\n",
    "                    tf.to_float(self.sen_de_length)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # classifier layer\n",
    "            with tf.variable_scope(\"classifier\") as scope:\n",
    "                clf_input = tf.concat([self.final_state_en.h, self.final_state_en.c], axis=1)\n",
    "                clf_dense = tf.layers.dense(clf_input, 30, activation=tf.nn.relu)\n",
    "                self.clf_output = tf.layers.dense(clf_dense, len(self.label))\n",
    "            \n",
    "            self.pred_clf = tf.nn.softmax(self.clf_output)\n",
    "                   \n",
    "            # construct classifier loss\n",
    "            self.loss_clf = tf.reduce_sum(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=self.sen_en_label,\n",
    "                    logits=self.clf_output,\n",
    "                )\n",
    "            )        \n",
    "            \n",
    "            # Optimization\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train_sa = optimizer.minimize(self.loss_sa)\n",
    "            self.op_train_clf = optimizer.minimize(self.loss_clf)\n",
    "               \n",
    "            # initializer\n",
    "            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph,\n",
    "                config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )           \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "    def train_sa(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        assert batch_sen_en.shape[1] == self.n_max_length  # training always input same length as self.n_max_length\n",
    "        _, loss, pred, sen_en_embedding, mask, cross_ent = self.sess.run(\n",
    "            [self.op_train_sa, self.loss_sa, self.pred_sa, self.sen_en_embedding, self.mask, self.cross_ent],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, pred, sen_en_embedding, mask, cross_ent\n",
    "        \n",
    "    def predict_sa(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        loss, pred = self.sess.run(\n",
    "            [self.loss_sa, self.pred_sa],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, pred\n",
    "    \n",
    "    def encode_sa(self, batch_sen_en, batch_sen_en_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        Returns\n",
    "        -------\n",
    "        #batch_state_en: numpy, shape=(n, self.state_size), dtype=float\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        \"\"\"\n",
    "        batch_state_en = self.sess.run(\n",
    "            self.final_state_en,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return batch_state_en\n",
    "    \n",
    "    def decode_sa(self, batch_state_en):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        Returns\n",
    "        -------\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        \"\"\"\n",
    "        batch_size = batch_state_en.c.shape[0]\n",
    "        batch_sen_de = np.empty([batch_size, self.n_max_length], dtype=np.int32)\n",
    "        \n",
    "        tmp_sen_de = np.empty([batch_size, 1], dtype=np.int32)\n",
    "        tmp_sen_de_length = np.ones([batch_size], dtype=np.int32)\n",
    "        tmp_sen_de[:] = self.vocabulary[\"<s>\"]\n",
    "        tmp_last_state = batch_state_en\n",
    "        for i in range(self.n_max_length):\n",
    "            tmp_predict, tmp_last_state = self.sess.run(\n",
    "                [self.pred_sa, self.final_state_de],\n",
    "                feed_dict={\n",
    "                    self.cell_de_init: tmp_last_state,\n",
    "                    self.sen_de: tmp_sen_de,\n",
    "                    self.sen_de_length: tmp_sen_de_length,\n",
    "                }\n",
    "            )\n",
    "            tmp_sen_de = np.argmax(tmp_predict, axis=2)\n",
    "            batch_sen_de[:,i] = tmp_sen_de[:,0]\n",
    "           \n",
    "        return batch_sen_de\n",
    "    \n",
    "    def train_clf(self, batch_sen_en, batch_sen_en_length, batch_label):\n",
    "        _, loss, pred = self.sess.run(\n",
    "            [self.op_train_clf, self.loss_clf, self.pred_clf],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_en_label: batch_label\n",
    "            }\n",
    "        )\n",
    "        return loss, pred\n",
    "    \n",
    "    def predict_clf(self, batch_sen_en, batch_sen_en_length):\n",
    "        pred = self.sess.run(\n",
    "            self.pred_clf,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return pred\n",
    "\n",
    "    \n",
    "def evaluate_sa(batch_sen_en, batch_sen_en_length, batch_prediction, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "    batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "    batch_prediction: numpy, shape=(n, max_length, len(vocabulary))\n",
    "    \"\"\"\n",
    "    assert batch_sen_en.shape[0] == batch_prediction.shape[0]\n",
    "    acc_word = 0\n",
    "    acc_sen_end = 0\n",
    "    for i in range(batch_sen_en.shape[0]):\n",
    "        is_first_end = False\n",
    "        for j in range(batch_sen_en_length[i]):\n",
    "            cur_pred_word = np.argmax(batch_prediction[i, j])\n",
    "            if cur_pred_word == batch_sen_en[i, j]:\n",
    "                acc_word += 1\n",
    "                if not is_first_end and cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                    acc_sen_end += 1\n",
    "            if cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                is_first_end = True\n",
    "    return 1. * acc_word / np.sum(batch_sen_en_length), 1. * acc_sen_end / batch_sen_en.shape[0]\n",
    "\n",
    "\n",
    "def evaluate_clf(batch_pred, batch_label):\n",
    "    assert batch_pred.shape[0] == batch_label.shape[0]\n",
    "    hit, tot = 0, 0\n",
    "    for i in range(batch_label.shape[0]):\n",
    "        if np.argmax(batch_pred[i]) == batch_label[i]:\n",
    "            hit += 1\n",
    "        tot += 1\n",
    "    return 1. * hit / tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 20000 n_valid 5000 n_test 25000\n",
      "last loss 5.58671\n",
      "valid_acc (0.16975503552151669, 0.0562)\n",
      "epoch 0 train_acc (0.17081212568663412, 0.0589) valid_acc (0.16975503552151669, 0.0562) test_acc (0.17025625644035475, 0.05716)\n",
      "best_epoch (valid) 0\n",
      "last loss 5.25354\n",
      "valid_acc (0.19516214148185898, 0.1218)\n",
      "epoch 1 train_acc (0.19795553639981706, 0.1208) valid_acc (0.19516214148185898, 0.1218) test_acc (0.19590410047851695, 0.12236)\n",
      "best_epoch (valid) 1\n",
      "last loss 5.0742\n",
      "valid_acc (0.20701821962849243, 0.1704)\n",
      "epoch 2 train_acc (0.21126114097680934, 0.17175) valid_acc (0.20701821962849243, 0.1704) test_acc (0.20709539361708754, 0.17432)\n",
      "best_epoch (valid) 2\n",
      "last loss 4.94456\n",
      "valid_acc (0.21368073016972719, 0.2176)\n",
      "epoch 3 train_acc (0.21974338193483109, 0.21015) valid_acc (0.21368073016972719, 0.2176) test_acc (0.21407629440486989, 0.20832)\n",
      "best_epoch (valid) 3\n",
      "last loss 4.84034\n",
      "valid_acc (0.21992684821428835, 0.5)\n",
      "epoch 4 train_acc (0.22746201197406896, 0.5047) valid_acc (0.21992684821428835, 0.5) test_acc (0.22013767982764709, 0.49792)\n",
      "best_epoch (valid) 4\n",
      "last loss 4.75951\n",
      "valid_acc (0.22445502610170853, 0.1902)\n",
      "epoch 5 train_acc (0.23298736918927682, 0.18305) valid_acc (0.22445502610170853, 0.1902) test_acc (0.22432619931338021, 0.18612)\n",
      "best_epoch (valid) 5\n",
      "last loss 4.68739\n",
      "valid_acc (0.22896844987644216, 0.4426)\n",
      "epoch 6 train_acc (0.23885977091604801, 0.4508) valid_acc (0.22896844987644216, 0.4426) test_acc (0.22881574758576378, 0.44436)\n",
      "best_epoch (valid) 6\n",
      "last loss 4.63199\n",
      "valid_acc (0.23215953076767765, 0.4814)\n",
      "epoch 7 train_acc (0.2434123831146226, 0.4931) valid_acc (0.23215953076767765, 0.4814) test_acc (0.23188270057138133, 0.48892)\n",
      "best_epoch (valid) 7\n",
      "last loss 4.57942\n",
      "valid_acc (0.23355731043686837, 0.3348)\n",
      "epoch 8 train_acc (0.24639117828707502, 0.3498) valid_acc (0.23355731043686837, 0.3348) test_acc (0.23321685438440176, 0.34888)\n",
      "best_epoch (valid) 8\n",
      "last loss 4.52564\n",
      "valid_acc (0.23421638311440163, 0.3414)\n",
      "epoch 9 train_acc (0.24805227743023045, 0.34925) valid_acc (0.23421638311440163, 0.3414) test_acc (0.23349946080668182, 0.34556)\n",
      "best_epoch (valid) 9\n",
      "last loss 4.48792\n",
      "valid_acc (0.23758744892013042, 0.5728)\n",
      "epoch 10 train_acc (0.25274343159644769, 0.5894) valid_acc (0.23758744892013042, 0.5728) test_acc (0.23618993414260214, 0.57008)\n",
      "best_epoch (valid) 10\n",
      "last loss 4.44929\n",
      "valid_acc (0.23972386652837802, 0.8402)\n",
      "epoch 11 train_acc (0.2563814110823236, 0.8464) valid_acc (0.23972386652837802, 0.8402) test_acc (0.23836256660763983, 0.83108)\n",
      "best_epoch (valid) 11\n",
      "last loss 4.41243\n",
      "valid_acc (0.24057757605944058, 0.9146)\n",
      "epoch 12 train_acc (0.25894677122804044, 0.9368) valid_acc (0.24057757605944058, 0.9146) test_acc (0.23933018754749313, 0.92292)\n",
      "best_epoch (valid) 12\n",
      "last loss 4.36927\n",
      "valid_acc (0.24241484998606258, 0.8652)\n",
      "epoch 13 train_acc (0.26227734942942221, 0.87465) valid_acc (0.24241484998606258, 0.8652) test_acc (0.24124039269689421, 0.8608)\n",
      "best_epoch (valid) 13\n",
      "last loss 4.33593\n",
      "valid_acc (0.24357829648318363, 0.9586)\n",
      "epoch 14 train_acc (0.26506122849538194, 0.9743) valid_acc (0.24357829648318363, 0.9586) test_acc (0.24230496153037615, 0.96076)\n",
      "best_epoch (valid) 14\n",
      "last loss 4.30479\n",
      "valid_acc (0.24458261420160365, 0.9498)\n",
      "epoch 15 train_acc (0.26767939501531807, 0.9662) valid_acc (0.24458261420160365, 0.9498) test_acc (0.24340935605127359, 0.95032)\n",
      "best_epoch (valid) 15\n",
      "last loss 4.28282\n",
      "valid_acc (0.24386699296076045, 0.9618)\n",
      "best_epoch (valid) 15\n",
      "last loss 4.25483\n",
      "valid_acc (0.24504301149913285, 0.9598)\n",
      "epoch 17 train_acc (0.27156354099454411, 0.97555) valid_acc (0.24504301149913285, 0.9598) test_acc (0.24421935684318483, 0.96064)\n",
      "best_epoch (valid) 17\n",
      "last loss 4.23058\n",
      "valid_acc (0.24598880936869194, 0.946)\n",
      "epoch 18 train_acc (0.27434530309457922, 0.96225) valid_acc (0.24598880936869194, 0.946) test_acc (0.24516862297541811, 0.94424)\n",
      "best_epoch (valid) 18\n",
      "last loss 4.20734\n",
      "valid_acc (0.24610179601450202, 0.8944)\n",
      "epoch 19 train_acc (0.27603414261369125, 0.9137) valid_acc (0.24610179601450202, 0.8944) test_acc (0.24527553584157274, 0.89308)\n",
      "best_epoch (valid) 19\n",
      "last loss 4.18827\n",
      "valid_acc (0.24689073931550146, 0.929)\n",
      "epoch 20 train_acc (0.27811967278718613, 0.94885) valid_acc (0.24689073931550146, 0.929) test_acc (0.24563274367250773, 0.92712)\n",
      "best_epoch (valid) 20\n",
      "last loss 4.16909\n",
      "valid_acc (0.24691999824051791, 0.9074)\n",
      "epoch 21 train_acc (0.28001234019727156, 0.9244) valid_acc (0.24691999824051791, 0.9074) test_acc (0.24578283650005345, 0.9042)\n",
      "best_epoch (valid) 21\n",
      "last loss 4.14621\n",
      "valid_acc (0.24703296643871481, 0.9436)\n",
      "epoch 22 train_acc (0.28156841353988349, 0.96385) valid_acc (0.24703296643871481, 0.9436) test_acc (0.2456394155939699, 0.9462)\n",
      "best_epoch (valid) 22\n",
      "last loss 4.13845\n",
      "valid_acc (0.24777373316209478, 0.9452)\n",
      "epoch 23 train_acc (0.28393098919763993, 0.9704) valid_acc (0.24777373316209478, 0.9452) test_acc (0.24625278977263604, 0.95204)\n",
      "best_epoch (valid) 23\n",
      "last loss 4.11598\n",
      "valid_acc (0.24750392575607835, 0.9604)\n",
      "best_epoch (valid) 23\n",
      "last loss 4.09471\n",
      "valid_acc (0.24734282622591716, 0.9666)\n",
      "best_epoch (valid) 23\n",
      "last loss 4.08573\n",
      "valid_acc (0.24781571802873179, 0.9698)\n",
      "epoch 26 train_acc (0.2878961154328471, 0.9886) valid_acc (0.24781571802873179, 0.9698) test_acc (0.24575177389908701, 0.97176)\n",
      "best_epoch (valid) 26\n",
      "last loss 4.06486\n",
      "valid_acc (0.24765034576963543, 0.9666)\n",
      "best_epoch (valid) 26\n",
      "last loss 4.05571\n",
      "valid_acc (0.2471921280889344, 0.9352)\n",
      "best_epoch (valid) 26\n",
      "last loss 4.03852\n",
      "valid_acc (0.24725699377126634, 0.9662)\n",
      "best_epoch (valid) 26\n",
      "last loss 4.02153\n",
      "valid_acc (0.24659990016469691, 0.9572)\n",
      "best_epoch (valid) 26\n",
      "last loss 4.00427\n",
      "valid_acc (0.24669200913709613, 0.972)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.99852\n",
      "valid_acc (0.24686359689994256, 0.974)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.98104\n",
      "valid_acc (0.24611022904938601, 0.9624)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.97296\n",
      "valid_acc (0.24598891511029841, 0.973)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.96159\n",
      "valid_acc (0.24553486067252586, 0.9664)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.94745\n",
      "valid_acc (0.24565408339187103, 0.9544)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.93936\n",
      "valid_acc (0.24493636298008353, 0.931)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.92979\n",
      "valid_acc (0.24488414728945584, 0.9598)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.92135\n",
      "valid_acc (0.24291301526738612, 0.9576)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.90433\n",
      "valid_acc (0.24366211177686881, 0.9552)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.89196\n",
      "valid_acc (0.24365168833505754, 0.9622)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.88849\n",
      "valid_acc (0.24264729026236467, 0.921)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.88207\n",
      "valid_acc (0.24277698052659685, 0.9328)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.8775\n",
      "valid_acc (0.2422078449051478, 0.9506)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.86043\n",
      "valid_acc (0.2426911829515736, 0.9782)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.85767\n",
      "valid_acc (0.2410737062341835, 0.9466)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.84947\n",
      "valid_acc (0.24099629361416633, 0.9772)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.84659\n",
      "valid_acc (0.24061120274768144, 0.9844)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.83014\n",
      "valid_acc (0.2410339281680034, 0.971)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.83124\n",
      "valid_acc (0.24057563251529879, 0.9868)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.82411\n",
      "valid_acc (0.23963617243951449, 0.979)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.82064\n",
      "valid_acc (0.23948545922049339, 0.9876)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.81314\n",
      "valid_acc (0.23891214018676887, 0.9808)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.81557\n",
      "valid_acc (0.23868408136846309, 0.9856)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.80539\n",
      "valid_acc (0.23769016119214917, 0.9908)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.8102\n",
      "valid_acc (0.23669824670215239, 0.9874)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.80834\n",
      "valid_acc (0.23525654542476393, 0.9906)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.79738\n",
      "valid_acc (0.2364075253861293, 0.9898)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.78614\n",
      "valid_acc (0.23611238769979775, 0.993)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.77816\n",
      "valid_acc (0.23685316529407019, 0.9968)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.77256\n",
      "valid_acc (0.23653515072496589, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.76623\n",
      "valid_acc (0.23538844547147506, 0.9938)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.7619\n",
      "valid_acc (0.23566677324657556, 0.9928)\n",
      "best_epoch (valid) 26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last loss 3.759\n",
      "valid_acc (0.23543236783948512, 0.9932)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.75758\n",
      "valid_acc (0.2352147598895295, 0.993)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.75114\n",
      "valid_acc (0.23555378212278416, 0.9962)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.75303\n",
      "valid_acc (0.23406809441819115, 0.9958)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.73568\n",
      "valid_acc (0.23496158933573727, 0.9938)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.74606\n",
      "valid_acc (0.23446984230035847, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.73022\n",
      "valid_acc (0.23419989342481681, 0.9936)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.73735\n",
      "valid_acc (0.23436938679487981, 0.9972)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.73621\n",
      "valid_acc (0.23384843450124329, 0.9978)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.73165\n",
      "valid_acc (0.23352615292932183, 0.9968)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.72678\n",
      "valid_acc (0.23365376467005117, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.72375\n",
      "valid_acc (0.23292357006713985, 0.9974)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.71573\n",
      "valid_acc (0.23267871508160448, 0.9982)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.71477\n",
      "valid_acc (0.23135617420467333, 0.995)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.71075\n",
      "valid_acc (0.2322475874670055, 0.9976)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.71435\n",
      "valid_acc (0.23199015452431571, 0.9974)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.7123\n",
      "valid_acc (0.23156759998295326, 0.9944)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.71588\n",
      "valid_acc (0.23138541247756897, 0.9952)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.69533\n",
      "valid_acc (0.23142516095111773, 0.9954)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6997\n",
      "valid_acc (0.23134156641498474, 0.9928)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.69954\n",
      "valid_acc (0.23107787331892826, 0.9982)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.71483\n",
      "valid_acc (0.23160101852646914, 0.9946)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.70632\n",
      "valid_acc (0.23128293779676765, 0.995)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.68793\n",
      "valid_acc (0.23149011545746523, 0.9994)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.69589\n",
      "valid_acc (0.23054005240089923, 0.9988)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.69616\n",
      "valid_acc (0.23117198022281296, 0.997)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.68779\n",
      "valid_acc (0.23013832879421964, 0.9958)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.68049\n",
      "valid_acc (0.230205303968646, 0.9962)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6879\n",
      "valid_acc (0.23084350469936285, 0.998)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.67873\n",
      "valid_acc (0.2300211734804774, 0.9942)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.68045\n",
      "valid_acc (0.23068035143836599, 0.9938)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.67912\n",
      "valid_acc (0.23023461187147101, 0.9958)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.68055\n",
      "valid_acc (0.23030794735340593, 0.9978)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.67295\n",
      "valid_acc (0.23020946011484383, 0.9984)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6639\n",
      "valid_acc (0.23037055827519978, 0.996)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6766\n",
      "valid_acc (0.23014877467961678, 0.9934)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.67678\n",
      "valid_acc (0.2296884723337253, 0.9972)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66957\n",
      "valid_acc (0.22897702384291685, 0.995)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.68738\n",
      "valid_acc (0.22911508330744074, 0.9962)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.67245\n",
      "valid_acc (0.2295859423569761, 0.9952)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6813\n",
      "valid_acc (0.22875101925249747, 0.9966)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6615\n",
      "valid_acc (0.22886398284485954, 0.998)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66394\n",
      "valid_acc (0.22932445466862691, 0.9956)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65904\n",
      "valid_acc (0.22933062558407463, 0.9982)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.658\n",
      "valid_acc (0.22858149578369968, 0.9976)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66452\n",
      "valid_acc (0.22868199095318534, 0.9968)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66229\n",
      "valid_acc (0.22872584206139937, 0.9948)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66932\n",
      "valid_acc (0.22862770696468199, 0.997)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65292\n",
      "valid_acc (0.22897915030149527, 0.997)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66984\n",
      "valid_acc (0.22909620717977722, 0.9966)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65906\n",
      "valid_acc (0.22876347542386569, 0.9932)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64726\n",
      "valid_acc (0.22919466652131001, 0.9956)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64788\n",
      "valid_acc (0.22848950955452835, 0.9956)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65786\n",
      "valid_acc (0.22833051038457705, 0.9956)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65948\n",
      "valid_acc (0.22842040696281468, 0.9956)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65532\n",
      "valid_acc (0.2283576536893176, 0.991)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66178\n",
      "valid_acc (0.22823629279760854, 0.9946)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64694\n",
      "valid_acc (0.22750802087892902, 0.9928)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65288\n",
      "valid_acc (0.22829281180411129, 0.9932)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64942\n",
      "valid_acc (0.22784292603685494, 0.9912)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66439\n",
      "valid_acc (0.22807102381397257, 0.9904)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65004\n",
      "valid_acc (0.22795804056227006, 0.994)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64418\n",
      "valid_acc (0.22818816421290519, 0.9938)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65369\n",
      "valid_acc (0.22835562036936033, 0.9946)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65078\n",
      "valid_acc (0.22800817336104437, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64216\n",
      "valid_acc (0.22789095305909354, 0.99)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64397\n",
      "valid_acc (0.22755621104551937, 0.99)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63909\n",
      "valid_acc (0.22784918332327406, 0.9928)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66455\n",
      "valid_acc (0.22758549648489243, 0.9946)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65026\n",
      "valid_acc (0.22742231402925858, 0.9948)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65103\n",
      "valid_acc (0.2275143612920478, 0.9932)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6489\n",
      "valid_acc (0.22734496090978301, 0.9928)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66202\n",
      "valid_acc (0.22714816621229264, 0.987)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63612\n",
      "valid_acc (0.22767548962898324, 0.9906)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63993\n",
      "valid_acc (0.22760658033503561, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66145\n",
      "valid_acc (0.22747254579018478, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66239\n",
      "valid_acc (0.22718369743692418, 0.992)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66091\n",
      "valid_acc (0.22739097138485759, 0.991)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63766\n",
      "valid_acc (0.22688669278340243, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.66091\n",
      "valid_acc (0.22733659088491928, 0.9918)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64481\n",
      "valid_acc (0.22678839241814269, 0.9884)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64337\n",
      "valid_acc (0.22698923567641338, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64112\n",
      "valid_acc (0.22682389605454872, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6535\n",
      "valid_acc (0.22694099225742448, 0.9904)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64854\n",
      "valid_acc (0.22727371102203106, 0.9912)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.65283\n",
      "valid_acc (0.22720472921628262, 0.9922)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64476\n",
      "valid_acc (0.22680508516837747, 0.991)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63737\n",
      "valid_acc (0.22684261833124275, 0.9898)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64369\n",
      "valid_acc (0.22663772991054318, 0.99)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64811\n",
      "valid_acc (0.22679042170442176, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63853\n",
      "valid_acc (0.22672565193949631, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63968\n",
      "valid_acc (0.22715455973437626, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62866\n",
      "valid_acc (0.22621713876685001, 0.9918)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64766\n",
      "valid_acc (0.22622969423065042, 0.9904)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64955\n",
      "valid_acc (0.22673805391782836, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64496\n",
      "valid_acc (0.2260915379488527, 0.9936)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64409\n",
      "valid_acc (0.22661249441001488, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63304\n",
      "valid_acc (0.22642844856124253, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64167\n",
      "valid_acc (0.22649956317349421, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64716\n",
      "valid_acc (0.22587182482394749, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6193\n",
      "valid_acc (0.22603510989864706, 0.9922)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63313\n",
      "valid_acc (0.22625259908686771, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.638\n",
      "valid_acc (0.22602876251577556, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid_acc (0.225294205032989, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63998\n",
      "valid_acc (0.22536756029804733, 0.9922)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62506\n",
      "valid_acc (0.22607480319986056, 0.9906)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64455\n",
      "valid_acc (0.22682174233944535, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63626\n",
      "valid_acc (0.22646398341216517, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64628\n",
      "valid_acc (0.22558094909566737, 0.9904)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.64127\n",
      "valid_acc (0.22556634555883931, 0.992)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63099\n",
      "valid_acc (0.22634263373288119, 0.9904)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63535\n",
      "valid_acc (0.22604141057112273, 0.9922)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61753\n",
      "valid_acc (0.22615837037647399, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62889\n",
      "valid_acc (0.22584456751249998, 0.992)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62396\n",
      "valid_acc (0.22586981421079314, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61616\n",
      "valid_acc (0.22584466905126985, 0.9924)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63532\n",
      "valid_acc (0.22563122487654472, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61121\n",
      "valid_acc (0.22579234443774268, 0.994)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61998\n",
      "valid_acc (0.22555171791888423, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61791\n",
      "valid_acc (0.22553921830060994, 0.9918)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63193\n",
      "valid_acc (0.22567094284500308, 0.9922)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61837\n",
      "valid_acc (0.22490927952388393, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63114\n",
      "valid_acc (0.22543872658092348, 0.9904)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.63471\n",
      "valid_acc (0.22577352590265415, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62049\n",
      "valid_acc (0.22490515020615182, 0.9928)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6171\n",
      "valid_acc (0.22547638167787748, 0.9916)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62315\n",
      "valid_acc (0.22575464187994956, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.6367\n",
      "valid_acc (0.22490717158547538, 0.9938)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61799\n",
      "valid_acc (0.22559562707123107, 0.9926)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61858\n",
      "valid_acc (0.22465828534367377, 0.9928)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62378\n",
      "valid_acc (0.22517290101501769, 0.9914)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62717\n",
      "valid_acc (0.22468750843928384, 0.992)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62259\n",
      "valid_acc (0.22544281142643077, 0.9906)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.61003\n",
      "valid_acc (0.22539266059483168, 0.9918)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62961\n",
      "valid_acc (0.22468124361718597, 0.9932)\n",
      "best_epoch (valid) 26\n",
      "last loss 3.62015\n",
      "valid_acc (0.22501389017020085, 0.991)\n",
      "best_epoch (valid) 26\n",
      "last loss 5.86771\n",
      "valid_acc 0.8132\n",
      "epoch 0 train_acc 0.882 valid_acc 0.8132 test_acc 0.7546\n",
      "best_epoch (valid) 0\n",
      "last loss 3.68606\n",
      "valid_acc 0.808\n",
      "best_epoch (valid) 0\n",
      "last loss 0.640623\n",
      "valid_acc 0.7956\n",
      "best_epoch (valid) 0\n",
      "last loss 0.0141985\n",
      "valid_acc 0.7906\n",
      "best_epoch (valid) 0\n",
      "last loss 0.0411943\n",
      "valid_acc 0.7864\n",
      "best_epoch (valid) 0\n",
      "last loss 0.0358931\n",
      "valid_acc 0.7878\n",
      "best_epoch (valid) 0\n",
      "last loss 0.0464345\n",
      "valid_acc 0.7812\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000192994\n",
      "valid_acc 0.79\n",
      "best_epoch (valid) 0\n",
      "last loss 0.00488508\n",
      "valid_acc 0.784\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000138398\n",
      "valid_acc 0.7868\n",
      "best_epoch (valid) 0\n",
      "last loss 0.000122186\n",
      "valid_acc 0.7908\n",
      "best_epoch (valid) 0\n",
      "last loss 8.32065e-05\n",
      "valid_acc 0.7908\n",
      "best_epoch (valid) 0\n",
      "last loss 5.47164e-05\n",
      "valid_acc 0.7902\n",
      "best_epoch (valid) 0\n",
      "last loss 3.50472e-05\n",
      "valid_acc 0.7894\n",
      "best_epoch (valid) 0\n",
      "last loss 2.20536e-05\n",
      "valid_acc 0.7896\n",
      "best_epoch (valid) 0\n",
      "last loss 1.35898e-05\n",
      "valid_acc 0.7894\n",
      "best_epoch (valid) 0\n",
      "last loss 8.22542e-06\n",
      "valid_acc 0.79\n",
      "best_epoch (valid) 0\n",
      "last loss 5.12599e-06\n",
      "valid_acc 0.7902\n",
      "best_epoch (valid) 0\n",
      "last loss 2.86102e-06\n",
      "valid_acc 0.79\n",
      "best_epoch (valid) 0\n",
      "last loss 1.54972e-06\n",
      "valid_acc 0.7886\n",
      "best_epoch (valid) 0\n",
      "last loss 8.34465e-07\n",
      "valid_acc 0.789\n",
      "best_epoch (valid) 0\n"
     ]
    }
   ],
   "source": [
    "def generate_data(corpus_sents, corpus_labels, max_length, extend_vocabulary):\n",
    "    sen_en = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_de = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_en_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_de_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_label = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "\n",
    "    def get_random_sequence(sent, max_length):\n",
    "        x = np.full((max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "        for i, word in enumerate(sent):\n",
    "            if word in extend_vocabulary:\n",
    "                x[i] = extend_vocabulary[word]\n",
    "            else:\n",
    "                x[i] = extend_vocabulary[\"<unk>\"]\n",
    "        return x\n",
    "\n",
    "    for i in range(len(corpus_sents)):\n",
    "        l = min(len(corpus_sents[i]), max_length-2) + 2\n",
    "        sen_en[i, :] = get_random_sequence([\"<s>\"] + corpus_sents[i][:max_length-2] + [\"</s>\"], max_length)\n",
    "        sen_de[i, :l-1] = sen_en[i, 1:l]\n",
    "        sen_en_length[i] = l\n",
    "        sen_de_length[i] = l - 1\n",
    "    \n",
    "    for i in range(len(corpus_sents)):\n",
    "        sen_label[i] = corpus_labels[i]\n",
    "    \n",
    "    return sen_en, sen_de, sen_en_length, sen_de_length, sen_label\n",
    "\n",
    "def get_total_accuracy_sa(data_sen_en, data_sen_de, data_sen_en_length, data_sen_de_length, extend_vocabulary, pretrained_lstm):\n",
    "    n_hit_word, n_hit_length = 0, 0\n",
    "    n_total_word, n_total_length = 0, 0\n",
    "    cur_idx = 0\n",
    "    while cur_idx < data_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = data_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = data_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        _, predictions = pretrained_lstm.predict_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "        cur_acc_word, cur_acc_length = evaluate_sa(batch_sen_de, batch_sen_de_length, predictions, extend_vocabulary)\n",
    "        n_hit_word += cur_acc_word * np.sum(batch_sen_en_length)\n",
    "        n_total_word += np.sum(batch_sen_en_length)\n",
    "        n_hit_length += cur_acc_length * batch_sen_en.shape[0]\n",
    "        n_total_length += batch_sen_en.shape[0]\n",
    "    return 1. * n_hit_word / n_total_word, 1. * n_hit_length / n_total_length\n",
    "\n",
    "def get_total_accuracy_clf(data_sen_en, data_sen_en_length, data_label, pretrained_lstm):\n",
    "    cur_idx = 0\n",
    "    hit, tot = 0, 0\n",
    "    while cur_idx < valid_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]   \n",
    "        batch_label = data_label[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_pred = pretrained_lstm.predict_clf(batch_sen_en, batch_sen_en_length)\n",
    "        assert batch_pred.shape[0] == batch_sen_en.shape[0]\n",
    "        cur_acc = evaluate_clf(batch_pred, batch_label)\n",
    "        hit += cur_acc * batch_pred.shape[0]\n",
    "        tot += batch_pred.shape[0]\n",
    "        cur_idx += n_batch_size\n",
    "    return 1. * hit / tot\n",
    "    \n",
    "# hyperparameter\n",
    "vocabulary_size = 40000\n",
    "origin_vocabulary = {}\n",
    "for word, n in corpus_counter.most_common(vocabulary_size):\n",
    "    origin_vocabulary[\"{}\".format(word)] = len(origin_vocabulary)\n",
    "extend_vocabulary = dict(origin_vocabulary)\n",
    "for w in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]:\n",
    "    extend_vocabulary[w] = len(extend_vocabulary)\n",
    "reverse_extend_vocabulary = {v: k for k, v in extend_vocabulary.items()}\n",
    "imdb_label = {\"0\": 0, \"1\": 1}\n",
    "\n",
    "state_size=128\n",
    "n_max_length=100\n",
    "n_batch_size=20\n",
    "\n",
    "# generate training/testing data\n",
    "preprocess_sents = []\n",
    "preprocess_labels = []\n",
    "random.shuffle(train_preprocess)\n",
    "for sent, label in train_preprocess:\n",
    "    preprocess_sents.append(sent)\n",
    "    preprocess_labels.append(label)\n",
    "n_train = int(len(preprocess_sents) * 0.8)\n",
    "n_valid = len(preprocess_sents) - n_train\n",
    "\n",
    "print(\"n_train\", n_train, \"n_valid\", n_valid, \"n_test\", len(test_preprocess))\n",
    "\n",
    "train_sents = preprocess_sents[:n_train]\n",
    "train_labels = preprocess_labels[:n_train]\n",
    "valid_sents = preprocess_sents[n_train:]\n",
    "valid_labels = preprocess_labels[n_train:]\n",
    "test_sents, test_labels = [], []\n",
    "for sent, label in test_preprocess:\n",
    "    test_sents.append(sent)\n",
    "    test_labels.append(label)\n",
    "unlabeled_sents, unlabeled_labels = [], []\n",
    "for sent in unlabeled_preprocess:\n",
    "    unlabeled_sents.append(sent)\n",
    "    unlabeled_labels.append(0)  # just assign anything\n",
    "    \n",
    "train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, train_sen_en_label = generate_data(\n",
    "    train_sents, train_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, valid_sen_en_label = generate_data(\n",
    "    valid_sents, valid_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, test_sen_en_label = generate_data(\n",
    "    test_sents, test_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "unlabeled_sen_en, unlabeled_sen_de, unlabeled_sen_en_length, unlabeled_sen_de_length, unlabeled_sen_en_label = generate_data(\n",
    "    unlabeled_sents, unlabeled_labels, n_max_length, extend_vocabulary\n",
    ")\n",
    "\"\"\"\n",
    "print(train_sen_en[0])\n",
    "print(preprocess_sents[0])\n",
    "print([reverse_extend_vocabulary[i] for i in train_sen_en[0]])\n",
    "print(valid_sen_en[0])\n",
    "print(preprocess_sents[n_train])\n",
    "print([reverse_extend_vocabulary[i] for i in valid_sen_en[0]])\n",
    "\"\"\"\n",
    "pretrained_lstm = EncoderDecoder(vocabulary=extend_vocabulary, label=imdb_label, state_size=state_size, n_max_length=n_max_length)\n",
    "\n",
    "# train_sa\n",
    "best_epoch = None\n",
    "best_valid_acc = None\n",
    "early_stopping = 20\n",
    "\n",
    "curve_valid_acc = []\n",
    "for epoch in range(200):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = train_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = train_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent = pretrained_lstm.train_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    cur_idx = 0\n",
    "    while cur_idx < unlabeled_sen_en.shape[0]:\n",
    "        batch_sen_en = unlabeled_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = unlabeled_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = unlabeled_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = unlabeled_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent = pretrained_lstm.train_sa(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"last loss\", loss)\n",
    "    valid_acc = get_total_accuracy_sa(\n",
    "        valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "    )\n",
    "    print(\"valid_acc\", valid_acc)\n",
    "    curve_valid_acc.append(valid_acc)\n",
    "    if best_valid_acc is None or best_valid_acc < valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        train_acc = get_total_accuracy_sa(\n",
    "            train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "        )\n",
    "        test_acc = get_total_accuracy_sa(\n",
    "            test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "        )\n",
    "        print(\"epoch\", epoch, \"train_acc\", train_acc, \"valid_acc\", valid_acc, \"test_acc\", test_acc)\n",
    "    print(\"best_epoch (valid)\", best_epoch)\n",
    "    \"\"\"\n",
    "    if epoch - best_epoch == early_stopping:\n",
    "        break\n",
    "    \"\"\"\n",
    "\n",
    "# train_clf\n",
    "best_epoch = None\n",
    "best_valid_acc = None\n",
    "early_stopping = 20\n",
    "\n",
    "for epoch in range(200):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_label = train_sen_en_label[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, pred = pretrained_lstm.train_clf(\n",
    "            batch_sen_en, batch_sen_en_length, batch_sen_en_label\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"last loss\", loss)\n",
    "    valid_acc = get_total_accuracy_clf(\n",
    "        valid_sen_en, valid_sen_en_length, valid_sen_en_label, pretrained_lstm\n",
    "    )\n",
    "    print(\"valid_acc\", valid_acc)\n",
    "    if best_valid_acc is None or best_valid_acc < valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        best_epoch = epoch\n",
    "        train_acc = get_total_accuracy_clf(\n",
    "            train_sen_en, train_sen_en_length, train_sen_en_label, pretrained_lstm\n",
    "        )\n",
    "        test_acc = get_total_accuracy_clf(\n",
    "            test_sen_en, test_sen_en_length, test_sen_en_label, pretrained_lstm\n",
    "        )\n",
    "        print(\"epoch\", epoch, \"train_acc\", train_acc, \"valid_acc\", valid_acc, \"test_acc\", test_acc)\n",
    "    print(\"best_epoch (valid)\", best_epoch)\n",
    "    if epoch - best_epoch == early_stopping:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XmUXGWd//H3tzvdSWeFbBDTSZqQ\nJhAhbGERUBFQAyrB5TDww+2HiM4oo6PCQZlBD84cF3Rm1F8UcRmVVQGXiHHEBVFRNAmEJcGQFdKh\nzUYSknTSS/r7++Nb16ouqrsrSXVX6tbndc49VXX79r1P3ar6PM997mbujoiIpEtNuQsgIiKlp3AX\nEUkhhbuISAop3EVEUkjhLiKSQgp3EZEUUriLiKSQwl1EJIUU7iIiKTSkXAseP368NzU1lWvxIiIV\nacmSJVvcfUJ/05Ut3Juamli8eHG5Fi8iUpHM7NliplO3jIhICincRURSSOEuIpJCCncRkRTqN9zN\n7NtmtsnMnurl72ZmXzazVWb2hJmdUvpiiojI/iim5f4dYG4ff78QaM4MVwNfO/hiiYjIweg33N39\nd8ALfUwyD/ieh0eAw8xsUqkKKCIi+68Ux7lPBtbnvG7JjGstwbwlDbq6YNMm2LULpkyBhoYY39kJ\na9fCtm1wyilQVwcbN8b48eNh2LDC81u5Eo48EkaNitfPPAP33QejR8OsWXDmmdllALS0wPDhMHZs\nz/ns3Al//Sts3x7LHzcu+zd3MHvpsvftg9Wro3xjx8bzF16ASZNg4kSor+9/HiKDYFBPYjKzq4mu\nG6ZOnTqYi5b95Q4PPQSf/GSE83e/C3v3wvr10NQUIdfeHkF8223whz/AyJFw0klw/vlw6qnQ1gY/\n+xl84QsR2onXvAbe+174t3+LcARobIzATk5sGzUKLrsMRoyALVtiOWeeCVu3wn/8Bxx2GFxxRSzj\n9tuhoyM7/yFDoLk55tHaGmVuaIB/+ieYPRueew4eeAD+9Kd4b8n/vO51cNFF8IMfRDnOPRe6u6Ps\nU6dGBfX447HM2lo4+uioWHLNmAGnnQaPPBIV19ChMTQ3w/veB2vWxDxymcFb3gKXXw7f/36U5dRT\nY/1OmRIVCcCLL8LChfHZ7NoVleIb3gAvf3nP+W3YAA8+CCtWxHppboYTT4zKaPr0bIXjHhXb0KGx\nfvqriLq7oWY/j8Foa4vvyogRL/3fbdvgySejbKNHR1m6uuJzy6/Yk+/b8OEvXcaePfFeCr0H9/i/\ntjYYMyY+N/dYL+3tsR6XLYPdu6McU6fGNL3ZujWWk5Rj0yZYtCh+E2PHxrwnTSpcjg0bYh0OUvZZ\nMTfINrMm4H53P77A374O/Nbd78q8XgGc6+59ttznzJnjOkO1DPJbk7/9Ldx1F7z5zfFl//GPIzj/\n+Ef4y1/gZS+LH8bOnfEDK6S+Hs47L35kixbF9LkuuCDCa+TIaHXPnx+t3aOOgn/91/ix/M//wI4d\nUY6xY+Hhh7NBN3FilHvdupjfFVdE0P3v/0ZoXHxxBH5tLTz6aJR9+fIoz9ixcPrpEdZ33hnzgWip\nv/71cMYZUa4HHoC7747gP/LICM0//CF+xEccEePHjYOTT44QWLkSliyJeTQ3RwXw/POxnEWLIuBP\nPDEqpfZ2+NWvIkRqa6OCGZLTrtq+PeY3bFhUoLnMYvqmJvjd7yIQ8518cryPUaOiTL/5TfZ/hwyJ\nMiTOOw9uvjkq65/8BJ59NjvtqFEwbVq85xdfjGHGDHjb2+DWW2Mrp6kpKrXhw+N/162LMh93XGw1\nTZkSwblqFTzxRLbyTuZ/1FFRwaxZE8He3R3rpKEhghaiEmhsjOW3t8f6b22N542N8bl2dERDoqUl\nKlGIZc+eHcsdMSLKuWhRBHBShrq6ng2BfLW18X3YtSs+76ammO/mzfGd2rIl5tPYGJ/X6tXxHnKN\nHg2HH55df8OGxXvdvj3+/vKXx/d13rzey9EHM1vi7nP6na4E4f4G4IPARcAZwJfd/fT+5qlwH2B/\n/GN8+evrowX82tdGCJ57brQKv/Y1+NSn4L//O77QSXCPHZv9EV1zDVx5Zfw4vvjF+AGfcEIEXX19\nDF1dcPbZ8QOE+N/HHovWadKSz29Zbt0alcill2a7VgrZty9+6Ell9PTTEW5nnXVg62T79lj2mDHZ\n1nAu91hGU1PhFuLB6O6O4J0+vWf3T/K373wnKtorr4zKddmyCLzly6OSWb8eZs6Ej340/r+hIVrc\nt98OP/95zLu9HSZPhne+Mz7rE06I9bdyZbyvFSvgxhtjurq6qMDOOiuWv3t3rJ+1ayP8Ro+Oz+83\nv4nPe8YMuOSSCPTVq6MCb2qKyqC+PoJ/+fJondbXR4ifcEIMSWWxfXts6axdG8F72mlRyS5aFI2H\npqb4382bo3I4/PB4vXFjNDJGjYr3MnJkrLfHHotW8qmnxvt89NEow+zZ8X5WrYI5c+J7O2xYfHc6\nOrJbU8kWy8yZMc8nnoj39+KL8XrLlqi8nn02vi+zZsGxx0bwr1kTldrMmVFhbtgQ4/ftg6eeiucj\nR8Z62bs3yjR7dqz7+++H666LhsEBKFm4m9ldwLnAeGAj8EmgDsDdbzEzA/4fcURNG/B/3b3f1Fa4\nD6B///fo8njXu2Du3Njch/jxJC2ghoZoAX3gA/DpT0creOjQCIUhZbvkkAy0Rx+NFvtVV0WLtD+d\nnbB0aWyF5O5P6I32Mwy4krbcB0JVh/vu3TFMnFia+V13XXRjPPwwfOtb8cOdPDm6CaZNi9CeOjW6\nBn7842jBfOUr8JnPRIteRCpGseGuM1RLbd262PwtpLMzuiLGjo3N0nXr4JvfjE27JUsObHnusQPw\nj3+Mzdk774xukKVLo/th3boI//vvj03iiy+OFv3ixQp2kRRTuJfKhg3RJzl9evTLnX9+hK07fPnL\nsePnttvgnnsiXN2ju+Saa6If8dxz4fe/3//lrlmT3Sn20ENxBMj550cf4ec/HzvarrgiNqlnzCjp\nWxaRQ5c6V/O5R3dFXV2EdXNz//+zdGnsnNqxA264IfbUf/zj8L3vxWF/H/pQdJfs2hU7eL7+9dg5\n8y//EjvHHn44dnq+852xI2348CjH2rXRT15TE6/nz4/pkz50iK4WiH7Or3wl+tFf+coY9973xiAi\n1cfdyzKceuqpfsjo6HA/6ST3r37V/a673CNK3YcOdV+5su//7ex0P/po98mT3R9/PDt++nT3N7/Z\n/UtfinmZxeOCBdn/++d/dv/Vr+L1b38bf7/hhnh9333xetIk9/e/3/0974nXM2f2XP7b3ube2Oj+\nildky/23v5VmvYjIIQdY7EVkrMLd3b2lJVZFTY372LHup57q/vTT7sOHu19+ed//e/vt8b8/+lHP\n8e94h/vEiRG+06a5f+Mb7lde6d7d3fu83vEO97o699ZW96uuch892v2tb41ygPvUqVHGtraYft++\nKO+73+3+sY/FNM3NB7UqROTQVmy4q88d4nhWiKNKtm2LY8CPPTa6U+66K7pdCtm3L7pwXv7y2FGZ\n65xz4vjw+++P51ddFV0zfR0mdu21sdP1pz+NMwzPPRfuvTfKt2JFHGve3R3HLEN057zwQpwkdPbZ\nMS7pkhGRqqZwh2y433FHHHVy2mnx+rrrov/8G9946f984hNxksWyZfE8/9TqJGz37o1wL8bxx8cO\n2fnz40SR17wmxjc0wDHHxEkQECdbANxySxwRc8kl8KpXRX/8JZcU/75FJLUU7pAN95kz4/olicMO\ni1Z5/vVD3OGrX42dovfc03MHZ+K44yL8ofjWtFmEc3LtkSTcE0cfHUH/5JNxJM6998aRNyNGxOGV\nW7bAm95U3LJEJNUU7pAN9/zTwiEOH1y1que41avjyJirrorrbhTqaqmpiRb72LER9MVKrjcxblyc\nup2rtjYqlCefhG9/O06lfv/7i5+3iFQNHQoJ2XDPvyQsRLjffXecsv+FL0RffHIlwTn9nCT2X/8V\n/e77cyW95Dotr3514f+bPTvOMk1OQtqfikNEqobCHSLcDzssjm3PN2NG7MRcsQJuuikuiPSmN8XO\n1/wLYuU7+ugY9kdtbVwoasyYwn8/4YS4gmJdXVz0S0SkAIU7RLgXukogZM/q/OEPoxtkxYo4GenE\nEwtXBqXQ15mkJ50Uj9deG100IiIFVG+fe1cX/OM/xqn/fYV7cobqnXdmx23YEJcZLYdXvzruOvTJ\nT5Zn+SJSEaq35b5yZRxKOGVKhHtjY+Hpxo2LLpKVK+MqjrNnxyn//fW3D5SamrjxhYhIH6q35d6a\nuVHUM8/03XI3y3aTnH46vP3t8fwVrxj4MoqIHCCFe3/hDtlwP+OM7MW9dJSKiBzCFO5PPhlnkRYT\n7qefHi157cgUkUOcwj25KW9f4X7eeRHwZ5wx8OUSESmB6t2hmoR7or9wX7lyYMsjIlJC1dtyf/75\nnoHeV7iLiFSY6g331tY41T85xV/hLiIpUt3hPm1aXE4AFO4ikirVGe67d8POnTBpUpyBWlMT15YR\nEUmJ6tyhmuxMnTQJzjoLnnsuLtglIpIS1dlyzw33G27I3hxDRCQl1HKvqdm/662LiFSA6ky13HAX\nEUmh6g33urrCt9UTEUmB6gz3jRvjVnaF7n0qIpIC1Rnuu3fDiBHlLoWIyICpznDv6Ih7oIqIpFR1\nhnt7u8JdRFJN4S4ikkJFhbuZzTWzFWa2ysyuL/D3qWb2oJk9ZmZPmNlFpS9qCSncRSTl+g13M6sF\n5gMXArOAy80s/1ZE/wr8wN1PBi4DvlrqgpaUwl1EUq6YlvvpwCp3X+PuHcDdwLy8aRwYnXk+Bni+\ndEUcAAp3EUm5Yi4/MBlYn/O6Bci/39yngAfM7BpgBHBBSUo3UBTuIpJypdqhejnwHXdvBC4CbjOz\nl8zbzK42s8Vmtnjz5s0lWvQBULiLSMoVE+4bgCk5rxsz43K9B/gBgLv/CRgGvOTuF+5+q7vPcfc5\nEyZMOLASl4LCXURSrphwXwQ0m9lRZlZP7DBdkDfNc8D5AGZ2HBHuZWya90PhLiIp12+4u3sX8EHg\nF8DTxFExy8zsJjO7ODPZR4H3mtnjwF3Au93dB6rQB03hLiIpV9T13N19IbAwb9yNOc+XA2eXtmgD\nSOEuIilXfWeodnVBd7fCXURSrfrCvb09HhXuIpJiCncRkRRSuIuIpJDCXUQkhRTuIiIppHAXEUkh\nhbuISAop3EVEUkjhLiKSQgp3EZEUUriLiKSQwl1EJIUU7iIiKaRwFxFJIYW7iEgKKdxFRFJI4S4i\nkkLVG+51deUth4jIAKrOcB86FMzKXRIRkQFTveEuIpJiCncRkRRSuIuIpJDCXUQkhRTuIiIppHAX\nEUkhhbuISApVX7h3dCjcRST1qi/c1XIXkSqgcBcRSSGFu4hICincRURSSOEuIpJCRYW7mc01sxVm\ntsrMru9lmkvNbLmZLTOzO0tbzBJSuItIFRjS3wRmVgvMB14LtACLzGyBuy/PmaYZ+DhwtrtvM7OJ\nA1Xgg6ZwF5EqUEzL/XRglbuvcfcO4G5gXt407wXmu/s2AHffVNpilpDCXUSqQDHhPhlYn/O6JTMu\n1zHAMWb2sJk9YmZzS1XAknJXuItIVei3W2Y/5tMMnAs0Ar8zsxPcfXvuRGZ2NXA1wNSpU0u06P3Q\n2RmPCncRSbliWu4bgCk5rxsz43K1AAvcvdPd1wLPEGHfg7vf6u5z3H3OhAkTDrTMB043xxaRKlFM\nuC8Cms3sKDOrBy4DFuRN82Oi1Y6ZjSe6adaUsJyloXAXkSrRb7i7exfwQeAXwNPAD9x9mZndZGYX\nZyb7BbDVzJYDDwLXuvvWgSr0AVO4i0iVKKrP3d0XAgvzxt2Y89yBj2SGQ5fCXUSqRHWdoapwF5Eq\nUV3hvndvPA4bVt5yiIgMMIW7iEgKKdxFRFJI4S4ikkIKdxGRFFK4i4ikUHWGe0NDecshIjLAqjPc\n1XIXkZRTuIuIpJDCXUQkhaov3M2grq7cJRERGVDVF+7DhkXAi4ikWHWGu4hIylVXuO/Zo3AXkaqQ\nnnBfvRre8AbYvbv3adRyF5EqkZ5w/9OfYOFCWLu292kU7iJSJdIT7h0dPR8LUbiLSJVIT7gnd1lS\nuIuIpDzcr70WPv7x7GuFu4hUiaJukF0RCnXL3HEHTJmSfb13L4wfP7jlEhEpg/S23DduhNZW2Lkz\nO41a7iJSJdIb7o8/Ho8KdxGpQukJ9/xumaVL43HXruw0e/fqWu4iUhXSE+75Lfck3HfuBPd4rpa7\niFSJ9IR7by33ffuyl/pVuItIlUhPuOe23NvaYMWK7JExSb+7wl1EqkQ6w/2pp6C7G845J8bt2hUt\n+M5OhbuIVIX0hHvSHdPeHodBAhx3XDzu3Km7MIlIVUlPuOe23JMgz+2WUbiLSBVJT7jn7lBNgl7h\nLiJVKj3hnttyT55PmBCPu3Yp3EWkqqQz3NUtIyJVrqhwN7O5ZrbCzFaZ2fV9TPdWM3Mzm1O6IhZJ\n3TIiIn/Xb7ibWS0wH7gQmAVcbmazCkw3CvgQ8OdSF7IohVru48bFo7plRKTKFNNyPx1Y5e5r3L0D\nuBuYV2C6TwOfA/aWsHzFK9RyHzUK6uvVcheRqlNMuE8G1ue8bsmM+zszOwWY4u4/K2HZ9k/+DtX6\nejCLgFe4i0iVOegdqmZWA/wn8NEipr3azBab2eLNmzcf7KJ7yu+WGTo0XivcRaQKFRPuG4Cc2xnR\nmBmXGAUcD/zWzNYBZwILCu1Udfdb3X2Ou8+ZkBymWCr53TJJiI8cqT53Eak6xYT7IqDZzI4ys3rg\nMmBB8kd33+Hu4929yd2bgEeAi9198YCUuDfFttx1PXcRqQL9hru7dwEfBH4BPA38wN2XmdlNZnbx\nQBewaPl97kkLPQn3PXvitVruIlIFirpBtrsvBBbmjbuxl2nPPfhi7afubujqiudJuOe23Fta1C0j\nIlUlHWeoJv3tyfPcbhn1uYtIFUpnuBfqltm7F2prYUhRGysiIhUtHeGe9LdD7ztU9+xRq11EqkY6\nwz2/5d7VBS++qHAXkaqRjnBPumVqal66Q3XkyHjcvFnhLiJVIx3hnrTcR44s3C0DCncRqSrpCPek\n5T5qVOFuGYDWVoW7iFSNdIR7MS33tWvhtNPKUz4RkUGWjuMCcy/x29ERhzwmrfSJE+PxH/4Bbrml\nPOUTERlk6Qj3/G4Zs2zL/cQT4fHH4fjjY4eriEgVSEe453bLJK+TcDeD2bPLUy4RkTJJR1M2t+We\n0M5TEali6Qj3/JY7ZFvuIiJVKF3hrpa7iAiQlnAv1C2jlruIVLF0hLu6ZUREekhXuKtbRkQESEu4\nJ90yarmLiABpCXe13EVEekhHuHd0xB2WcgNdLXcRqWLpCPf2dqiv7xnoCncRqWLpCfehQyPgE+qW\nEZEqlo5w7+iIYM8Nd7XcRaSKpSPc1XIXEekhHeGulruISA/pCPdCLXeFu4hUsfRcz/1gu2W2bYNn\nnoGtW+PuTVu2wMqVMezcCW1t8PzzsHs37NsXw/btcePt5DDMIUNi2mRLYujQ7DBuHIweDV1dMQwZ\nApMmxYlXdXXZoaEBjjwyxg8fDuecAxMmxHK6u+Gww2DMmNKuPxFJnXSE+/52yzzzDCxZAmvWwB/+\nAIsXR5gXMnIkjB0b83vZy6CxMe7oVFsbYT1xYgR9e3uUY/ToKEdyo+6ODtizJ+a/c2fMJ7nX61NP\nRaXR2RmvOzvjdVdX3+93zJgoQ309HH54VDgNDXGP2HHjouKoq+v5eMQRMGVKLHvEiBhGjoTx42Ma\nEUmVdIR7fsu9piYCLdHVBZs2wX33wfz5sGJF9m+zZsEll8Cxx8Ixx0TYbdoUgX7MMRHeZoP3Xrq7\n4YUXIuS3bYMHH4RduyKca2vjb88+m33f27ZFUO/YAQ89FNN2dma3EPbt63t5dXUwY0a8/2OPjQrg\n2WfhxRdjnW7aFJXTqFEwfXpUcADusZ6HDo1yDBkStzTs7IytnxEjoqJraIhy1NdHpTRmTCyzu7vn\nZyQiJZWOX1d7e7bFDBE4q1dHkP/617B8eTbkzjorxr/qVTBtWs9LFhwKamqiggGYOjUC82C4R+A+\n/zxs2BCt/GTYtQueew6efjqGn/40griuLtZnR0eUZfjwqDxuuy3mVyrjxkVlMXlyDGZRRujZpZU7\nJN1dyXtItkBGjYqhrS22kKZNiy6szs7o0qqtje6u7dtjmmReyVBX1/Nx+PCoUM1iOW1tUTEdcUR2\nS6u9PYYRI2JZSSPAPSqvpPsuqWSTIanoBrPRIFUnHeHe0RE/1qR7obs7Wt21tXDBBfDGN0aL9JRT\n4IwzylvWwWYWYdLUFENfOjth48YIwUKt6qR7ySyG7u7sVlNbW9yIfNiw2Eewe3d2X8WQIfEZ7dgR\nw759EYAbN0aYb9gAS5fG/BobY95Jl1YSoMnQ0RFhOWxYhOru3bB370Csuf2TfPf27Yv30Z+hQ6P8\nvVVgAK2t8ThyZLaSSLbKOjtj3NChURE1NMRQUxOfk1ms9yFD4neQVFpJd+W2bTGfmpq+B7P+p+nv\n//fsic9u6tTs/qLkO5Q8LzQu//mwYfHdrK+P71VrazRaduyIin3MmGiUjBkTDZdkf1hupV1bm51f\n/pC7rJqaaHyMGRNbsTt2xPpKvp87dsT4ESPi+751a8x/8uRYtnt8bps3x+9gyJBoKI0cGd/ZqVOj\noTCA0hHuScDce2/29Qc+AJ/4RLYbQfpXVxdf3t40NEQl2ZuZM0tfpkK6u+PHl+jqih/Uzp0RdMOH\nw7p1MS75UXV1RWVy+OHx946O7JC7zyN53LUL/va3+CEn89y+PbqpkpBMHnftivEQ4ZEMSbDmD+3t\nMe/du3tWWrmVmHs0RGpr430lAZ3sR6mri3XQ3h7h2dYWj8lO92S9JMOePRFIybwPPzzCsrs7hq6u\n7PNkSLZADnZoaIjyfv/7/e9P2l/Jvq+kO7JS3HILvO99A7oI81JuZu+HOXPm+OLFi0szs+bmeFy1\nKn6MRx4ZNbqIHDr27YtKzD3bvZf/vNC45HlbW1SKyZbbpElRcdfWxjR792Zb2cOHx/6y7u6eFXmy\n1Zg/5C4rqdQ2b455JfuKamuhpSWmTcYlWwjjxsXyW1uzXb07d0arfvToWO7mzdmuxNmzo/V+AMxs\nibvP6W+6olruZjYX+BJQC3zT3T+b9/ePAFcBXcBm4Ep3f3a/S30gdu2KVlpXF1x/PXzta7EyReTQ\nUlsbrfgDNXZs71uWZtmuqfzujgM9W73QluisWQc2rzLo9yQmM6sF5gMXArOAy80s/x0+Bsxx99nA\nvcDnS13QgrZuhVe+MoL90kvhM5956dUhRUSqUDFnqJ4OrHL3Ne7eAdwNzMudwN0fdPe2zMtHgD46\nbkvommviWHGAq66KR4W7iEhR4T4ZWJ/zuiUzrjfvAX5+MIUqyi9/CXfdBa9/fbxONqHq63XRMBGp\neiU9WsbM3g7MAV7dy9+vBq4GmHqAOxOA2OHxoQ/FyTfTp0c/W9IXp5a7iEhRLfcNQO7xb42ZcT2Y\n2QXADcDF7t5eaEbufqu7z3H3ORMmTDiQ8obkpJuPfSxOVmpuzh4aN2zYwe20ERFJgWJa7ouAZjM7\nigj1y4D/kzuBmZ0MfB2Y6+6bSl7KfD/P9PpcdBHcfHOcnJT47Gd1YS0RqXr9hru7d5nZB4FfEIdC\nftvdl5nZTcBid18A3AyMBO6xONPrOXe/eMBKvXAhHH98HMe6di1cfnn2b3PnDthiRUQqRVF97u6+\nEFiYN+7GnOcXlLhcvdu5E37/e/jwh6NLprt78M6MFBGpEJV3s45f/zpOM77oouzVHRXuIiI9VF64\nb9wY1zc5+2yFu4hILyov3N/3vrjcQF1dhPuRR+pyAyIieSov3CF72OOKFWq1i4gUUJnhnlC4i4gU\nVLnhvmVL3HJO4S4i8hKVG+7amSoi0iuFu4hIClV2uNfV9X9fUBGRKlTZ4T5jRuEbOYuIVLnKDnd1\nyYiIFFSZ4e4eFwybMaPcJREROSRVZrhv2QLt7XEZAhEReYnKDPf1mbv+KdxFRAqqzHBvaYnHxsG5\nD7eISKWpzHBXy11EpE+VGe4tLXGM+8SJ5S6JiMghqTLDff16mDw5e3VIERHpoTLTsaVF/e0iIn2o\nzHBfv1797SIifai8cHdXy11EpB+VF+6bN0NHh8JdRKQPlRfuyTHu6pYREelV5YV7coy7Wu4iIr2q\nvHBXy11EpF+VF+6NjTBvnk5gEhHpQ+Xd6WLevBhERKRXlddyFxGRfincRURSSOEuIpJCCncRkRRS\nuIuIpJDCXUQkhRTuIiIppHAXEUkhc/fyLNhsM/DsAf77eGBLCYtTSodq2VSu/aNy7b9DtWxpK9c0\nd5/Q30RlC/eDYWaL3X1OuctRyKFaNpVr/6hc++9QLVu1lkvdMiIiKaRwFxFJoUoN91vLXYA+HKpl\nU7n2j8q1/w7VslVluSqyz11ERPpWqS13ERHpQ8WFu5nNNbMVZrbKzK4vYzmmmNmDZrbczJaZ2Ycy\n4z9lZhvMbGlmuKgMZVtnZk9mlr84M26smf3SzFZmHg8f5DLNzFknS83sRTP7cLnWl5l928w2mdlT\nOeMKriMLX858554ws1MGuVw3m9lfM8v+kZkdlhnfZGZ7ctbdLYNcrl4/OzP7eGZ9rTCz1w9Uufoo\n2/dzyrXOzJZmxg/KOusjHwbvO+buFTMAtcBqYDpQDzwOzCpTWSYBp2SejwKeAWYBnwI+Vub1tA4Y\nnzfu88D1mefXA58r8+f4N2BaudYX8CrgFOCp/tYRcBHwc8CAM4E/D3K5XgcMyTz/XE65mnKnK8P6\nKvjZZX4HjwNDgaMyv9nawSxb3t+/CNw4mOusj3wYtO9YpbXcTwdWufsad+8A7gbKclsmd29190cz\nz3cCTwOTy1GWIs0Dvpt5/l3gkjKW5Xxgtbsf6ElsB83dfwe8kDe6t3U0D/ieh0eAw8xs0mCVy90f\ncPeuzMtHgEG/O3wv66s384C73b3d3dcCq4jf7qCXzcwMuBS4a6CW30uZesuHQfuOVVq4TwbW57xu\n4RAIVDNrAk4G/pwZ9cHMptXvKf32AAACn0lEQVS3B7v7I8OBB8xsiZldnRl3hLu3Zp7/DTiiDOVK\nXEbPH1u511eit3V0KH3vriRaeImjzOwxM3vIzF5ZhvIU+uwOpfX1SmCju6/MGTeo6ywvHwbtO1Zp\n4X7IMbORwH3Ah939ReBrwNHASUArsUk42M5x91OAC4EPmNmrcv/osR1YlsOkzKweuBi4JzPqUFhf\nL1HOddQbM7sB6ALuyIxqBaa6+8nAR4A7zWz0IBbpkPzs8lxOz4bEoK6zAvnwdwP9Hau0cN8ATMl5\n3ZgZVxZmVkd8cHe4+w8B3H2ju+9z927gGwzg5mhv3H1D5nET8KNMGTYmm3mZx02DXa6MC4FH3X1j\npoxlX185eltHZf/emdm7gTcCV2RCgUy3x9bM8yVE3/Yxg1WmPj67sq8vADMbArwF+H4ybjDXWaF8\nYBC/Y5UW7ouAZjM7KtMCvAxYUI6CZPryvgU87e7/mTM+t5/szcBT+f87wOUaYWajkufEzriniPX0\nrsxk7wJ+MpjlytGjJVXu9ZWnt3W0AHhn5oiGM4EdOZvWA87M5gLXARe7e1vO+AlmVpt5Ph1oBtYM\nYrl6++wWAJeZ2VAzOypTrr8MVrlyXAD81d1bkhGDtc56ywcG8zs20HuNSz0Qe5WfIWrcG8pYjnOI\nTaongKWZ4SLgNuDJzPgFwKRBLtd04kiFx4FlyToCxgG/BlYCvwLGlmGdjQC2AmNyxpVlfREVTCvQ\nSfRvvqe3dUQcwTA/8517EpgzyOVaRfTHJt+zWzLTvjXzGS8FHgXeNMjl6vWzA27IrK8VwIWD/Vlm\nxn8HeH/etIOyzvrIh0H7jukMVRGRFKq0bhkRESmCwl1EJIUU7iIiKaRwFxFJIYW7iEgKKdxFRFJI\n4S4ikkIKdxGRFPr/GuaOA8Bd5asAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff398170a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(curve_valid_acc)), curve_valid_acc, 'r-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
