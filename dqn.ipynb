{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class TurnOffLightGame:\n",
    "    def __init__(self, sen_init, vocabulary, target_word, state_size):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        sen_init: list of string\n",
    "        vocabulary: dictionary\n",
    "            key: string\n",
    "            value: int\n",
    "        target_word: string\n",
    "        state_size: int\n",
    "        \"\"\"\n",
    "        self.sen = []\n",
    "        self.vocabulary = vocabulary\n",
    "        self.actions = {v: k for k, v in self.vocabulary.items()}\n",
    "        self.actions[len(self.actions)] = \"<L>\"\n",
    "        self.actions[len(self.actions)] = \"<R>\"\n",
    "        self.state_size = state_size\n",
    "        \n",
    "        # set target that need to turn off\n",
    "        assert target_word in vocabulary\n",
    "        self.target_word = target_word\n",
    "        \n",
    "        # init self.sen\n",
    "        self.sen.append(vocabulary[\"<s>\"])\n",
    "        for i, w in enumerate(sen_init):\n",
    "            if w not in self.vocabulary:\n",
    "                self.sen.append(vocabulary[\"<unk>\"])\n",
    "            else:\n",
    "                self.sen.append(vocabulary[w])\n",
    "        self.sen.append(vocabulary[\"</s>\"])\n",
    "        self.cursor = random.randint(1, len(self.sen) - 2)\n",
    "    \n",
    "    def is_done(self):\n",
    "        cur_state = self.get_state()\n",
    "        for i in range(self.state_size):\n",
    "            if cur_state[i] == 1:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def get_action_list(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def action_lookup(self, idx):\n",
    "        return self.actions[idx]\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        cur_state: numpy, shape=(state_size,), dtype=float\n",
    "            cur_state[0] == 1 if there are self.target in self.sen[:self.cursor]\n",
    "            cur_state[1] == 1 if there are self.target in self.sen[self.cursor+1:]\n",
    "            cur_state[2] == 1 if self.sen[self.curosr] == self.target_word\n",
    "        \"\"\"\n",
    "        cur_state = np.zeros((self.state_size,), dtype=np.float32)\n",
    "        flag = False\n",
    "        for i in range(0, self.cursor):\n",
    "            if self.sen[i] == self.vocabulary[self.target_word]:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            cur_state[0] = 1\n",
    "        flag = False\n",
    "        for i in range(self.cursor+1, len(self.sen)):\n",
    "            if self.sen[i] == self.vocabulary[self.target_word]:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            cur_state[1] = 1\n",
    "        if self.sen[self.cursor] == self.vocabulary[self.target_word]:\n",
    "            cur_state[2] = 1\n",
    "        return cur_state\n",
    "    \n",
    "    def apply_action(self, a):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        a: int\n",
    "        Return\n",
    "        ------\n",
    "        reward: float\n",
    "        \"\"\"\n",
    "        reward = 0\n",
    "        action = self.actions[a]\n",
    "        if action == \"<L>\":\n",
    "            self.cursor = max(self.cursor - 1, 1)\n",
    "        elif action == \"<R>\":\n",
    "            self.cursor = min(self.cursor + 1, len(self.sen) - 2)\n",
    "        else:\n",
    "            if (self.sen[self.cursor] == self.vocabulary[self.target_word] and\n",
    "                action != self.target_word):\n",
    "                reward = 1\n",
    "            self.sen[self.cursor] = action\n",
    "        return reward\n",
    "        \n",
    "class DQN:\n",
    "    def __init__(self, dim_in, dim_out, dim_hidden=32, gamma=0.5, l2_alpha=1):\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.gamma = gamma\n",
    "        self.l2_alpha=l2_alpha\n",
    "        \n",
    "        ######################\n",
    "        # Graph Construction #\n",
    "        ######################\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            self.x_state = tf.placeholder(tf.float32, shape=(None, self.dim_in,), name=\"x_state\")\n",
    "            self.y_mask = tf.placeholder(tf.int32, shape=(None, self.dim_out,), name=\"y_mask\")\n",
    "            self.y_target = tf.placeholder(tf.float32, shape=(None, self.dim_out,), name=\"y_target\")\n",
    "        \n",
    "            # hidden layer and output layer\n",
    "            regularizer = tf.contrib.layers.l2_regularizer(self.l2_alpha)\n",
    "            hidden_layer = tf.layers.dense(\n",
    "                self.x_state, dim_hidden,\n",
    "                kernel_regularizer=regularizer, bias_regularizer=regularizer,name=\"hidden_layer\"\n",
    "            )\n",
    "            self.prediction = tf.layers.dense(\n",
    "                hidden_layer, dim_out,\n",
    "                kernel_regularizer=regularizer, bias_regularizer=regularizer, name=\"output_layer\"\n",
    "            )\n",
    "\n",
    "            loss_l2 = tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "            self.loss = tf.reduce_sum(\n",
    "                tf.multiply(tf.pow(self.y_target - self.prediction, 2), tf.cast(self.y_mask, tf.float32))\n",
    "            ) + loss_l2\n",
    "            \n",
    "            # Calculate and clip gradients\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "            # Optimization\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train = optimizer.apply_gradients(zip(self.clipped_gradients, params))\n",
    "\n",
    "            # initializer\n",
    "            self.init = tf.global_variables_initializer()\n",
    "        \n",
    "        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
    "        self.sess = tf.Session(\n",
    "            graph=self.graph,\n",
    "            config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "        )           \n",
    "        self.sess.run(self.init)\n",
    "    \n",
    "    def train(self, x_state, y_mask, y_target):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_state: numpy, shape=(n_batch, dim_in), dtype=float\n",
    "        y_mask: numpy, shape=(n_batch, dim_out), dtype=int\n",
    "        y_target: numpy, shape=(n_batch, dim_out), dtype=float\n",
    "        Returns\n",
    "        -------\n",
    "        loss: numpy, shape=(n_batch, dim_out), dtype=float\n",
    "        prediction: numpy, shape=(n_batch, dim_out), dtype=float\n",
    "        \"\"\"      \n",
    "        _, loss, prediction = self.sess.run(\n",
    "            [self.op_train, self.loss, self.prediction],\n",
    "            feed_dict={\n",
    "                self.x_state: x_state,\n",
    "                self.y_mask: y_mask,\n",
    "                self.y_target: y_target\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return loss, prediction\n",
    "        \n",
    "    def predict(self, x_state):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_state: numpy, shape=(n_batch, dim_in), dtype=float\n",
    "        Returns\n",
    "        -------\n",
    "        prediction: numpy, shape=(n_batch, dim_out), dtype=float\n",
    "        \"\"\"\n",
    "        prediction = self.sess.run(\n",
    "            [self.prediction],\n",
    "            feed_dict={\n",
    "                self.x_state: x_state\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 step 123 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 0 len(memory) 124=====\n",
      "episode 1 step 296 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 1 len(memory) 421=====\n",
      "loss 1.71378\n",
      "episode 2 step 34 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 2 len(memory) 455=====\n",
      "loss 0.855086\n",
      "episode 3 step 33 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 3 len(memory) 488=====\n",
      "loss 1.19058\n",
      "episode 4 step 48 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 4 len(memory) 536=====\n",
      "loss 0.807829\n",
      "episode 5 step 105 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 5 len(memory) 641=====\n",
      "loss 0.644671\n",
      "episode 6 step 10 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 6 len(memory) 651=====\n",
      "loss 0.978677\n",
      "episode 7 step 34 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 7 len(memory) 685=====\n",
      "loss 0.979478\n",
      "episode 8 step 15 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 8 len(memory) 700=====\n",
      "loss 0.917637\n",
      "episode 9 step 1 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 9 len(memory) 701=====\n",
      "loss 0.645168\n",
      "episode 10 step 9 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 10 len(memory) 710=====\n",
      "loss 0.838697\n",
      "episode 11 step 5 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 11 len(memory) 715=====\n",
      "loss 0.668934\n",
      "=====training episode 12 len(memory) 729=====\n",
      "loss 0.851975\n",
      "episode 13 step 20 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 13 len(memory) 749=====\n",
      "loss 0.842484\n",
      "episode 14 step 0 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 14 len(memory) 749=====\n",
      "loss 0.867405\n",
      "episode 15 step 2 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 15 len(memory) 751=====\n",
      "loss 0.666923\n",
      "episode 16 step 5 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 16 len(memory) 756=====\n",
      "loss 0.894169\n",
      "episode 17 step 6 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 17 len(memory) 762=====\n",
      "loss 0.980744\n",
      "episode 18 step 61 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 18 len(memory) 823=====\n",
      "loss 0.900884\n",
      "episode 19 step 6 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 19 len(memory) 829=====\n",
      "loss 0.962684\n",
      "episode 20 step 65 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 20 len(memory) 894=====\n",
      "loss 0.84585\n",
      "=====training episode 21 len(memory) 920=====\n",
      "loss 0.821042\n",
      "=====training episode 22 len(memory) 921=====\n",
      "loss 0.921834\n",
      "episode 23 step 9 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 23 len(memory) 930=====\n",
      "loss 0.813059\n",
      "episode 24 step 28 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 24 len(memory) 958=====\n",
      "loss 0.644961\n",
      "episode 25 step 13 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 25 len(memory) 971=====\n",
      "loss 0.967409\n",
      "episode 26 step 81 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 26 len(memory) 1052=====\n",
      "loss 0.982734\n",
      "=====training episode 27 len(memory) 1088=====\n",
      "loss 0.809143\n",
      "episode 28 step 7 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 28 len(memory) 1095=====\n",
      "loss 0.69249\n",
      "=====training episode 29 len(memory) 1099=====\n",
      "loss 0.969434\n",
      "episode 30 step 1 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 30 len(memory) 1100=====\n",
      "loss 0.847835\n",
      "episode 31 step 1 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 31 len(memory) 1101=====\n",
      "loss 0.996265\n",
      "episode 32 step 2 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 32 len(memory) 1103=====\n",
      "loss 3.34129\n",
      "episode 33 step 46 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 33 len(memory) 1149=====\n",
      "loss 2.70374\n",
      "episode 34 step 211 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 34 len(memory) 1360=====\n",
      "loss 0.672462\n",
      "episode 35 step 28 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 35 len(memory) 1388=====\n",
      "loss 1.08109\n",
      "episode 36 step 11 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 36 len(memory) 1399=====\n",
      "loss 0.876354\n",
      "episode 37 step 43 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 37 len(memory) 1442=====\n",
      "loss 1.0436\n",
      "episode 38 step 8 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 38 len(memory) 1450=====\n",
      "loss 0.838015\n",
      "episode 39 step 0 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 39 len(memory) 1450=====\n",
      "loss 0.723744\n",
      "episode 40 step 4 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 40 len(memory) 1454=====\n",
      "loss 0.60469\n",
      "episode 41 step 42 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 41 len(memory) 1496=====\n",
      "loss 0.620743\n",
      "episode 42 step 20 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 42 len(memory) 1516=====\n",
      "loss 0.794738\n",
      "episode 43 step 22 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 43 len(memory) 1538=====\n",
      "loss 0.841094\n",
      "episode 44 step 0 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 44 len(memory) 1538=====\n",
      "loss 0.809334\n",
      "episode 45 step 20 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 45 len(memory) 1558=====\n",
      "loss 0.788128\n",
      "episode 46 step 26 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 46 len(memory) 1584=====\n",
      "loss 0.981305\n",
      "episode 47 step 7 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 47 len(memory) 1591=====\n",
      "loss 0.817243\n",
      "=====training episode 48 len(memory) 1610=====\n",
      "loss 0.984859\n",
      "episode 49 step 20 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 49 len(memory) 1630=====\n",
      "loss 0.611912\n",
      "episode 50 step 63 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 50 len(memory) 1693=====\n",
      "loss 0.965464\n",
      "episode 51 step 7 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 51 len(memory) 1700=====\n",
      "loss 0.734248\n",
      "episode 52 step 26 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 52 len(memory) 1726=====\n",
      "loss 0.762205\n",
      "episode 53 step 24 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 53 len(memory) 1750=====\n",
      "loss 0.869047\n",
      "episode 54 step 42 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 54 len(memory) 1792=====\n",
      "loss 0.713256\n",
      "episode 55 step 48 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 55 len(memory) 1840=====\n",
      "loss 0.791471\n",
      "episode 56 step 0 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 56 len(memory) 1840=====\n",
      "loss 0.696674\n",
      "episode 57 step 18 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 57 len(memory) 1858=====\n",
      "loss 0.622564\n",
      "episode 58 step 13 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 58 len(memory) 1871=====\n",
      "loss 1.05008\n",
      "episode 59 step 0 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 59 len(memory) 1871=====\n",
      "loss 1.15845\n",
      "episode 60 step 27 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 60 len(memory) 1898=====\n",
      "loss 0.756626\n",
      "episode 61 step 14 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 61 len(memory) 1912=====\n",
      "loss 0.448953\n",
      "episode 62 step 7 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 62 len(memory) 1919=====\n",
      "loss 0.88058\n",
      "episode 63 step 12 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 63 len(memory) 1931=====\n",
      "loss 0.740386\n",
      "episode 64 step 7 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 64 len(memory) 1938=====\n",
      "loss 0.981535\n",
      "episode 65 step 55 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 65 len(memory) 1993=====\n",
      "loss 0.591837\n",
      "episode 66 step 38 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 66 len(memory) 2031=====\n",
      "loss 0.502016\n",
      "episode 67 step 0 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 67 len(memory) 2031=====\n",
      "loss 0.785506\n",
      "episode 68 step 110 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 68 len(memory) 2141=====\n",
      "loss 0.671633\n",
      "episode 69 step 0 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 69 len(memory) 2141=====\n",
      "loss 0.800298\n",
      "episode 70 step 60 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 70 len(memory) 2201=====\n",
      "loss 1.01999\n",
      "episode 71 step 16 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 71 len(memory) 2217=====\n",
      "loss 0.752146\n",
      "episode 72 step 9 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 72 len(memory) 2226=====\n",
      "loss 0.507696\n",
      "episode 73 step 33 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 73 len(memory) 2259=====\n",
      "loss 0.395251\n",
      "episode 74 step 43 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 74 len(memory) 2302=====\n",
      "loss 0.589741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 75 step 65 cur_state [ 0.  0.  1.] action <pad> r 1\n",
      "=====training episode 75 len(memory) 2367=====\n",
      "loss 0.733967\n",
      "=====training episode 76 len(memory) 2381=====\n",
      "loss 0.927058\n",
      "=====training episode 77 len(memory) 2474=====\n",
      "loss 0.872087\n",
      "episode 78 step 0 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 78 len(memory) 2474=====\n",
      "loss 0.756117\n",
      "episode 79 step 87 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 79 len(memory) 2561=====\n",
      "loss 0.57316\n",
      "episode 80 step 3 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 80 len(memory) 2564=====\n",
      "loss 0.631563\n",
      "episode 81 step 59 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 81 len(memory) 2623=====\n",
      "loss 0.89631\n",
      "episode 82 step 54 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 82 len(memory) 2677=====\n",
      "loss 1.02605\n",
      "episode 83 step 61 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 83 len(memory) 2738=====\n",
      "loss 0.854731\n",
      "episode 84 step 68 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 84 len(memory) 2806=====\n",
      "loss 0.615275\n",
      "=====training episode 85 len(memory) 2884=====\n",
      "loss 0.668437\n",
      "episode 86 step 1 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 86 len(memory) 2885=====\n",
      "loss 0.632789\n",
      "episode 87 step 16 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 87 len(memory) 2901=====\n",
      "loss 0.504641\n",
      "=====training episode 88 len(memory) 2937=====\n",
      "loss 0.879974\n",
      "episode 89 step 13 cur_state [ 0.  0.  1.] action <s> r 1\n",
      "=====training episode 89 len(memory) 2950=====\n",
      "loss 0.555595\n",
      "episode 90 step 116 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 90 len(memory) 3066=====\n",
      "loss 0.506401\n",
      "=====training episode 91 len(memory) 3074=====\n",
      "loss 0.399128\n",
      "episode 92 step 39 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 92 len(memory) 3113=====\n",
      "loss 0.823116\n",
      "episode 93 step 8 cur_state [ 0.  0.  1.] action </s> r 1\n",
      "=====training episode 93 len(memory) 3121=====\n",
      "loss 0.685412\n",
      "episode 94 step 37 cur_state [ 0.  0.  1.] action <unk> r 1\n",
      "=====training episode 94 len(memory) 3158=====\n",
      "loss 0.766927\n",
      "episode 95 step 16 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 95 len(memory) 3174=====\n",
      "loss 0.805149\n",
      "=====training episode 96 len(memory) 3192=====\n",
      "loss 0.599982\n",
      "episode 97 step 0 cur_state [ 0.  0.  1.] action 0 r 1\n",
      "=====training episode 97 len(memory) 3192=====\n",
      "loss 0.440408\n",
      "=====training episode 98 len(memory) 3202=====\n",
      "loss 0.899286\n",
      "=====training episode 99 len(memory) 3216=====\n",
      "loss 0.648923\n",
      "=======test_sen[0]========\n",
      "['0', '0', '1', '0', '0', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 4 action <L>\n",
      "[ 0.  0.  1.]\n",
      "step 1 cursor 3 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[1]========\n",
      "['0', '0', '0', '0', '0', '0', '0', '1']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 6 action <R>\n",
      "[ 0.  1.  0.]\n",
      "step 1 cursor 7 action <R>\n",
      "[ 0.  0.  1.]\n",
      "step 2 cursor 8 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[2]========\n",
      "['0', '0', '0', '0', '1', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 4 action <R>\n",
      "[ 0.  0.  1.]\n",
      "step 1 cursor 5 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[3]========\n",
      "['0', '0', '0', '0', '0', '0', '1', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 7 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[4]========\n",
      "['0', '0', '0', '0', '0', '0', '0', '1']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 3 action <R>\n",
      "[ 0.  1.  0.]\n",
      "step 1 cursor 4 action <R>\n",
      "[ 0.  1.  0.]\n",
      "step 2 cursor 5 action <R>\n",
      "[ 0.  1.  0.]\n",
      "step 3 cursor 6 action <R>\n",
      "[ 0.  1.  0.]\n",
      "step 4 cursor 7 action <R>\n",
      "[ 0.  0.  1.]\n",
      "step 5 cursor 8 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[5]========\n",
      "['0', '1', '0', '0', '0', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 5 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 1 cursor 4 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 2 cursor 3 action <L>\n",
      "[ 0.  0.  1.]\n",
      "step 3 cursor 2 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[6]========\n",
      "['1', '0', '0', '0', '0', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 8 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 1 cursor 7 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 2 cursor 6 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 3 cursor 5 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 4 cursor 4 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 5 cursor 3 action <L>\n",
      "[ 1.  0.  0.]\n",
      "step 6 cursor 2 action <L>\n",
      "[ 0.  0.  1.]\n",
      "step 7 cursor 1 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[7]========\n",
      "['0', '0', '0', '0', '1', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 6 action <L>\n",
      "[ 0.  0.  1.]\n",
      "step 1 cursor 5 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[8]========\n",
      "['0', '0', '0', '0', '1', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 5 action <s>\n",
      "[ 0.  0.  0.]\n",
      "=======test_sen[9]========\n",
      "['1', '0', '0', '0', '0', '0', '0', '0']\n",
      "{'1': 5, '</s>': 3, '<s>': 2, '<pad>': 0, '<unk>': 1, '0': 4}\n",
      "step 0 cursor 2 action <L>\n",
      "[ 0.  0.  1.]\n",
      "step 1 cursor 1 action <s>\n",
      "[ 0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "origin_vocabulary = [\"0\", \"1\"]\n",
    "extend_vocabulary = {\"<pad>\": 0, \"<unk>\": 1, \"<s>\": 2, \"</s>\": 3}\n",
    "for w in origin_vocabulary:\n",
    "    extend_vocabulary[w] = len(extend_vocabulary)\n",
    "    \n",
    "def generate_batch_data(n_batch, n_max_length, target_word):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_batch: int\n",
    "    n_max_length: int\n",
    "    _word: string\n",
    "    Returns\n",
    "    -------\n",
    "    sen: list of list\n",
    "        sen[i]: list of string\n",
    "    \"\"\"\n",
    "    batch_data = []\n",
    "    for i in range(n_batch):\n",
    "        sen = []\n",
    "        #for j in range(random.randint(n_max_length // 2, n_max_length)):\n",
    "        for j in range(random.randint(n_max_length, n_max_length)):\n",
    "            sen.append(\"0\")\n",
    "        idx_target = random.randint(0, len(sen)-1)\n",
    "        sen[idx_target] = target_word\n",
    "        batch_data.append(sen)\n",
    "    return batch_data  \n",
    "    \n",
    "n_episode = 100\n",
    "n_max_step = 10000\n",
    "n_batch_size = 100\n",
    "n_max_length = 10\n",
    "n_state_size = 3\n",
    "dqn_dim_in = n_state_size\n",
    "dqn_dim_out = len(extend_vocabulary) + 2\n",
    "dqn_gamma = 0.5\n",
    "epsilon = 0.9\n",
    "target_word = \"1\"\n",
    "\n",
    "memory_pool = []\n",
    "n_min_memory_pool_size = 200\n",
    "n_max_memory_pool_size = 10000\n",
    "train_sen = generate_batch_data(n_episode, n_max_length-2, target_word)\n",
    "test_sen = generate_batch_data(10, n_max_length-2, target_word)\n",
    "\n",
    "dqn = DQN(dqn_dim_in, dqn_dim_out, dim_hidden=32, gamma=dqn_gamma, l2_alpha=.001)\n",
    "\n",
    "# train on train_sen\n",
    "for episode in range(n_episode):\n",
    "    game = TurnOffLightGame(train_sen[episode], extend_vocabulary, target_word, n_state_size)\n",
    "    for step in range(n_max_step):\n",
    "        #print(\"=====training episode {} step {} len(memory) {}=====\".format(episode, step, len(memory_pool)))\n",
    "        if game.is_done():\n",
    "            break\n",
    "        cur_state = game.get_state()\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            actions = game.get_action_list()\n",
    "            a = random.randint(0, len(actions)-1)\n",
    "        else:\n",
    "            batch_state = np.zeros((1, n_state_size), dtype=np.float32)\n",
    "            batch_state[0, :] = game.get_state()\n",
    "            a = np.argmax(dqn.predict(batch_state)[0])\n",
    "        r = game.apply_action(a)\n",
    "        if r > 0:\n",
    "            print(\"episode {} step {} cur_state {} action {} r {}\".format(episode, step, cur_state, game.action_lookup(a), r))\n",
    "        if game.is_done():\n",
    "            memory_pool.append((cur_state, a, r, game.get_state()))\n",
    "        else:\n",
    "            batch_state = np.zeros((1, n_state_size), dtype=np.float32)\n",
    "            batch_state[0, :] = game.get_state()\n",
    "            memory_pool.append((cur_state, a, r + dqn_gamma*np.max(dqn.predict(batch_state)[0]), game.get_state()))\n",
    "        \n",
    "        if len(memory_pool) >= n_min_memory_pool_size:\n",
    "            batch_x_state = np.zeros((n_batch_size, n_state_size), dtype=np.float32)\n",
    "            batch_y_mask = np.zeros((n_batch_size, dqn_dim_out), dtype=np.int32)\n",
    "            batch_y_target = np.zeros((n_batch_size, dqn_dim_out), dtype=np.float32)\n",
    "        \n",
    "            for i, idx in enumerate(random.sample(range(len(memory_pool)), n_batch_size)):\n",
    "                s, a, r, new_s = memory_pool[idx]\n",
    "                batch_x_state[i, :] = s\n",
    "                batch_y_mask[i, a] = 1\n",
    "                batch_y_target[i, a] = r\n",
    "            loss, _ = dqn.train(batch_x_state, batch_y_mask, batch_y_target)\n",
    "    print(\"=====training episode {} len(memory) {}=====\".format(episode, len(memory_pool)))\n",
    "    if len(memory_pool) > n_min_memory_pool_size:\n",
    "        random.shuffle(memory_pool)\n",
    "        memory_pool = memory_pool[-n_max_memory_pool_size-1:-1]\n",
    "        print(\"loss\", loss)\n",
    "\n",
    "# evaulate on test_sen\n",
    "for i in range(len(test_sen)):\n",
    "    print(\"=======test_sen[{}]========\".format(i))\n",
    "    print(test_sen[i])\n",
    "    print(extend_vocabulary)\n",
    "    game = TurnOffLightGame(test_sen[i], extend_vocabulary, \"1\", n_state_size)\n",
    "    for step in range(n_max_step):\n",
    "        batch_state = np.zeros((1, n_state_size), dtype=np.float32)\n",
    "        batch_state[0, :] = game.get_state()\n",
    "        a = np.argmax(dqn.predict(batch_state)[0])\n",
    "        print(\"step\", step, \"cursor\", game.cursor, \"action\", game.action_lookup(a))\n",
    "        game.apply_action(a)\n",
    "        print(game.get_state())\n",
    "        if game.is_done():\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"product:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"product:0\", shape=(1, 1), dtype=float32)\n",
      "Tensor(\"product:0\", shape=(1, 1), dtype=float32)\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def Graph1():\n",
    "    g1 = tf.Graph()\n",
    "    with g1.as_default() as g:\n",
    "        matrix1 = tf.constant([[3., 3.]])\n",
    "        matrix2 = tf.constant([[2.],[2.]])\n",
    "        product = tf.matmul( matrix1, matrix2, name = \"product\")\n",
    "\n",
    "    with tf.Session( graph = g ) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        return product\n",
    "\n",
    "\n",
    "def Graph2(incoming):\n",
    "    i = incoming\n",
    "    g2 = tf.Graph()\n",
    "    with g2.as_default() as g:\n",
    "        matrix1 = tf.constant([[4., 4.]])\n",
    "        matrix2 = tf.constant([[5.],[5.]])\n",
    "        product = tf.matmul( matrix1, matrix2, name = \"product\" )\n",
    "\n",
    "    with tf.Session( graph = g ) as sess:\n",
    "        tf.initialize_all_variables().run()\n",
    "        print( product)\n",
    "        print( i)\n",
    "        print(i == i)\n",
    "        print(i == product)\n",
    "\n",
    "print (Graph1())\n",
    "\n",
    "Graph2(Graph1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 3\n",
      "b 5\n"
     ]
    }
   ],
   "source": [
    "for k, v in {\"a\": 3, \"b\": 5}.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
