{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brown Corpus from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique word: 56057\n",
      "# of words: 1161192 # of sents: 57340\n",
      "max len(sents[i]): 180\n",
      "# of sents with length < 30: 45692\n",
      "sample sent: ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\n",
      "===============after preprocessing===============\n",
      "unique word: 48052\n",
      "# of words: 653927 # of sents: 48129\n",
      "max len(sents[i]): 28\n",
      "sample sent: ['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "import collections\n",
    "import re\n",
    "\n",
    "words = brown.words()  # only use 'news' for quick development purpose\n",
    "sents = brown.sents()  # only use 'news' for quick development purpose\n",
    "counter = collections.Counter()\n",
    "for word in words:\n",
    "    counter[word] += 1\n",
    "print(\"unique word:\", len(counter))\n",
    "print(\"# of words:\", len(words), \"# of sents:\", len(sents))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s in sents]))\n",
    "print(\"# of sents with length < 30:\", len([len(s) for s in sents if len(s) < 30]))\n",
    "print(\"sample sent:\", sents[0])\n",
    "\n",
    "\n",
    "preprocess_sents = []\n",
    "corpus_counter = collections.Counter()\n",
    "for sent in sents:\n",
    "    tmp_sent = []\n",
    "    for word in sent:\n",
    "        if re.search('[a-zA-Z]', word):\n",
    "            tmp_sent.append(word.lower())\n",
    "            corpus_counter[word.lower()] += 1\n",
    "    if len(tmp_sent) <= 28:\n",
    "        preprocess_sents.append(tmp_sent)\n",
    "\n",
    "print(\"===============after preprocessing===============\")\n",
    "\n",
    "print(\"unique word:\", len(corpus_counter))\n",
    "print(\"# of words:\", sum([len(s) for s in preprocess_sents]), \"# of sents:\", len(preprocess_sents))\n",
    "print(\"max len(sents[i]):\", max([len(s) for s in preprocess_sents]))\n",
    "print(\"sample sent:\", preprocess_sents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Brown Corpus to train an seq2seq autoencoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "random.seed(1337)\n",
    "\n",
    "class EncoderDecoder:\n",
    "    def __init__(self, vocabulary={}, state_size=64, n_max_length=30):     \n",
    "        self.state_size = state_size\n",
    "        self.n_max_length = n_max_length\n",
    "        self.vocabulary = vocabulary\n",
    "        self.reverse_vocabulary = {k: v for k, v in vocabulary.items()}\n",
    "        \n",
    "        ######################\n",
    "        # Graph Construction #\n",
    "        ######################\n",
    "        self.graph = tf.Graph()\n",
    "        with self.graph.as_default():\n",
    "            #self.sen_en = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_en\")\n",
    "            #self.sen_de = tf.placeholder(tf.int32, shape=(None, self.n_max_length), name=\"sen_de\")\n",
    "            self.sen_en = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_en\")\n",
    "            self.sen_de = tf.placeholder(tf.int32, shape=(None, None), name=\"sen_de\")\n",
    "            self.sen_en_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_en_length\")\n",
    "            self.sen_de_length = tf.placeholder(tf.int32, shape=(None,), name=\"sen_de_length\")\n",
    "            \n",
    "            batch_size_en = tf.shape(self.sen_en)[0]\n",
    "            batch_size_de = tf.shape(self.sen_de)[0]\n",
    "            batch_max_length_de = tf.shape(self.sen_de)[1]\n",
    "            \n",
    "            # TODO sen_en_embedding could also be self-trained embedding: embedding_lookup\n",
    "            self.embedding = tf.Variable(tf.random_uniform([len(self.vocabulary), self.state_size], -1.0, 1.0), dtype=tf.float32)\n",
    "            #self.sen_en_embedding = tf.one_hot(self.sen_en, len(self.vocabulary))\n",
    "            #self.sen_de_embedding = tf.one_hot(self.sen_de, len(self.vocabulary))\n",
    "            self.sen_en_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_en)\n",
    "            self.sen_de_embedding = tf.nn.embedding_lookup(self.embedding, self.sen_de)\n",
    "            \n",
    "            # build encoder decoder structure\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en_fw = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "                self.cell_en_bw = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de = tf.contrib.rnn.BasicLSTMCell(self.state_size*2)\n",
    "            with tf.variable_scope(\"encoder\") as scope:\n",
    "                self.cell_en_fw_init = self.cell_en_fw.zero_state(batch_size_en, tf.float32)\n",
    "                self.cell_en_bw_init = self.cell_en_bw.zero_state(batch_size_en, tf.float32)\n",
    "                self.h_state_en, self.final_state_en = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    self.cell_en_fw,\n",
    "                    self.cell_en_bw,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_en_length,\n",
    "                    initial_state_fw=self.cell_en_fw_init,\n",
    "                    initial_state_bw=self.cell_en_bw_init,      \n",
    "                )\n",
    "            with tf.variable_scope(\"decoder\") as scope:\n",
    "                self.cell_de_init = tf.contrib.rnn.LSTMStateTuple(\n",
    "                    c=tf.concat([self.final_state_en[0].c, self.final_state_en[1].c], 1),\n",
    "                    h=tf.concat([self.final_state_en[0].h, self.final_state_en[1].h], 1),\n",
    "                )             \n",
    "                self.h_state_de, self.final_state_de = tf.nn.dynamic_rnn(\n",
    "                    self.cell_de,\n",
    "                    self.sen_en_embedding,\n",
    "                    sequence_length=self.sen_de_length,\n",
    "                    initial_state=self.cell_de_init,\n",
    "                )\n",
    "\n",
    "            with tf.variable_scope(\"softmax\") as scope:\n",
    "                W = tf.get_variable(\"W\", [self.state_size*2, len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))\n",
    "                b = tf.get_variable(\"b\", [len(self.vocabulary)], initializer=tf.random_normal_initializer(seed=None))               \n",
    "            self.logits = tf.reshape(\n",
    "                tf.add(tf.matmul(tf.reshape(self.h_state_de, (-1, self.state_size*2)), W), b),\n",
    "                shape=(-1, batch_max_length_de, len(self.vocabulary))\n",
    "            )\n",
    "            self.prediction = tf.nn.softmax(self.logits)\n",
    "                \n",
    "            # construct loss and train op\n",
    "            self.cross_ent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=self.sen_de,\n",
    "                logits=self.logits\n",
    "            )        \n",
    "            #self.mask = tf.sign(tf.reduce_max(tf.abs(self.sen_de_embedding), 2))\n",
    "            self.mask = tf.sequence_mask(self.sen_de_length, maxlen=batch_max_length_de)\n",
    "            self.loss = tf.reduce_mean(\n",
    "                #tf.reduce_sum(tf.multiply(self.cross_ent, self.mask), 1) / tf.reduce_sum(self.mask, 1)\n",
    "                tf.divide(\n",
    "                    tf.reduce_sum(\n",
    "                        tf.where(\n",
    "                            self.mask,\n",
    "                            self.cross_ent,\n",
    "                            tf.zeros_like(self.cross_ent)\n",
    "                        ), 1\n",
    "                    ),\n",
    "                    tf.to_float(self.sen_de_length)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \"\"\"\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train = optimizer.minimize(self.loss)\n",
    "            \"\"\"\n",
    "            # Calculate and clip gradients\n",
    "            params = tf.trainable_variables()\n",
    "            gradients = tf.gradients(self.loss, params)\n",
    "            self.clipped_gradients, _ = tf.clip_by_global_norm(gradients, 1)\n",
    "            # Optimization\n",
    "            optimizer = tf.train.AdamOptimizer()\n",
    "            self.op_train = optimizer.apply_gradients(zip(self.clipped_gradients, params))\n",
    "            \n",
    "            # initializer\n",
    "            gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1)\n",
    "            self.sess = tf.Session(\n",
    "                graph=self.graph,\n",
    "                config=tf.ConfigProto(gpu_options=gpu_options)\n",
    "            )           \n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.sess.run(self.init)\n",
    "            \n",
    "    def train(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        assert batch_sen_en.shape[1] == self.n_max_length  # training always input same length as self.n_max_length\n",
    "        _, loss, prediction, sen_en_embedding, mask, cross_ent, clipped_gradients = self.sess.run(\n",
    "            [self.op_train, self.loss, self.prediction, self.sen_en_embedding, self.mask, self.cross_ent, self.clipped_gradients],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, prediction, sen_en_embedding, mask, cross_ent, clipped_gradients\n",
    "        \n",
    "    def predict(self, batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "        \"\"\"\n",
    "        assert batch_sen_en.shape[0] == batch_sen_de.shape[0]\n",
    "        loss, prediction = self.sess.run(\n",
    "            [self.loss, self.prediction],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_de: batch_sen_de,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "                self.sen_de_length: batch_sen_de_length,\n",
    "            }\n",
    "        )\n",
    "        return loss, prediction\n",
    "    \n",
    "    def get_dqn_state(self, batch_sen_en, batch_sen_en_length, batch_cursor):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        batch_cursor: numpy, shape=(n,), dtype=int\n",
    "        Returns\n",
    "        -------\n",
    "        dqn_state: numpy, shape=(n, state_size*3)\n",
    "        \"\"\"\n",
    "        batch_size = batch_sen_en.shape[0]\n",
    "        batch_dqn_state = np.empty((batch_size, self.state_size*3), dtype=np.float32)\n",
    "        batch_h_state_en, batch_sen_en_embedding = self.sess.run(\n",
    "            [self.h_state_en, self.sen_en_embedding],\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length\n",
    "            }\n",
    "        )\n",
    "        for i in range(batch_size):\n",
    "            batch_dqn_state[i,:self.state_size] = batch_h_state_en[0][i,batch_cursor[i]-1]\n",
    "            batch_dqn_state[i,self.state_size:self.state_size*2] = batch_sen_en_embedding[i,batch_cursor[i]]\n",
    "            batch_dqn_state[i,self.state_size*2:] = batch_h_state_en[1][i,batch_cursor[i]+1]\n",
    "        return batch_dqn_state\n",
    "    \n",
    "    def encode(self, batch_sen_en, batch_sen_en_length):  # is wrong in nltk_brown_ae_bilstm version\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_sen_en: numpy, shape=(n, max_length), dtype=int\n",
    "        batch_sen_en_length: numpy, shape=(n,), dtype=int\n",
    "        Returns\n",
    "        -------\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        \"\"\"\n",
    "        batch_state_en = self.sess.run(\n",
    "            self.cell_de_init,\n",
    "            feed_dict={\n",
    "                self.sen_en: batch_sen_en,\n",
    "                self.sen_en_length: batch_sen_en_length,\n",
    "            }\n",
    "        )\n",
    "        return batch_state_en\n",
    "    \n",
    "    def decode(self, batch_state_en):  # is wrong in nltk_brown_ae_bilstm version\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_state_en: LSTMStateTuple\n",
    "        Returns\n",
    "        -------\n",
    "        batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "        \"\"\"\n",
    "        batch_size = batch_state_en.c.shape[0]\n",
    "        batch_sen_de = np.empty([batch_size, self.n_max_length], dtype=np.int32)\n",
    "        \n",
    "        tmp_sen_en = np.empty([batch_size, 1], dtype=np.int32)\n",
    "        tmp_sen_en_length = np.ones([batch_size], dtype=np.int32)\n",
    "        tmp_sen_en[:] = self.vocabulary[\"<s>\"]\n",
    "        tmp_last_state = batch_state_en\n",
    "        for i in range(self.n_max_length):\n",
    "            tmp_predict, tmp_last_state = self.sess.run(\n",
    "                [self.prediction, self.final_state_de],\n",
    "                feed_dict={\n",
    "                    self.cell_de_init: tmp_last_state,\n",
    "                    self.sen_en: tmp_sen_en,\n",
    "                    self.sen_de: tmp_sen_en,  # only need its batch_size\n",
    "                    self.sen_de_length: tmp_sen_en_length,  # only need its length\n",
    "                }\n",
    "            )\n",
    "            tmp_sen_en = np.argmax(tmp_predict, axis=2)\n",
    "            batch_sen_de[:,i] = tmp_sen_en[:,0]\n",
    "           \n",
    "        return batch_sen_de\n",
    "\n",
    "    \n",
    "def evaluate(batch_sen_de, batch_sen_de_length, batch_prediction, vocabulary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_sen_de: numpy, shape=(n, max_length), dtype=int\n",
    "    batch_sen_de_length: numpy, shape=(n,), dtype=int\n",
    "    batch_prediction: numpy, shape=(n, max_length, len(vocabulary))\n",
    "    \"\"\"\n",
    "    assert batch_sen_de.shape[0] == batch_prediction.shape[0]\n",
    "    acc_word = 0\n",
    "    acc_sen_end = 0\n",
    "    for i in range(batch_sen_de.shape[0]):\n",
    "        is_first_end = False\n",
    "        for j in range(batch_sen_de_length[i]):\n",
    "            cur_pred_word = np.argmax(batch_prediction[i, j])\n",
    "            if cur_pred_word == batch_sen_de[i, j]:\n",
    "                acc_word += 1\n",
    "                if not is_first_end and cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                    acc_sen_end += 1\n",
    "            if cur_pred_word == vocabulary[\"</s>\"]:\n",
    "                is_first_end = True\n",
    "    return 1. * acc_word / np.sum(batch_sen_de_length), 1. * acc_sen_end / batch_sen_de.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_train 30802 n_valid 7700 n_test 9627\n",
      "[202   0 201 201 201 201  52 201  28 201   1 201 201 201 201 201  49 201\n",
      "   6  73 201 201 159 203 200 200 200 200 200 200]\n",
      "['the', 'fulton', 'county', 'grand', 'jury', 'said', 'friday', 'an', 'investigation', 'of', \"atlanta's\", 'recent', 'primary', 'election', 'produced', 'no', 'evidence', 'that', 'any', 'irregularities', 'took', 'place']\n",
      "['<s>', 'the', '<unk>', '<unk>', '<unk>', '<unk>', 'said', '<unk>', 'an', '<unk>', 'of', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', 'no', '<unk>', 'that', 'any', '<unk>', '<unk>', 'place', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "[202 201 129 201 203 200 200 200 200 200 200 200 200 200 200 200 200 200\n",
      " 200 200 200 200 200 200 200 200 200 200 200 200]\n",
      "['ekstrohm', 'never', 'slept']\n",
      "['<s>', '<unk>', 'never', '<unk>', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "epoch 0 valid (0.59329301220434172, 0.6536363636363637)\n",
      "last loss 1.7642\n",
      "epoch 1 valid (0.66507182201101633, 0.817012987012987)\n",
      "last loss 0.881515\n",
      "epoch 2 valid (0.70557295604276915, 0.8716883116883117)\n",
      "last loss 0.539448\n",
      "epoch 3 valid (0.74494005832163301, 0.9107792207792208)\n",
      "last loss 0.354553\n",
      "epoch 4 valid (0.76590344529646825, 0.9383116883116883)\n",
      "last loss 0.186482\n",
      "epoch 5 valid (0.7853439896317097, 0.9323376623376624)\n",
      "last loss 0.0936722\n",
      "epoch 6 valid (0.82039097094718649, 0.8993506493506493)\n",
      "last loss 0.0721936\n",
      "epoch 7 valid (0.83539259099254781, 0.9140259740259741)\n",
      "last loss 0.0584044\n",
      "epoch 8 valid (0.8497245922885841, 0.9416883116883117)\n",
      "last loss 0.0278158\n",
      "epoch 9 valid (0.86038449076574142, 0.9671428571428572)\n",
      "last loss 0.0212504\n",
      "epoch 10 valid (0.86987795658278433, 0.9509090909090909)\n",
      "last loss 0.0265318\n",
      "epoch 11 valid (0.86793390214926014, 0.9763636363636363)\n",
      "last loss 0.0250264\n",
      "epoch 12 valid (0.88694243438816289, 0.9759740259740259)\n",
      "last loss 0.0134258\n",
      "epoch 13 valid (0.89663030564855817, 0.957012987012987)\n",
      "last loss 0.00646151\n",
      "epoch 14 valid (0.89835835403391295, 0.985974025974026)\n",
      "last loss 0.00678236\n",
      "epoch 15 valid (0.90989307700615618, 0.9892207792207792)\n",
      "last loss 0.00627602\n",
      "epoch 16 valid (0.9134571768009504, 0.9902597402597403)\n",
      "last loss 0.0119083\n",
      "epoch 17 valid (0.91364078194189435, 0.9928571428571429)\n",
      "last loss 0.00528544\n",
      "epoch 18 valid (0.91697807538611076, 0.9920779220779221)\n",
      "last loss 0.00470362\n",
      "epoch 19 valid (0.92505670158764441, 0.99)\n",
      "last loss 0.0035811\n",
      "train (0.94338651883726699, 0.9885072397896241)\n",
      "test (0.92380742049469966, 0.9877428066895191)\n"
     ]
    }
   ],
   "source": [
    "def generate_data(corpus_sents, max_length, extend_vocabulary):\n",
    "    sen_en = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_de = np.full((len(corpus_sents), max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "    sen_en_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "    sen_de_length = np.zeros((len(corpus_sents),), dtype=np.int32)\n",
    "\n",
    "    def get_random_sequence(sent, max_length):\n",
    "        x = np.full((max_length), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "        for i, word in enumerate(sent):\n",
    "            if word in extend_vocabulary:\n",
    "                x[i] = extend_vocabulary[word]\n",
    "            else:\n",
    "                x[i] = extend_vocabulary[\"<unk>\"]\n",
    "        return x\n",
    "\n",
    "    for i in range(len(corpus_sents)):\n",
    "        l = len(corpus_sents[i])\n",
    "        sen_en[i, :] = get_random_sequence(corpus_sents[i], max_length)\n",
    "        sen_en[i,1:l+1] = sen_en[i,:l]\n",
    "        sen_en[i,0] = extend_vocabulary[\"<s>\"]\n",
    "        sen_en[i, l+1] = extend_vocabulary[\"</s>\"]\n",
    "        sen_de[i, :max_length-1] = sen_en[i, 1:]\n",
    "        sen_en_length[i] = l + 2\n",
    "        sen_de_length[i] = l + 1\n",
    "    \n",
    "    return sen_en, sen_de, sen_en_length, sen_de_length\n",
    "\n",
    "def get_total_accuracy(data_sen_en, data_sen_de, data_sen_en_length, data_sen_de_length, extend_vocabulary, pretrained_lstm):\n",
    "    n_hit_word, n_hit_length = 0, 0\n",
    "    n_total_word, n_total_length = 0, 0\n",
    "    cur_idx = 0\n",
    "    while cur_idx < data_sen_en.shape[0]:\n",
    "        batch_sen_en = data_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = data_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = data_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = data_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        _, predictions = pretrained_lstm.predict(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "        cur_acc_word, cur_acc_length = evaluate(batch_sen_de, batch_sen_de_length, predictions, extend_vocabulary)\n",
    "        n_hit_word += cur_acc_word * np.sum(batch_sen_de_length)\n",
    "        n_total_word += np.sum(batch_sen_de_length)\n",
    "        n_hit_length += cur_acc_length * batch_sen_de.shape[0]\n",
    "        n_total_length += batch_sen_de.shape[0]\n",
    "    return 1. * n_hit_word / n_total_word, 1. * n_hit_length / n_total_length\n",
    "    \n",
    "# hyperparameter\n",
    "vocabulary_size = 200\n",
    "origin_vocabulary = {}\n",
    "for word, n in corpus_counter.most_common(vocabulary_size):\n",
    "    origin_vocabulary[\"{}\".format(word)] = len(origin_vocabulary)\n",
    "extend_vocabulary = dict(origin_vocabulary)\n",
    "for w in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"]:\n",
    "    extend_vocabulary[w] = len(extend_vocabulary)\n",
    "#vocabulary = {\"<pad>\": 0, \"<unk>\": 1, \"<s>\": 2, \"</s>\": 3, \"a\": 4, \"b\": 5}\n",
    "state_size=64\n",
    "n_max_length=30\n",
    "n_batch_size=100\n",
    "\n",
    "# generate training/testing data\n",
    "n_train = int(len(preprocess_sents) * 0.8 * 0.8)\n",
    "n_valid = int(len(preprocess_sents) * 0.8 * 0.2)\n",
    "n_test = len(preprocess_sents) - n_train - n_valid\n",
    "print(\"n_train\", n_train, \"n_valid\", n_valid, \"n_test\", n_test)\n",
    "train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length = generate_data(preprocess_sents[:n_train],\n",
    "                                                                                     n_max_length, extend_vocabulary)\n",
    "valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length = generate_data(preprocess_sents[n_train:n_train+n_valid],\n",
    "                                                                                     n_max_length, extend_vocabulary)\n",
    "test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length = generate_data(preprocess_sents[n_train+n_valid:],\n",
    "                                                                                 n_max_length, extend_vocabulary)\n",
    "reverse_extend_vocabulary = {v: k for k, v in extend_vocabulary.items()}\n",
    "print(train_sen_en[0])\n",
    "print(preprocess_sents[0])\n",
    "print([reverse_extend_vocabulary[i] for i in train_sen_en[0]])\n",
    "print(test_sen_en[0])\n",
    "print(preprocess_sents[n_train+n_valid])\n",
    "print([reverse_extend_vocabulary[i] for i in test_sen_en[0]])\n",
    "\n",
    "pretrained_lstm = EncoderDecoder(vocabulary=extend_vocabulary, state_size=state_size, n_max_length=n_max_length)\n",
    "\n",
    "for epoch in range(20):\n",
    "    cur_idx = 0\n",
    "    while cur_idx < train_sen_en.shape[0]:\n",
    "        batch_sen_en = train_sen_en[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de = train_sen_de[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_en_length = train_sen_en_length[cur_idx: cur_idx + n_batch_size]\n",
    "        batch_sen_de_length = train_sen_de_length[cur_idx: cur_idx + n_batch_size]\n",
    "        \n",
    "        loss, predictions, sen_en_embedding, mask, cross_ent, clipped_gradients = pretrained_lstm.train(\n",
    "            batch_sen_en, batch_sen_de, batch_sen_en_length, batch_sen_de_length\n",
    "        )\n",
    "        cur_idx += n_batch_size\n",
    "    print(\"epoch\", epoch, \"valid\", get_total_accuracy(\n",
    "        valid_sen_en, valid_sen_de, valid_sen_en_length, valid_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "    )) \n",
    "    print(\"last loss\", loss)\n",
    "print(\"train\", get_total_accuracy(\n",
    "    train_sen_en, train_sen_de, train_sen_en_length, train_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    "))\n",
    "print(\"test\", get_total_accuracy(\n",
    "    test_sen_en, test_sen_de, test_sen_en_length, test_sen_de_length, extend_vocabulary, pretrained_lstm\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128)\n",
      "encode [[202  19  77  32 203 200 200 200 200 200 200 200 200 200 200 200 200 200\n",
      "  200 200 200 200 200 200 200 200 200 200 200 200]]\n",
      "decode [[ 19  77  32 203 203  20 203 192  20   7 201 170 203  12   4 201 203  20\n",
      "  140  82 107  71 203  12  14 201   2 201 203  60]]\n",
      "HAHA\n",
      "my_loss 17.6518 my_prediction (1, 30, 204)\n",
      "19\n",
      "77\n",
      "32\n",
      "203\n",
      "107\n"
     ]
    }
   ],
   "source": [
    "my_sen_en = np.full((1,30), extend_vocabulary[\"<pad>\"], dtype=np.int32)\n",
    "my_sen_en[0,0] = extend_vocabulary[\"<s>\"]\n",
    "my_sen_en[0,1] = extend_vocabulary[\"i\"]\n",
    "my_sen_en[0,2] = extend_vocabulary[\"like\"]\n",
    "my_sen_en[0,3] = extend_vocabulary[\"you\"]\n",
    "my_sen_en[0,4] = extend_vocabulary[\"</s>\"]\n",
    "my_sen_en_length = np.empty((1,), dtype=np.int32)\n",
    "my_sen_de_length = np.empty((1,), dtype=np.int32)\n",
    "my_sen_en_length[0] = 5\n",
    "my_sen_de_length[0] = 4\n",
    "\n",
    "my_state_en = pretrained_lstm.encode(my_sen_en, my_sen_en_length)\n",
    "print(my_state_en.c.shape)\n",
    "print(\"encode\", my_sen_en)\n",
    "print(\"decode\", pretrained_lstm.decode(my_state_en))  # exposure bias\n",
    "print(\"HAHA\")\n",
    "my_loss, my_prediction = pretrained_lstm.predict(my_sen_en, my_sen_en, my_sen_en_length, my_sen_de_length)\n",
    "print(\"my_loss\", my_loss, \"my_prediction\", my_prediction.shape)\n",
    "print(np.argmax(my_prediction[0,0]))\n",
    "print(np.argmax(my_prediction[0,1]))\n",
    "print(np.argmax(my_prediction[0,2]))\n",
    "print(np.argmax(my_prediction[0,3]))\n",
    "print(np.argmax(my_prediction[0,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_sen_en.shape (2, 30)\n",
      "batch_sen_en_length.shape (2,)\n",
      "batch_dqn_state.shape (2, 192)\n"
     ]
    }
   ],
   "source": [
    "print(\"batch_sen_en.shape\", batch_sen_en.shape)\n",
    "print(\"batch_sen_en_length.shape\", batch_sen_en_length.shape)\n",
    "batch_cursor = np.array([2, 2], dtype=np.int32)\n",
    "batch_dqn_state = pretrained_lstm.get_dqn_state(batch_sen_en, batch_sen_en_length, batch_cursor)\n",
    "print(\"batch_dqn_state.shape\", batch_dqn_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
